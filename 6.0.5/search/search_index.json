{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Addax \u4ecb\u7ecd","text":""},{"location":"#_1","title":"\u6982\u89c8","text":"<p>Addax \u662f\u4e00\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u79bb\u7ebf\u540c\u6b65\u5de5\u5177\uff0c\u6700\u521d\u6765\u6e90\u4e8e\u963f\u91cc\u7684 DataX \uff0c\u81f4\u529b\u4e8e\u5b9e\u73b0\u5305\u62ec\u5173\u7cfb\u578b\u6570\u636e\u5e93(MySQL\u3001Oracle \u7b49)\u3001HDFS\u3001Hive\u3001HBase\u3001FTP \u7b49\u5404\u79cd\u5f02\u6784\u6570\u636e\u6e90\u4e4b\u95f4\u7a33\u5b9a\u9ad8\u6548\u7684\u6570\u636e\u540c\u6b65\u529f\u80fd\u3002</p> <p></p> <p>\u4e3a\u4e86\u89e3\u51b3\u5f02\u6784\u6570\u636e\u6e90\u540c\u6b65\u95ee\u9898\uff0cAddax \u5c06\u590d\u6742\u7684\u7f51\u72b6\u7684\u540c\u6b65\u94fe\u8def\u53d8\u6210\u4e86\u661f\u578b\u6570\u636e\u94fe\u8def\uff0cAddax \u4f5c\u4e3a\u4e2d\u95f4\u4f20\u8f93\u8f7d\u4f53\u8d1f\u8d23\u8fde\u63a5\u5404\u79cd\u6570\u636e\u6e90\u3002\u5f53\u9700\u8981\u63a5\u5165\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u6e90\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u5c06\u6b64\u6570\u636e\u6e90\u5bf9\u63a5\u5230 Addax\uff0c\u4fbf\u80fd\u8ddf\u5df2\u6709\u7684\u6570\u636e\u6e90\u505a\u5230\u65e0\u7f1d\u6570\u636e\u540c\u6b65\u3002</p>"},{"location":"#_2","title":"\u6846\u67b6\u8bbe\u8ba1","text":"<pre><code>graph LR\nMySQL\nsubgraph Addax\n    direction LR\n    subgraph reader[\"Reader Plugin\"]\n        mr[\"MySQLReader\"]\n    end\n    subgraph writer[\"Writer Plugin\"]\n    hw[\"HDFSWriter\"]\n    end\n    Framework\n    mr --&gt; Framework --&gt; writer\nend\n\nMySQL ==&gt; Addax ==&gt; HDFS\n</code></pre> <p>Addax \u672c\u8eab\u4f5c\u4e3a\u79bb\u7ebf\u6570\u636e\u540c\u6b65\u6846\u67b6\uff0c\u91c7\u7528 Framework + plugin \u67b6\u6784\u6784\u5efa\u3002\u5c06\u6570\u636e\u6e90\u8bfb\u53d6\u548c\u5199\u5165\u62bd\u8c61\u6210\u4e3a Reader/Writer \u63d2\u4ef6\uff0c\u7eb3\u5165\u5230\u6574\u4e2a\u540c\u6b65\u6846\u67b6\u4e2d\u3002</p> <ul> <li>Reader\uff1aReader \u4e3a\u6570\u636e\u91c7\u96c6\u6a21\u5757\uff0c\u8d1f\u8d23\u91c7\u96c6\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5c06\u6570\u636e\u53d1\u9001\u7ed9 Framework\u3002</li> <li>Writer\uff1a Writer \u4e3a\u6570\u636e\u5199\u5165\u6a21\u5757\uff0c\u8d1f\u8d23\u4e0d\u65ad\u5411 Framework \u53d6\u6570\u636e\uff0c\u5e76\u5c06\u6570\u636e\u5199\u5165\u5230\u76ee\u7684\u7aef\u3002</li> <li>Framework\uff1aFramework \u7528\u4e8e\u8fde\u63a5 reader \u548c writer\uff0c\u4f5c\u4e3a\u4e24\u8005\u7684\u6570\u636e\u4f20\u8f93\u901a\u9053\uff0c\u5e76\u5904\u7406\u7f13\u51b2\uff0c\u6d41\u63a7\uff0c\u5e76\u53d1\uff0c\u6570\u636e\u8f6c\u6362\u7b49\u6838\u5fc3\u6280\u672f\u95ee\u9898\u3002</li> </ul> <p>Addax Framework \u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u63a5\u53e3\u4e0e\u63d2\u4ef6\u4ea4\u4e92\uff0c\u63d0\u4f9b\u7b80\u5355\u7684\u63d2\u4ef6\u63a5\u5165\u673a\u5236\uff0c\u53ea\u9700\u8981\u4efb\u610f\u52a0\u4e0a\u4e00\u79cd\u63d2\u4ef6\uff0c\u5c31\u80fd\u65e0\u7f1d\u5bf9\u63a5\u5176\u4ed6\u6570\u636e\u6e90\u3002</p>"},{"location":"#_3","title":"\u6838\u5fc3\u67b6\u6784","text":"<p>\u672c\u5c0f\u8282\u6309\u4e00\u4e2a Addax \u4f5c\u4e1a\u751f\u547d\u5468\u671f\u7684\u65f6\u5e8f\u56fe\uff0c\u4ece\u6574\u4f53\u67b6\u6784\u8bbe\u8ba1\u975e\u5e38\u7b80\u8981\u8bf4\u660e\u5404\u4e2a\u6a21\u5757\u76f8\u4e92\u5173\u7cfb\u3002</p> <pre><code>graph TB\nsubgraph Job\nend\nsubgraph task\n  direction TB\n  t1[\"Task\"]\n  t2[\"Task\"]\n  t3[\"Task\"]\n  t4[\"Task\"]\n  t5[\"Task\"]\n  t6[\"Task\"]\nend\nsubgraph taskgroup[\" \"]\n    direction TB\n  subgraph tg1[\"TaskGroup\"]\n    subgraph tg1_Task[\"Task\"]\n      tg1_r[\"Reader\"]\n      tg1_c[\"Channel\"]\n      tg1_w[\"Writer\"]\n    end\n    t7[\"Task\"]\n    t8[\"Task\"]\n  end\n\n  subgraph tg2[\"TaskGroup\"]\n    subgraph tg2_Task[\"Task\"]\n      direction LR\n      tg2_r[\"Reader\"]\n      tg2_c[\"Channel\"]\n      tg2_w[\"Writer\"]\n    end\n    t9[\"Task\"]\n    t10[\"Task\"]\n  end\n\n  subgraph tg3[\"TaskGroup\"]\n    direction LR\n    subgraph tg3_Task[\"Task\"]\n      tg3_r[\"Reader\"]\n      tg3_c[\"Channel\"]\n      tg3_w[\"Writer\"]\n    end\n    t11[\"Task\"]\n    t12[\"Task\"]\n  end\nend\n\nJob == split ==&gt; task\ntask == Schedule ==&gt; taskgroup</code></pre>"},{"location":"#_4","title":"\u6838\u5fc3\u6a21\u5757\u4ecb\u7ecd","text":"<ol> <li>Addax \u5b8c\u6210\u5355\u4e2a\u6570\u636e\u540c\u6b65\u7684\u4f5c\u4e1a\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a Job\uff0cAddax \u63a5\u6536\u5230\u4e00\u4e2a Job \u4e4b\u540e\uff0c\u5c06\u542f\u52a8\u4e00\u4e2a\u8fdb\u7a0b\u6765\u5b8c\u6210\u6574\u4e2a\u4f5c\u4e1a\u540c\u6b65\u8fc7\u7a0b\u3002Addax Job \u6a21\u5757\u662f\u5355\u4e2a\u4f5c\u4e1a\u7684\u4e2d\u67a2\u7ba1\u7406\u8282\u70b9\uff0c\u627f\u62c5\u4e86\u6570\u636e\u6e05\u7406\u3001\u5b50\u4efb\u52a1\u5207\u5206(\u5c06\u5355\u4e00\u4f5c\u4e1a\u8ba1\u7b97\u8f6c\u5316\u4e3a\u591a\u4e2a\u5b50 Task)\u3001TaskGroup \u7ba1\u7406\u7b49\u529f\u80fd\u3002</li> <li>Addax Job \u542f\u52a8\u540e\uff0c\u4f1a\u6839\u636e\u4e0d\u540c\u7684\u6e90\u7aef\u5207\u5206\u7b56\u7565\uff0c\u5c06 Job \u5207\u5206\u6210\u591a\u4e2a\u5c0f\u7684 Task (\u5b50\u4efb\u52a1)\uff0c\u4ee5\u4fbf\u4e8e\u5e76\u53d1\u6267\u884c\u3002Task \u4fbf\u662f Addax \u4f5c\u4e1a\u7684\u6700\u5c0f\u5355\u5143\uff0c\u6bcf\u4e00\u4e2a Task \u90fd\u4f1a\u8d1f\u8d23\u4e00\u90e8\u5206\u6570\u636e\u7684\u540c\u6b65\u5de5\u4f5c\u3002</li> <li>\u5207\u5206\u591a\u4e2a Task \u4e4b\u540e\uff0cAddax Job \u4f1a\u8c03\u7528 Scheduler \u6a21\u5757\uff0c\u6839\u636e\u914d\u7f6e\u7684\u5e76\u53d1\u6570\u636e\u91cf\uff0c\u5c06\u62c6\u5206\u6210\u7684 Task \u91cd\u65b0\u7ec4\u5408\uff0c\u7ec4\u88c5\u6210 TaskGroup(\u4efb\u52a1\u7ec4)\u3002\u6bcf\u4e00\u4e2a TaskGroup \u8d1f\u8d23\u4ee5\u4e00\u5b9a\u7684\u5e76\u53d1\u8fd0\u884c\u5b8c\u6bd5\u5206\u914d\u597d\u7684\u6240\u6709 Task\uff0c\u9ed8\u8ba4\u5355\u4e2a\u4efb\u52a1\u7ec4\u7684\u5e76\u53d1\u6570\u91cf\u4e3a 5\u3002</li> <li>\u6bcf\u4e00\u4e2a Task \u90fd\u7531 TaskGroup \u8d1f\u8d23\u542f\u52a8\uff0cTask \u542f\u52a8\u540e\uff0c\u4f1a\u56fa\u5b9a\u542f\u52a8 <code>Reader\u2014&gt;Channel\u2014&gt;Writer</code> \u7684\u7ebf\u7a0b\u6765\u5b8c\u6210\u4efb\u52a1\u540c\u6b65\u5de5\u4f5c\u3002</li> <li>Addax \u4f5c\u4e1a\u8fd0\u884c\u8d77\u6765\u4e4b\u540e\uff0c Job \u76d1\u63a7\u5e76\u7b49\u5f85\u591a\u4e2a TaskGroup \u6a21\u5757\u4efb\u52a1\u5b8c\u6210\uff0c\u7b49\u5f85\u6240\u6709 TaskGroup \u4efb\u52a1\u5b8c\u6210\u540e Job \u6210\u529f\u9000\u51fa\u3002\u5426\u5219\uff0c\u5f02\u5e38\u9000\u51fa\uff0c\u8fdb\u7a0b\u9000\u51fa\u503c\u975e 0</li> </ol>"},{"location":"#_5","title":"\u8c03\u5ea6\u6d41\u7a0b","text":"<p>\u4e3e\u4f8b\u6765\u8bf4\uff0c\u7528\u6237\u63d0\u4ea4\u4e86\u4e00\u4e2a\u4f5c\u4e1a\uff0c\u5e76\u4e14\u914d\u7f6e\u4e86 20 \u4e2a\u5e76\u53d1\uff0c\u76ee\u7684\u662f\u5c06\u4e00\u4e2a 100 \u5f20\u5206\u8868\u7684 MySQL \u6570\u636e\u540c\u6b65\u5230 Oracle \u91cc\u9762\u3002 \u8c03\u5ea6\u51b3\u7b56\u601d\u8def\u662f\uff1a</p> <ol> <li>Addax Job \u6839\u636e\u5206\u5e93\u5206\u8868\u5207\u5206\u6210\u4e86 100 \u4e2a Task\u3002</li> <li>\u6839\u636e 20 \u4e2a\u5e76\u53d1\uff0c\u8ba1\u7b97\u5171\u9700\u8981\u5206\u914d <code>20/5 = 4</code> \u4e2a TaskGroup\u3002</li> <li>4 \u4e2a TaskGroup \u5e73\u5206\u5207\u5206\u597d\u7684 100 \u4e2a Task\uff0c\u6bcf\u4e00\u4e2a TaskGroup \u8d1f\u8d23\u4ee5 5 \u4e2a\u5e76\u53d1\u5171\u8ba1\u8fd0\u884c 25 \u4e2a Task\u3002</li> </ol>"},{"location":"#_6","title":"\u6838\u5fc3\u4f18\u52bf","text":""},{"location":"#_7","title":"\u53ef\u9760\u7684\u6570\u636e\u8d28\u91cf\u76d1\u63a7","text":"<ul> <li>\u5b8c\u7f8e\u89e3\u51b3\u6570\u636e\u4f20\u8f93\u4e2a\u522b\u7c7b\u578b\u5931\u771f\u95ee\u9898</li> </ul> <p>\u652f\u6301\u6240\u6709\u7684\u5f3a\u6570\u636e\u7c7b\u578b\uff0c\u6bcf\u4e00\u79cd\u63d2\u4ef6\u90fd\u6709\u81ea\u5df1\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u7b56\u7565\uff0c\u8ba9\u6570\u636e\u53ef\u4ee5\u5b8c\u6574\u65e0\u635f\u7684\u4f20\u8f93\u5230\u76ee\u7684\u7aef\u3002</p> <ul> <li>\u63d0\u4f9b\u4f5c\u4e1a\u5168\u94fe\u8def\u7684\u6d41\u91cf\u3001\u6570\u636e\u91cf\u8fd0\u884c\u65f6\u76d1\u63a7</li> </ul> <p>\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u5c06\u4f5c\u4e1a\u672c\u8eab\u72b6\u6001\u3001\u6570\u636e\u6d41\u91cf\u3001\u6570\u636e\u901f\u5ea6\u3001\u6267\u884c\u8fdb\u5ea6\u7b49\u4fe1\u606f\u8fdb\u884c\u5168\u9762\u7684\u5c55\u793a\uff0c\u8ba9\u7528\u6237\u53ef\u4ee5\u5b9e\u65f6\u4e86\u89e3\u4f5c\u4e1a\u72b6\u6001\u3002\u5e76\u53ef\u5728\u4f5c\u4e1a\u6267\u884c\u8fc7\u7a0b\u4e2d\u667a\u80fd\u5224\u65ad\u6e90\u7aef\u548c\u76ee\u7684\u7aef\u7684\u901f\u5ea6\u5bf9\u6bd4\u60c5\u51b5\uff0c\u7ed9\u4e88\u7528\u6237\u66f4\u591a\u6027\u80fd\u6392\u67e5\u4fe1\u606f\u3002</p> <ul> <li>\u63d0\u4f9b\u810f\u6570\u636e\u63a2\u6d4b</li> </ul> <p>\u5728\u5927\u91cf\u6570\u636e\u7684\u4f20\u8f93\u8fc7\u7a0b\u4e2d\uff0c\u5fc5\u5b9a\u4f1a\u7531\u4e8e\u5404\u79cd\u539f\u56e0\u5bfc\u81f4\u5f88\u591a\u6570\u636e\u4f20\u8f93\u62a5\u9519(\u6bd4\u5982\u7c7b\u578b\u8f6c\u6362\u9519\u8bef)\uff0c\u8fd9\u79cd\u6570\u636e Addax \u8ba4\u4e3a\u5c31\u662f\u810f\u6570\u636e\u3002Addax \u76ee\u524d\u53ef\u4ee5\u5b9e\u73b0\u810f\u6570\u636e\u7cbe\u786e\u8fc7\u6ee4\u3001\u8bc6\u522b\u3001\u91c7\u96c6\u3001\u5c55\u793a\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u591a\u79cd\u7684\u810f\u6570\u636e\u5904\u7406\u6a21\u5f0f\uff0c\u8ba9\u7528\u6237\u51c6\u786e\u628a\u63a7\u6570\u636e\u8d28\u91cf\u5173\uff01</p>"},{"location":"#_8","title":"\u4e30\u5bcc\u7684\u6570\u636e\u8f6c\u6362\u529f\u80fd","text":"<p>\u4f5c\u4e3a\u4e00\u4e2a\u670d\u52a1\u4e8e\u5927\u6570\u636e\u7684 ETL \u5de5\u5177\uff0c\u9664\u4e86\u63d0\u4f9b\u6570\u636e\u5feb\u7167\u642c\u8fc1\u529f\u80fd\u4e4b\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e30\u5bcc\u6570\u636e\u8f6c\u6362\u7684\u529f\u80fd\uff0c\u8ba9\u6570\u636e\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u8f7b\u677e\u5b8c\u6210\u6570\u636e\u8131\u654f\uff0c\u8865\u5168\uff0c\u8fc7\u6ee4\u7b49\u6570\u636e\u8f6c\u6362\u529f\u80fd\uff0c\u53e6\u5916\u8fd8\u63d0\u4f9b\u4e86\u81ea\u52a8 <code>groovy</code> \u51fd\u6570\uff0c\u8ba9\u7528\u6237\u81ea\u5b9a\u4e49\u8f6c\u6362\u51fd\u6570\u3002\u8be6\u60c5\u8bf7\u770b transformer \u8be6\u7ec6\u4ecb\u7ecd\u3002</p>"},{"location":"#_9","title":"\u7cbe\u51c6\u7684\u901f\u5ea6\u63a7\u5236","text":"<p>\u63d0\u4f9b\u4e86\u5305\u62ec\u901a\u9053(\u5e76\u53d1)\u3001\u8bb0\u5f55\u6d41\u3001\u5b57\u8282\u6d41\u4e09\u79cd\u6d41\u63a7\u6a21\u5f0f\uff0c\u53ef\u4ee5\u968f\u610f\u63a7\u5236\u4f60\u7684\u4f5c\u4e1a\u901f\u5ea6\uff0c\u8ba9\u4f60\u7684\u4f5c\u4e1a\u5728\u5e93\u53ef\u4ee5\u627f\u53d7\u7684\u8303\u56f4\u5185\u8fbe\u5230\u6700\u4f73\u7684\u540c\u6b65\u901f\u5ea6\u3002</p> <pre><code>{\n  \"speed\": {\n    \"channel\": 5,\n    \"byte\": 1048576,\n    \"record\": 10000\n  }\n}\n</code></pre>"},{"location":"#_10","title":"\u5f3a\u52b2\u5730\u540c\u6b65\u6027\u80fd","text":"<p>\u6bcf\u4e00\u79cd\u8bfb\u63d2\u4ef6\u90fd\u6709\u4e00\u79cd\u6216\u591a\u79cd\u5207\u5206\u7b56\u7565\uff0c\u90fd\u80fd\u5c06\u4f5c\u4e1a\u5408\u7406\u5207\u5206\u6210\u591a\u4e2a Task \u5e76\u884c\u6267\u884c\uff0c\u5355\u673a\u591a\u7ebf\u7a0b\u6267\u884c\u6a21\u578b\u53ef\u4ee5\u8ba9\u901f\u5ea6\u968f\u5e76\u53d1\u6210\u7ebf\u6027\u589e\u957f\u3002 \u5728\u6e90\u7aef\u548c\u76ee\u7684\u7aef\u6027\u80fd\u90fd\u8db3\u591f\u7684\u60c5\u51b5\u4e0b\uff0c\u5355\u4e2a\u4f5c\u4e1a\u4e00\u5b9a\u53ef\u4ee5\u6253\u6ee1\u7f51\u5361\u3002</p>"},{"location":"#_11","title":"\u5065\u58ee\u7684\u5bb9\u9519\u673a\u5236","text":"<p>\u4f5c\u4e1a\u662f\u6781\u6613\u53d7\u5916\u90e8\u56e0\u7d20\u7684\u5e72\u6270\uff0c\u7f51\u7edc\u95ea\u65ad\u3001\u6570\u636e\u6e90\u4e0d\u7a33\u5b9a\u7b49\u56e0\u7d20\u5f88\u5bb9\u6613\u8ba9\u540c\u6b65\u5230\u4e00\u534a\u7684\u4f5c\u4e1a\u62a5\u9519\u505c\u6b62\u3002\u56e0\u6b64\u7a33\u5b9a\u6027\u662f Addax \u7684\u57fa\u672c\u8981\u6c42\uff0c\u5728 Addax \u7684\u8bbe\u8ba1\u4e2d\uff0c\u91cd\u70b9\u5b8c\u5584\u4e86\u6846\u67b6\u548c\u63d2\u4ef6\u7684\u7a33\u5b9a\u6027\u3002 \u76ee\u524d Addax \u53ef\u4ee5\u505a\u5230\u7ebf\u7a0b\u7ea7\u522b\u3001\u4f5c\u4e1a\u7ea7\u522b\u591a\u5c42\u6b21\u5c40\u90e8/\u5168\u5c40\u7684\u91cd\u8bd5\uff0c\u4fdd\u8bc1\u7528\u6237\u7684\u4f5c\u4e1a\u7a33\u5b9a\u8fd0\u884c\u3002</p>"},{"location":"commandline/","title":"\u547d\u4ee4\u884c\u5de5\u5177","text":"<p>\u8fd9\u91cc\u4ecb\u7ecd addax.sh \u547d\u4ee4\u884c\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5\u3002</p>"},{"location":"commandline/#_2","title":"\u547d\u4ee4\u884c\u5de5\u5177","text":"<p>Addax \u63d0\u4f9b\u4e86\u4e00\u4e2a\u547d\u4ee4\u884c\u5de5\u5177 <code>addax.sh</code>\uff0c\u7528\u4e8e\u6267\u884c Addax \u4f5c\u4e1a\u3002\u8be5\u5de5\u5177\u4f4d\u4e8e Addax \u5b89\u88c5\u76ee\u5f55\u7684 <code>bin</code> \u76ee\u5f55\u4e0b\u3002</p>"},{"location":"commandline/#_3","title":"\u4f7f\u7528\u65b9\u6cd5","text":"<p>\u6267\u884c Addax \u4f5c\u4e1a\u7684\u57fa\u672c\u547d\u4ee4\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>bin/addax.sh &lt;job_file&gt;\n</code></pre> <p>\u5176\u4e2d <code>&lt;job_file&gt;</code> \u662f\u4e00\u4e2a JSON \u683c\u5f0f\u7684\u4f5c\u4e1a\u914d\u7f6e\u6587\u4ef6\uff0c\u5305\u542b\u4e86\u6570\u636e\u6e90\u7684\u8bfb\u53d6\u548c\u5199\u5165\u914d\u7f6e\u3002</p>"},{"location":"commandline/#_4","title":"\u53c2\u6570","text":"<p><code>addax.sh</code> \u547d\u4ee4\u884c\u5de5\u5177\u652f\u6301\u4ee5\u4e0b\u53c2\u6570\uff1a</p> <ul> <li><code>-h</code>, <code>--help</code>: \u663e\u793a\u5e2e\u52a9\u4fe1\u606f\u3002</li> <li><code>-v</code>, <code>--version</code>: \u663e\u793a Addax \u7684\u7248\u672c\u4fe1\u606f\u3002</li> <li><code>-l</code>, <code>--log</code>: \u6307\u5b9a\u65e5\u5fd7\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u662f\u5728 <code>$ADDAX_HOME/log</code> \u76ee\u5f55\u4e0b\u3002</li> <li><code>-d</code>, <code>--debug</code>: \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f\uff0c\u8be6\u89c1 \u8c03\u8bd5\u6a21\u5f0f\u3002</li> <li><code>-L</code>, <code>--log-level</code>: \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\uff0c\u652f\u6301 <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code> \u7b49\u7ea7\u522b\uff0c\u9ed8\u8ba4\u662f <code>INFO</code>\u3002</li> <li><code>-j</code>, <code>--jvm</code>: \u6307\u5b9a JVM \u53c2\u6570\uff0c\u53ef\u4ee5\u4f20\u9012\u7ed9 Java \u865a\u62df\u673a\u7684\u53c2\u6570\u3002</li> <li><code>-p</code>, <code>--params</code> : \u4f20\u9012\u989d\u5916\u7684\u53c2\u6570\u7ed9\u4f5c\u4e1a\u914d\u7f6e\u6587\u4ef6\uff0c\u53ef\u4ee5\u5728\u4f5c\u4e1a\u914d\u7f6e\u4e2d\u4f7f\u7528 <code>${param}</code> \u7684\u65b9\u5f0f\u5f15\u7528\u8fd9\u4e9b\u53c2\u6570\u3002</li> </ul>"},{"location":"commandline/#params","title":"params \u53c2\u6570\u8bf4\u660e","text":"<p><code>-p</code>, <code>--params</code> \u53c2\u6570\u7528\u6765\u5411\u8bbe\u7f6e\u67d0\u4e9b\u914d\u7f6e\u4efb\u52a1\u7684\u53c2\u6570\uff0c\u4f7f\u5f97\u4f5c\u4e1a\u914d\u7f6e\u6587\u4ef6\u53ef\u4ee5\u66f4\u52a0\u7075\u6d3b\u3002\u53ef\u4ee5\u5728\u4f5c\u4e1a\u914d\u7f6e\u6587\u4ef6\u4e2d\u4f7f\u7528 <code>${param}</code> \u7684\u65b9\u5f0f\u5f15\u7528\u8fd9\u4e9b\u53c2\u6570\u3002</p> <p>\u5047\u5b9a\u4f5c\u4e1a\u914d\u7f6e\u6587\u4ef6\u4e2d\u6709\u5982\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>{\n  \"job\": {\n    \"settings\": {\n      \"param1\": \"${param1}\",\n      \"param2\": \"${param2}\"\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mysqlreader\",\n        \"parameter\": {\n          \"username\": \"${username}\",\n          \"password\": \"${password}\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"mysqlwriter\",\n        \"parameter\": {\n          \"username\": \"${username}\",\n          \"password\": \"${password}\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u6267\u884c\u547d\u4ee4\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>-p</code> \u53c2\u6570\u4f20\u9012\u53c2\u6570\uff1a</p> <pre><code>bin/addax.sh job/test.json -p \"-Dusername=root -Dpassword=123456 -Dparam1=value1 -Dparam2=value2\"\n</code></pre> <p>\u4e3a\u4e86\u65b9\u4fbf\u4f7f\u7528\uff0c\u7a0b\u5e8f\u5185\u7f6e\u4e86\u90e8\u5206\u5e38\u89c1\u65f6\u95f4\u53d8\u91cf\uff0c\u53ef\u4ee5\u76f4\u63a5\u3002\u5047\u5b9a\u5f53\u524d\u65f6\u95f4\u662f <code>2025-07-16 12:13:14</code>\uff0c\u4e0b\u9762\u5217\u51fa\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u7684\u53d8\u91cf\u4ee5\u53ca\u53d8\u91cf\u503c\uff1a</p> \u53d8\u91cf\u540d \u53d8\u91cf\u503c <code>${curr_date_short}</code> <code>20250716</code> <code>${curr_date_dash}</code> <code>2025-07-16</code> <code>$curr_datetime_short}</code> <code>20250716121314</code> <code>${curr_datetime_dash}</code> <code>2025-07-16 12:13:14</code> <code>${biz_date_short}</code> <code>20250715</code> <code>${biz_date_dash}</code> <code>2025-07-15</code> <code>${biz_datetime_short}</code> <code>20250715121314</code> <code>${biz_datetime_dash}</code> <code>2025-07-15 12:13:14</code> <code>${biz_datetime_0_short}</code> <code>20250715000000</code> <code>${biz_datetime_0_dash}</code> <code>2025-07-15 00:00:00</code>"},{"location":"debug/","title":"\u5982\u4f55\u8c03\u8bd5\u9879\u76ee","text":"<p>\u5982\u679c\u4f60\u60f3\u5728\u672c\u5730\u4fee\u6539\u6216\u8005\u8c03\u8bd5\u4ee3\u7801\uff0c\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u65b9\u5f0f\uff0c\u4ee5\u4e0b\u7684\u64cd\u4f5c\u5747\u4ee5 Intellij IDEA \u5f00\u53d1\u5de5\u5177\u4e3a\u4f8b\u3002</p> <p>\u8c03\u8bd5\u53ef\u4ee5\u5206\u6210\u4e24\u65b9\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f\u4ee3\u7801\u548c\u4e8c\u8fdb\u5236\u5305\u90fd\u5728\u672c\u5730\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u672c\u5730\u8c03\u8bd5\uff1b\u53e6\u5916\u4e00\u79cd\u6e90\u4ee3\u7801\u5728\u672c\u5730\uff0c\u4f46\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u5df2\u7ecf\u90e8\u7f72\u5728\u8fdc\u7a0b\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6211\u4eec\u79f0\u4e4b\u4e3a\u8fdc\u7a0b\u8c03\u8bd5\u3002</p> <p>\u4e0b\u9762\u5206\u522b\u63cf\u8ff0</p>"},{"location":"debug/#_2","title":"\u672c\u5730\u8c03\u8bd5","text":""},{"location":"debug/#_3","title":"\u4e00\u4e9b\u8bbe\u5b9a","text":"<p>\u6211\u4eec\u5047\u5b9a\u672c\u5730\u90e8\u7f72\u7684 <code>Addax</code> \u5728 <code>/opt/app/addax/4.0.3</code> \u6587\u4ef6\u5939\u4e0b\u3002\u5176 <code>job</code> \u76ee\u5f55\u4e0b\u6709\u8fd9\u6837\u7684\u4e00\u4e2a <code>job.json</code> \u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/job.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u4e0a\u8ff0 job \u6587\u4ef6\u8fd0\u884c\u6ca1\u6709\u7b26\u5408\u6211\u4eec\u7684\u9884\u671f\uff0c\u731c\u6d4b\u662f <code>streamreader</code> \u8fd9\u4e2a\u63d2\u4ef6\u7684 <code>parseMixupFunctions</code> \u51fd\u6570\u6709\u95ee\u9898\uff0c\u6211\u60f3\u8c03\u8bd5\u770b\u5177\u4f53\u95ee\u9898\u5728\u54ea\u91cc\u3002</p>"},{"location":"debug/#idea","title":"\u914d\u7f6e IDEA","text":"<p>\u6253\u5f00 IDEA\u5de5\u5177\uff0c\u5e76\u6253\u5f00 <code>addax</code> \u9879\u76ee\u6e90\u4ee3\u7801\uff0c\u6253\u5f00 <code>plugin/reader/streamreader/StreamReader.java</code> \u6587\u4ef6\uff0c\u627e\u5230 <code>parseMixupFunctions</code> \u51fd\u6570\uff0c\u5e76\u5728\u51fd\u6570\u7533\u660e\u5904\u7684\u70b9\u51fb\u5de6\u4fa7\u8fb9\u7f18\u5904\u589e\u52a0\u65ad\u70b9\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> <p>\u70b9\u51fb <code>IDEA</code> \u7684 <code>Run-&gt;Edit Configurations...</code> \u83dc\u5355\uff0c\u5728\u5f39\u51fa\u7684 <code>Run/Debug Configurations</code> \u7a97\u53e3\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u7684 <code>+</code> \u6309\u94ae\uff0c\u7136\u540e\u9009\u62e9 \u9009\u62e9 <code>Application</code> \uff0c\u5728\u53f3\u4fa7\u914d\u7f6e\u6846\u4e2d\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f\u5982\u4e0b\uff1a</p> <ul> <li>Name: \u8c03\u8bd5\u63cf\u8ff0\u540d\u79f0\uff0c\u8fd9\u91cc\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u597d\u586b\u5199</li> <li>Run on: \u9009\u62e9 <code>Local machine</code></li> <li>Build and run:</li> <li>\u7b2c\u4e00\u4e2a\u6846: \u9009\u62e9 JDK \u7248\u672c\uff0c\u5f53\u524d\u652f\u6301 17 \u7248\u672c\uff0c\u5efa\u8bae\u9009\u62e9 17 \u7248\u672c</li> <li>\u7b2c\u4e8c\u4e2a\u6846: \u9009\u62e9 <code>addax-core</code> \u6a21\u5757</li> <li><code>Main class</code>: \u586b\u5199 <code>com.wgzhao.addax.core.Engine</code></li> <li>\u70b9\u51fb <code>Modify options</code>\uff0c\u5728\u5f39\u51fa\u7684\u4e0b\u62c9\u6846\u4e2d\uff0c\u9009\u62e9 <code>Add VM Options</code>\uff0c\u5728\u589e\u52a0\u7684 <code>VM Options</code> \u4e2d\uff0c\u586b\u5199 <code>-Daddax.home=/opt/app/addax/4.0.3</code>\u3002\u5982\u679c\u4f60\u7684reader\u6216\u8005writer\u4f9d\u8d56\u4e86lib\u4e2d\u7684\u4ee3\u7801\uff0c\u5219\u8fd8\u9700\u8981\u6dfb\u52a0 <code>-classpath .:/opt/app/addax/4.0.3/lib/*</code>\uff0c\u5982\u679c\u6ca1\u6709\u4f9d\u8d56\uff0c\u5219\u4e0d\u9700\u8981\u6dfb\u52a0\u3002</li> <li><code>Program arguments</code>: \u586b\u5199 <code>-job job/job.json</code></li> <li><code>Working directory</code>\uff1a \u586b\u5199 <code>/opt/app/addax/4.0.3</code></li> </ul> <p>\u5176\u4ed6\u4fdd\u6301\u4e0d\u53d8\uff0c\u70b9\u51fb <code>Apply</code> \u6309\u94ae\u3002\u5f97\u5230\u7c7b\u4f3c\u4e0b\u56fe\u914d\u7f6e\u5185\u5bb9</p> <p></p> <p>\u70b9\u51fb <code>OK</code> \u6309\u94ae\u4fdd\u5b58\u4e0a\u8ff0\u914d\u7f6e\uff0c\u56de\u5230 <code>IDEA</code> \u4e3b\u7a97\u53e3\uff0c\u5728\u7a97\u53e3\u83dc\u5355\u680f\u6709\u7eff\u8272<code>\ud83d\udd28</code>\u7684\u53f3\u4fa7\uff0c\u5e94\u8be5\u53ef\u4ee5\u770b\u5230\u521a\u624d\u914d\u7f6e\u7684\u63cf\u8ff0\u6587\u4ef6\uff0c\u7c7b\u4f3c\u4e0b\u56fe\uff1a</p> <p></p> <p>\u70b9\u51fb\u4e0a\u8ff0\u622a\u56fe\u4e2d\u7684\u7eff\u8272 DEBUG \u5c0f\u866b\u6309\u94ae\uff0c\u8fdb\u5165\u8c03\u8bd5\uff0c\u5f97\u5230\u7c7b\u4f3c\u4e0b\u56fe\u7684\u8c03\u8bd5\u7a97\u53e3\uff1a</p> <p></p>"},{"location":"debug/#_4","title":"\u8fdc\u7a0b\u8c03\u8bd5","text":""},{"location":"debug/#_5","title":"\u4e00\u4e9b\u5047\u5b9a","text":"<p>\u5047\u5b9a\u7a0b\u5e8f\u90e8\u7f72\u5728\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u8bd5\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u7684\u7a0b\u5e8f\uff0c\u5047\u5b9a\u8fdc\u7a0b\u670d\u52a1\u5668IP\u5730\u5740\u4e3a <code>192.168.1.100</code>\uff0c\b<code>Addax</code> \u90e8\u7f72\u5728 <code>/opt/addax/4.0.3</code> \u76ee\u5f55\u4e0b\uff0c\u5176 <code>job</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u4e5f\u6709\u4e00\u4e2a\u548c\u672c\u5730\u8c03\u8bd5\u4e2d\u63cf\u8ff0\u7684 <code>job.json</code> \u6587\u4ef6\u3002 \u540c\u6837\u7684\uff0c\u4e0a\u8ff0 job \u6587\u4ef6\u8fd0\u884c\u6ca1\u6709\u7b26\u5408\u6211\u4eec\u7684\u9884\u671f\uff0c\u731c\u6d4b\u662f <code>streamreader</code> \u8fd9\u4e2a\u63d2\u4ef6\u7684 <code>parseMixupFunctions</code> \u51fd\u6570\u6709\u95ee\u9898\uff0c\u6211\u60f3\u8c03\u8bd5\u770b\u5177\u4f53\u95ee\u9898\u5728\u54ea\u91cc\u3002</p> <p>\u6ce8\u610f\uff1a\u8fdc\u7a0b\u8c03\u8bd5\u9700\u8981\u5728\u670d\u52a1\u5668\u6253\u5f00 <code>9999</code> \u7aef\u53e3\uff0c\u56e0\u6b64\u8981\u4fdd\u8bc1\u670d\u52a1\u5668\u4e0a\u7684 <code>9999</code> \u7aef\u53e3\u6ca1\u6709\u88ab\u5360\u7528\uff0c\u5982\u679c\u88ab\u5360\u7528\uff0c\u5219\u9700\u8981\u4fee\u6539\u6b64\u7aef\u53e3\u3002</p> <p>\u4fee\u6539\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <ol> <li>\u6253\u5f00 <code>bin/addax.sh</code> \u811a\u672c</li> <li>\u5b9a\u4f4d\u5230\u5927\u7ea624\u884c\u5904\uff0c\u627e\u5230 <code>address=0.0.0.0:9999</code> \u5b57\u7b26\u4e32</li> <li>\u5c06 <code>9999</code> \u4fee\u6539\u6210\u5176\u4ed6\u672a\u88ab\u5360\u7528\u7684\u7aef\u53e3</li> <li>\u4fdd\u5b58\u9000\u51fa</li> </ol>"},{"location":"debug/#idea_1","title":"\u914d\u7f6e IDEA","text":"<p>\u6253\u5f00 IDEA\u5de5\u5177\uff0c\u5e76\u6253\u5f00 <code>addax</code> \u9879\u76ee\u6e90\u4ee3\u7801\uff0c\u6253\u5f00 <code>plugin/reader/streamreader/StreamReader.java</code> \u6587\u4ef6\uff0c\u627e\u5230 <code>parseMixupFunctions</code> \u51fd\u6570\uff0c\u5e76\u5728\u51fd\u6570\u7533\u660e\u5904\u7684\u70b9\u51fb\u5de6\u4fa7\u8fb9\u7f18\u5904\u589e\u52a0\u65ad\u70b9\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> <p>\u70b9\u51fb <code>IDEA</code> \u7684 <code>Run-&gt;Edit Configurations...</code> \u83dc\u5355\uff0c\u5728\u5f39\u51fa\u7684 <code>Run/Debug Configurations</code> \u7a97\u53e3\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u7684 <code>+</code> \u6309\u94ae\uff0c\u7136\u540e\u9009\u62e9 \u9009\u62e9 <code>Remove JVM Debug</code> \uff0c\u5728\u53f3\u4fa7\u914d\u7f6e\u6846\u4e2d\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f\u5982\u4e0b\uff1a</p> <ul> <li>Name: \u8c03\u8bd5\u63cf\u8ff0\u540d\u79f0\uff0c\u8fd9\u91cc\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u597d\u586b\u5199</li> <li>Configuration:</li> <li>Host: \u586b\u5199\u8fdc\u7a0b\u670d\u52a1\u5668IP\u5730\u5740\uff0c\u8fd9\u91cc\u586b\u5199 <code>192.168.1.100</code></li> <li>Port: \u586b\u5199\u8c03\u8bd5\u7aef\u53e3\uff0c\u8fd9\u91cc\u586b\u5199 <code>9999</code> \u6216\u8005\u4f60\u4fee\u6539\u8fc7\u7684\u7aef\u53e3</li> </ul> <p>\u5176\u4ed6\u4fdd\u6301\u4e0d\u53d8\uff0c\u70b9\u51fb <code>Apply</code> \u6309\u94ae\uff0c\u5f97\u5230\u5982\u4e0b\u914d\u7f6e\u4fe1\u606f\uff1a</p> <p></p> <p>\u70b9\u51fb <code>OK</code> \u6309\u94ae\u4fdd\u5b58\u5e76\u8fd4\u56de\u5230 <code>IDEA</code> \u4e3b\u7a97\u53e3</p> <p>\u786e\u4fdd\u5728\u7a97\u53e3\u5de5\u5177\u680f\u6709\u7eff\u8272<code>\ud83d\udd28</code>\u7684\u53f3\u4fa7\u9009\u62e9\u7684\u662f\u4e0a\u8ff0\u586b\u5199 <code>Name</code> \u7684\u63cf\u8ff0\u914d\u7f6e\uff0c\u5426\u5219\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\u521a\u624d\u7684\u914d\u7f6e\u3002</p> <p></p>"},{"location":"debug/#_6","title":"\u8fd0\u884c\u8c03\u8bd5","text":"<p>\u8fd0\u884c\u8fdc\u7a0b\u8c03\u8bd5\u5206\u6210\u4e24\u4e2a\u6b65\u9aa4\uff0c\u4e00\u662f\u542f\u52a8\u7a0b\u5e8f\uff0c\u4e8c\u662f\u8c03\u8bd5\u5de5\u5177\u8054\u63a5\u5230\u8fd0\u884c\u7684\u7a0b\u5e8f\u4e0a\u3002</p> <p>\u5728\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\uff1a</p> <p><code>bin/addax.sh -d job/job.json</code></p> <p>\u5982\u679c\u8fd0\u884c\u6b63\u5e38\uff0c\u4f1a\u5f97\u5230\u5982\u4e0b\u4fe1\u606f\uff1a</p> <pre><code>bin/addax.sh -d job/job.json\nListening for transport dt_socket at address: 9999\n</code></pre> <p>\u8868\u793a\u7a0b\u5e8f\u76d1\u542c\u5728 <code>9999</code> \u7aef\u53e3\u4e0a\uff0c\u7b49\u5f85\u8054\u63a5\u3002</p> <p>\u8fd4\u56de <code>IDEA</code> \u7a97\u53e3\uff0c\u70b9\u51fb\u5de5\u5177\u680f\u4e0a\u7eff\u8272 DEBUG \u5c0f\u866b\u6309\u94ae\uff0c\u5f00\u59cb\u8c03\u8bd5\uff0c\u5982\u679c\u8fd0\u884c\u6b63\u5e38\uff0c\u4f1a\u5f97\u5230\u7c7b\u4f3c\u4e0b\u56fe\u7684\u8c03\u8bd5\u7a97\u53e3\uff1a</p> <p></p>"},{"location":"encrypt_password/","title":"\u52a0\u5bc6\u914d\u7f6e\u6587\u4ef6\u7684\u4e2d\u5bc6\u7801","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u914d\u7f6e\u6587\u4ef6\u7684\u5bc6\u7801\u662f\u660e\u6587\u7684\uff0c\u8fd9\u5e26\u6765\u4e86\u4e00\u5b9a\u7684\u5b89\u5168\u9690\u60a3\uff0c\u4ece <code>4.0.9</code> \u7248\u672c\u8d77\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u5bf9\u914d\u7f6e\u6587\u4ef6\u7684\u7684\u5bc6\u7801\u52a0\u5bc6\u529f\u80fd\u3002 \u540c\u65f6\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a <code>shell</code> \u811a\u672c <code>encrypt_password.sh</code> \u6765\u5e2e\u52a9\u4f60\u52a0\u5bc6\u914d\u7f6e\u6587\u4ef6\u7684\u5bc6\u7801\u3002</p> <p>\u5047\u5b9a\u4f60\u7684\u539f\u59cb\u5bc6\u7801\u662f <code>123456</code>\uff0c\u4f60\u5e0c\u671b\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4f7f\u7528\u52a0\u5bc6\u5bc6\u7801\u914d\u7f6e\u3002 \u9996\u5148\u6267\u884c\u4e0b\u9762\u7684\u6307\u4ee4\uff1a</p> <pre><code>$ bin/encrypt_password.sh 123456\nThe encrypt string is : '${enc:tFd05jnm1mSq+PEK9t/Rgg==}', you can paste it to json file.\n</code></pre> <p>\u4e0a\u8ff0\u7ed3\u679c\u4e2d\u7684 <code>tFd05jnm1mSq+PEK9t/Rgg==</code> \u4e3a <code>123456</code> \u7684\u5bc6\u6587\u3002 <code>${enc:</code> \u5f00\u5934\u662f\u4e3a\u4e86\u8ba9 <code>addax</code> \u77e5\u9053\u8fd9\u662f\u4e00\u4e2a\u52a0\u5bc6\u5bc6\u6587\u3002</p> <p>\u4f60\u5c06\u4e0a\u8ff0\u5b57\u7b26\u4e32 <code>${enc:tFd05jnm1mSq+PEK9t/Rgg==}</code> \u66ff\u6362\u4f60\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e\u5bc6\u7801\u4e3a <code>123456</code> \u7684\u5730\u65b9\u5373\u53ef\u3002</p>"},{"location":"plugin_development/","title":"\u63d2\u4ef6\u5f00\u53d1","text":"<p>\u672c\u6307\u5357\u4e3b\u8981\u9762\u5411\u90a3\u4e9b\u9700\u8981\u5f00\u53d1\u7b26\u5408\u81ea\u5df1\u9700\u6c42\u7684 Addax \u63d2\u4ef6\u5f00\u53d1\u4eba\u5458\u3002</p>"},{"location":"plugin_development/#addax","title":"Addax \u6d41\u7a0b","text":"<p>Addax \u8fd0\u884c\u4e00\u4e2a\u4efb\u52a1\u7684\u5927\u81f4\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <p></p> <p>\u542f\u52a8\u6b65\u9aa4\u4e3a\uff1a</p> <ol> <li>\u89e3\u6790\u914d\u7f6e\uff0c\u5305\u62ec <code>job.json</code>\u3001<code>core.json</code>\u3001<code>plugin.json</code> \u4e09\u4e2a\u914d\u7f6e</li> <li>\u8bbe\u7f6e <code>jobId</code> \u5230 <code>configuration</code> \u5f53\u4e2d</li> <li>\u542f\u52a8 Engine\uff0c\u901a\u8fc7 <code>Engine.start()</code> \u8fdb\u5165\u542f\u52a8\u7a0b\u5e8f</li> <li>\u8bbe\u7f6e <code>RUNTIME_MODE</code> \u5230 <code>configuration</code> \u5f53\u4e2d</li> <li>\u901a\u8fc7 JobContainer \u7684 <code>start()</code> \u65b9\u6cd5\u542f\u52a8</li> <li>\u4f9d\u6b21\u6267\u884c job \u7684 <code>preHandler()</code>\u3001<code>init()</code>\u3001<code>prepare()</code>\u3001<code>split()</code>\u3001<code>schedule()</code>\u3001<code>post()</code>\u3001<code>postHandle()</code> \u7b49\u65b9\u6cd5\u3002</li> <li><code>init()</code> \u65b9\u6cd5\u6d89\u53ca\u5230\u6839\u636e configuration \u6765\u521d\u59cb\u5316 reader \u548c writer \u63d2\u4ef6\uff0c\u8fd9\u91cc\u6d89\u53ca\u5230 jar \u5305\u70ed\u52a0\u8f7d\u4ee5\u53ca\u8c03\u7528\u63d2\u4ef6 <code>init()</code> \u64cd\u4f5c\u65b9\u6cd5\uff0c\u540c\u65f6\u8bbe\u7f6e reader \u548c writer \u7684 configuration \u4fe1\u606f</li> <li><code>prepare()</code> \u65b9\u6cd5\u6d89\u53ca\u5230\u521d\u59cb\u5316 reader \u548c writer \u63d2\u4ef6\u7684\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u8c03\u7528\u63d2\u4ef6\u7684 <code>prepare()</code> \u65b9\u6cd5\u5b9e\u73b0\uff0c\u6bcf\u4e2a\u63d2\u4ef6\u90fd\u6709\u81ea\u5df1\u7684 jarLoader\uff0c\u901a\u8fc7\u96c6\u6210 <code>URLClassloader</code> \u5b9e\u73b0\u800c\u6765</li> <li><code>split()</code> \u65b9\u6cd5\u901a\u8fc7 <code>adjustChannelNumber()</code> \u65b9\u6cd5\u8c03\u6574 channel \u4e2a\u6570\uff0c\u540c\u65f6\u6267\u884c reader \u548c writer \u6700\u7ec6\u7c92\u5ea6\u7684\u5207\u5206\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cwriter \u7684\u5207\u5206\u7ed3\u679c\u8981\u53c2\u7167 reader    \u7684\u5207\u5206\u7ed3\u679c\uff0c\u8fbe\u5230\u5207\u5206\u540e\u6570\u76ee\u76f8\u7b49\uff0c\u624d\u80fd\u6ee1\u8db3 1\uff1a1 \u7684\u901a\u9053\u6a21\u578b</li> <li>channel \u7684\u8ba1\u6570\u4e3b\u8981\u662f\u6839\u636e byte \u548c record \u7684\u9650\u901f\u6765\u5b9e\u73b0\u7684\uff0c\u5728 <code>split()</code> \u7684\u51fd\u6570\u4e2d\u7b2c\u4e00\u6b65\u5c31\u662f\u8ba1\u7b97 channel \u7684\u5927\u5c0f</li> <li><code>split()</code> \u65b9\u6cd5 reader \u63d2\u4ef6\u4f1a\u6839\u636e channel \u7684\u503c\u8fdb\u884c\u62c6\u5206\uff0c\u4f46\u662f\u6709\u4e9b reader \u63d2\u4ef6\u53ef\u80fd\u4e0d\u4f1a\u53c2\u8003 channel \u7684\u503c\uff0cwriter \u63d2\u4ef6\u4f1a\u5b8c\u5168\u6839\u636e reader \u7684\u63d2\u4ef6 1:1 \u8fdb\u884c\u8fd4\u56de</li> <li><code>split()</code> \u65b9\u6cd5\u5185\u90e8\u7684 <code>mergeReaderAndWriterTaskConfigs()</code> \u8d1f\u8d23\u5408\u5e76 reader\u3001writer\u3001\u4ee5\u53ca transformer \u4e09\u8005\u5173\u7cfb\uff0c\u751f\u6210 task \u7684\u914d\u7f6e\uff0c\u5e76\u4e14\u91cd\u5199 <code>job.content</code> \u7684\u914d\u7f6e</li> <li><code>schedule()</code> \u65b9\u6cd5\u6839\u636e <code>split()</code> \u62c6\u5206\u751f\u6210\u7684 task \u914d\u7f6e\u5206\u914d\u751f\u6210 taskGroup \u5bf9\u8c61\uff0c\u6839\u636e task \u7684\u6570\u91cf\u548c\u5355\u4e2a taskGroup \u652f\u6301\u7684 task \u6570\u91cf\u8fdb\u884c\u914d\u7f6e\uff0c\u4e24\u8005\u76f8\u9664\u5c31\u53ef\u4ee5\u5f97\u51fa taskGroup \u7684\u6570\u91cf</li> <li><code>schedule()</code> \u5185\u90e8\u901a\u8fc7 AbstractScheduler \u7684 <code>schedule()</code> \u6267\u884c\uff0c\u7ee7\u7eed\u6267\u884c<code>startAllTaskGroup()</code> \u65b9\u6cd5\u521b\u5efa\u6240\u6709\u7684 TaskGroupContainer \u7ec4\u7ec7\u76f8\u5173\u7684     task\uff0cTaskGroupContainerRunner \u8d1f\u8d23\u8fd0\u884c TaskGroupContainer \u6267\u884c\u5206\u914d\u7684 task\u3002scheduler \u7684\u5177\u4f53\u5b9e\u73b0\u7c7b\u4e3a <code>ProcessInnerScheduler</code>\u3002</li> <li><code>taskGroupContainerExecutorService</code> \u542f\u52a8\u56fa\u5b9a\u7684\u7ebf\u7a0b\u6c60\u7528\u4ee5\u6267\u884c <code>TaskGroupContainerRunner</code> \u5bf9\u8c61\uff0cTaskGroupContainerRunner \u7684 <code>run()</code> \u65b9\u6cd5\u8c03\u7528 <code>taskGroupContainer. start()</code> \u65b9\u6cd5\uff0c\u9488\u5bf9\u6bcf\u4e2a channel \u521b\u5efa\u4e00\u4e2a TaskExecutor\uff0c\u901a\u8fc7 <code>taskExecutor.doStart()</code> \u542f\u52a8\u4efb\u52a1\u3002</li> </ol>"},{"location":"plugin_development/#_2","title":"\u63d2\u4ef6\u673a\u5236","text":"<p><code>Addax</code> \u4e3a\u4e86\u5e94\u5bf9\u4e0d\u540c\u6570\u636e\u6e90\u7684\u5dee\u5f02\u3001\u540c\u65f6\u63d0\u4f9b\u4e00\u81f4\u5730\u540c\u6b65\u539f\u8bed\u548c\u6269\u5c55\u80fd\u529b\uff0c\u91c7\u7528\u4e86 <code>\u6846\u67b6</code> + <code>\u63d2\u4ef6</code> \u7684\u6a21\u5f0f\uff1a</p> <ul> <li>\u63d2\u4ef6\u53ea\u9700\u5173\u5fc3\u6570\u636e\u7684\u8bfb\u53d6\u6216\u8005\u5199\u5165\u672c\u8eab\u3002</li> <li>\u800c\u540c\u6b65\u7684\u5171\u6027\u95ee\u9898\uff0c\u6bd4\u5982\uff1a\u7c7b\u578b\u8f6c\u6362\u3001\u6027\u80fd\u3001\u7edf\u8ba1\uff0c\u5219\u4ea4\u7531\u6846\u67b6\u6765\u5904\u7406\u3002</li> </ul> <p>\u4f5c\u4e3a\u63d2\u4ef6\u5f00\u53d1\u4eba\u5458\uff0c\u5219\u9700\u8981\u5173\u6ce8\u4e24\u4e2a\u95ee\u9898\uff1a</p> <ol> <li>\u6570\u636e\u6e90\u672c\u8eab\u7684\u8bfb\u5199\u6570\u636e\u6b63\u786e\u6027\u3002</li> <li>\u5982\u4f55\u4e0e\u6846\u67b6\u6c9f\u901a\u3001\u5408\u7406\u6b63\u786e\u5730\u4f7f\u7528\u6846\u67b6\u3002</li> </ol>"},{"location":"plugin_development/#_3","title":"\u63d2\u4ef6\u89c6\u89d2\u770b\u6846\u67b6","text":""},{"location":"plugin_development/#_4","title":"\u903b\u8f91\u6267\u884c\u6a21\u578b","text":"<p>\u63d2\u4ef6\u5f00\u53d1\u8005\u4e0d\u7528\u5173\u5fc3\u592a\u591a\uff0c\u57fa\u672c\u53ea\u9700\u8981\u5173\u6ce8\u7279\u5b9a\u7cfb\u7edf\u8bfb\u548c\u5199\uff0c\u4ee5\u53ca\u81ea\u5df1\u7684\u4ee3\u7801\u5728\u903b\u8f91\u4e0a\u662f\u600e\u6837\u88ab\u6267\u884c\u7684\uff0c\u54ea\u4e00\u4e2a\u65b9\u6cd5\u662f\u5728\u4ec0\u4e48\u65f6\u5019\u88ab\u8c03\u7528\u7684\u3002\u5728\u6b64\u4e4b\u524d\uff0c\u9700\u8981\u660e\u786e\u4ee5\u4e0b\u6982\u5ff5\uff1a</p> <ul> <li><code>Job</code>: \u7528\u4ee5\u63cf\u8ff0\u4ece\u4e00\u4e2a\u6e90\u5934\u5230\u4e00\u4e2a\u76ee\u7684\u7aef\u7684\u540c\u6b65\u4f5c\u4e1a\uff0c\u662f\u6570\u636e\u540c\u6b65\u7684\u6700\u5c0f\u4e1a\u52a1\u5355\u5143\u3002\u6bd4\u5982\uff1a\u4ece\u4e00\u5f20 MySQL \u7684\u8868\u540c\u6b65\u5230 PostgreSQL \u7684\u4e00\u4e2a\u8868\u3002</li> <li><code>Task</code>: \u4e3a\u6700\u6027\u80fd\u5927\u5316\u800c\u628a <code>Job</code> \u62c6\u5206\u5f97\u5230\u7684\u6700\u5c0f\u6267\u884c\u5355\u5143\u3002\u6bd4\u5982\uff1a\u8bfb\u4e00\u5f20\u6709 1024 \u4e2a\u5206\u8868\u7684 MySQL \u5206\u5e93\u5206\u8868\u7684 <code>Job</code>\uff0c\u62c6\u5206\u6210 1024 \u4e2a\u8bfb <code>Task</code>\uff0c\u7528\u82e5\u5e72\u4e2a\u5e76\u53d1\u6267\u884c\u3002</li> <li><code>TaskGroup</code>: \u4e00\u7ec4 <code>Task</code> \u96c6\u5408\u3002\u5728\u540c\u4e00\u4e2a <code>TaskGroupContainer</code> \u6267\u884c\u4e0b\u7684 <code>Task</code> \u96c6\u5408\u79f0\u4e4b\u4e3a <code>TaskGroup</code></li> <li><code>JobContainer</code>: <code>Job</code> \u6267\u884c\u5668\uff0c\u8d1f\u8d23 <code>Job</code> \u5168\u5c40\u62c6\u5206\u3001\u8c03\u5ea6\u3001\u524d\u7f6e\u8bed\u53e5\u548c\u540e\u7f6e\u8bed\u53e5\u7b49\u5de5\u4f5c\u7684\u5de5\u4f5c\u5355\u5143\u3002\u7c7b\u4f3c Yarn \u4e2d\u7684 JobTracker</li> <li><code>TaskGroupContainer</code>: <code>TaskGroup</code> \u6267\u884c\u5668\uff0c\u8d1f\u8d23\u6267\u884c\u4e00\u7ec4 <code>Task</code> \u7684\u5de5\u4f5c\u5355\u5143\uff0c\u7c7b\u4f3c Yarn \u4e2d\u7684 TaskTracker\u3002</li> </ul> <p>\u7b80\u800c\u8a00\u4e4b\uff0c <code>Job</code>\u62c6\u5206\u6210<code>Task</code>\uff0c\u5206\u522b\u5728\u6846\u67b6\u63d0\u4f9b\u7684\u5bb9\u5668\u4e2d\u6267\u884c\uff0c\u63d2\u4ef6\u53ea\u9700\u8981\u5b9e\u73b0 <code>Job</code> \u548c <code>Task</code> \u4e24\u90e8\u5206\u903b\u8f91\u3002</p>"},{"location":"plugin_development/#_5","title":"\u7f16\u7a0b\u63a5\u53e3","text":"<p>\u90a3\u4e48\uff0c<code>Job</code> \u548c <code>Task</code> \u7684\u903b\u8f91\u5e94\u662f\u600e\u4e48\u5bf9\u5e94\u5230\u5177\u4f53\u7684\u4ee3\u7801\u4e2d\u7684\uff1f</p> <p>\u9996\u5148\uff0c\u63d2\u4ef6\u7684\u5165\u53e3\u7c7b\u5fc5\u987b\u6269\u5c55 <code>Reader</code> \u6216 <code>Writer</code> \u62bd\u8c61\u7c7b\uff0c\u5e76\u4e14\u5b9e\u73b0\u5206\u522b\u5b9e\u73b0 <code>Job</code> \u548c <code>Task</code> \u4e24\u4e2a\u5185\u90e8\u62bd\u8c61\u7c7b\uff0c<code>Job</code> \u548c <code>Task</code> \u7684\u5b9e\u73b0\u5fc5\u987b\u662f \u5185\u90e8\u7c7b \u7684\u5f62\u5f0f\uff0c\u539f\u56e0\u89c1 \u52a0\u8f7d\u539f\u7406 \u4e00\u8282\u3002\u4ee5 <code>Reader</code> \u4e3a\u4f8b\uff1a</p> <pre><code>public class SomeReader\n        extends Reader\n{\n    public static class Job\n            extends Reader.Job\n    {\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public List&lt;Configuration&gt; split(int adviceNumber)\n        {\n            return null;\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n\n    public static class Task\n            extends Reader.Task\n    {\n\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public void startRead(RecordSender recordSender)\n        {\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n}\n</code></pre> <p><code>Job</code> \u63a5\u53e3\u529f\u80fd\u5982\u4e0b\uff1a</p> <ul> <li><code>init</code>: Job \u5bf9\u8c61\u521d\u59cb\u5316\u5de5\u4f5c\uff0c\u6b64\u65f6\u53ef\u4ee5\u901a\u8fc7 <code>super.getPluginJobConf()</code> \u83b7\u53d6\u4e0e\u672c\u63d2\u4ef6\u76f8\u5173\u7684\u914d\u7f6e\u3002\u8bfb\u63d2\u4ef6\u83b7\u5f97\u914d\u7f6e\u4e2d <code>reader</code> \u90e8\u5206\uff0c\u5199\u63d2\u4ef6\u83b7\u5f97 <code>writer</code> \u90e8\u5206\u3002</li> <li><code>prepare</code>: \u5168\u5c40\u51c6\u5907\u5de5\u4f5c\uff0c\u6bd4\u5982 MySQL \u6e05\u7a7a\u76ee\u6807\u8868\u3002</li> <li><code>split</code>: \u62c6\u5206 <code>Task</code>\u3002\u53c2\u6570 <code>adviceNumber</code> \u6846\u67b6\u5efa\u8bae\u7684\u62c6\u5206\u6570\uff0c\u4e00\u822c\u662f\u8fd0\u884c\u65f6\u6240\u914d\u7f6e\u7684\u5e76\u53d1\u5ea6\u3002\u503c\u8fd4\u56de\u7684\u662f <code>Task</code> \u7684\u914d\u7f6e\u5217\u8868\u3002</li> <li><code>post</code>: \u5168\u5c40\u7684\u540e\u7f6e\u5de5\u4f5c\uff0c\u6bd4\u5982 MySQL writer \u540c\u6b65\u5b8c\u5f71\u5b50\u8868\u540e\u7684 <code>rename</code> \u64cd\u4f5c\u3002</li> <li><code>destroy</code>: Job \u5bf9\u8c61\u81ea\u8eab\u7684\u9500\u6bc1\u5de5\u4f5c\u3002</li> </ul> <p><code>Task</code> \u63a5\u53e3\u529f\u80fd\u5982\u4e0b\uff1a</p> <ul> <li><code>init</code>\uff1aTask \u5bf9\u8c61\u7684\u521d\u59cb\u5316\u3002\u6b64\u65f6\u53ef\u4ee5\u901a\u8fc7 <code>super.getPluginJobConf()</code> \u83b7\u53d6\u4e0e\u672c <code>Task</code> \u76f8\u5173\u7684\u914d\u7f6e\u3002\u8fd9\u91cc\u7684\u914d\u7f6e\u662f <code>Job#split</code> \u65b9\u6cd5\u8fd4\u56de\u7684\u914d\u7f6e\u5217\u8868\u4e2d\u7684\u5176\u4e2d\u4e00\u4e2a\u3002</li> <li><code>prepare</code>\uff1a\u5c40\u90e8\u7684\u51c6\u5907\u5de5\u4f5c\u3002</li> <li><code>startRead</code>: \u4ece\u6570\u636e\u6e90\u8bfb\u6570\u636e\uff0c\u5199\u5165\u5230 <code>RecordSender</code> \u4e2d\u3002<code>RecordSender</code> \u4f1a\u628a\u6570\u636e\u5199\u5165\u8fde\u63a5 <code>Reader</code> \u548c <code>Writer</code> \u7684\u7f13\u5b58\u961f\u5217\u3002</li> <li><code>startWrite</code>\uff1a\u4ece <code>RecordReceiver</code> \u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5199\u5165\u76ee\u6807\u6570\u636e\u6e90\u3002<code>RecordReceiver</code> \u4e2d\u7684\u6570\u636e\u6765\u81ea <code>Reader</code> \u548c <code>Writer</code> \u4e4b\u95f4\u7684\u7f13\u5b58\u961f\u5217\u3002</li> <li><code>post</code>: \u5c40\u90e8\u7684\u540e\u7f6e\u5de5\u4f5c\u3002</li> <li><code>destroy</code>: Task \u5bf9\u8c61\u81ea\u8eab\u7684\u9500\u6bc1\u5de5\u4f5c\u3002</li> </ul> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a</p> <ul> <li><code>Job</code> \u548c <code>Task</code> \u4e4b\u95f4\u4e00\u5b9a\u4e0d\u80fd\u6709\u5171\u4eab\u53d8\u91cf\uff0c\u56e0\u4e3a\u5206\u5e03\u5f0f\u8fd0\u884c\u65f6\u4e0d\u80fd\u4fdd\u8bc1\u5171\u4eab\u53d8\u91cf\u4f1a\u88ab\u6b63\u786e\u521d\u59cb\u5316\u3002\u4e24\u8005\u4e4b\u95f4\u53ea\u80fd\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u4f9d\u8d56\u3002</li> <li><code>prepare</code> \u548c <code>post</code> \u5728 <code>Job</code> \u548c <code>Task</code> \u4e2d\u90fd\u5b58\u5728\uff0c\u63d2\u4ef6\u9700\u8981\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u786e\u5b9a\u5728\u4ec0\u4e48\u5730\u65b9\u6267\u884c\u64cd\u4f5c\u3002</li> </ul> <p>\u6846\u67b6\u6309\u7167\u5982\u4e0b\u7684\u987a\u5e8f\u6267\u884c <code>Job</code> \u548c <code>Task</code> \u7684\u63a5\u53e3\uff1a</p> <pre><code>stateDiagram-v2\ndirection TB\nInit:::job --&gt; Prepare:::job\nPrepare --&gt; Split:::job\nSplit --&gt; Schedule:::fw\nstate Schedule {\n    direction LR\n    init\\nprepare\\nstartRead\\npost\\ndestroy1 --&gt; init\\nprepare\\nstartRead\\npost\\ndestroy : Channel\n}\nSchedule --&gt; Post:::job\n\nclassDef job fill:yellow\nclassDef fw fill:#c6fac4\nclassDef ctask fill:blue</code></pre> <p>\u4e0a\u56fe\u4e2d\uff0c\u9ec4\u8272\u8868\u793a <code>Job</code> \u90e8\u5206\u7684\u6267\u884c\u9636\u6bb5\uff0c\u7070\u8272\u8868\u793a <code>Task</code> \u90e8\u5206\u7684\u6267\u884c\u9636\u6bb5\uff0c\u7eff\u8272\u8868\u793a\u6846\u67b6\u6267\u884c\u9636\u6bb5\u3002</p> <p>\u76f8\u5173\u7c7b\u5173\u7cfb\u5982\u4e0b\uff1a</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\n    class Pluginable {\n    + init()\n    + destroy()\n    + others()\n    }\n    class AbstractPlugin {\n        + prepare()\n        + post()\n        + others()\n    }\n    class AbstractJobPlugin {\n        + getJobPluginCollector(): JobPluginCollector\n        + setJobPluginCollector(JobPluginCollector)\n    }\n\n    class AbstractTaskPlugin {\n        + getTaskPluginCollector(): TaskPluginCollector\n        + setTaskPluginCollector(TaskPluginCollector)\n    }\n    class Reader_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Writer_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Reader_Task {\n        + startRead(RecordSender)\n    }\n\n    class Writer_Task {\n        + startWrite(RecordReceiver)\n    }\n\n    AbstractJobPlugin &lt;|-- Reader_Job\n    AbstractJobPlugin &lt;|-- Writer_Job\n\n    AbstractTaskPlugin &lt;|-- Reader_Task\n    AbstractTaskPlugin &lt;|-- Writer_Task\n\n    AbstractPlugin &lt;|-- AbstractJobPlugin\n    AbstractPlugin &lt;|-- AbstractTaskPlugin\n\n    Pluginable &lt;|-- AbstractPlugin\n</code></pre>"},{"location":"plugin_development/#_6","title":"\u63d2\u4ef6\u5b9a\u4e49","text":"<p>\u5728\u6bcf\u4e2a\u63d2\u4ef6\u7684\u9879\u76ee\u4e2d\uff0c\u90fd\u6709\u4e00\u4e2a<code>plugin.json</code>\u6587\u4ef6\uff0c\u8fd9\u4e2a\u6587\u4ef6\u5b9a\u4e49\u4e86\u63d2\u4ef6\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u5305\u62ec\u5165\u53e3\u7c7b\u3002\u4f8b\u5982\uff1a</p> <pre><code>{\n  \"name\": \"mysqlwriter\",\n  \"class\": \"com.wgzhao.addax.plugin.writer.mysqlwriter.MysqlWriter\",\n  \"description\": \"Use Jdbc connect to database, execute insert sql.\",\n  \"developer\": \"wgzhao\"\n}\n</code></pre> <ul> <li><code>name</code>: \u63d2\u4ef6\u540d\u79f0\uff0c\u5927\u5c0f\u5199\u654f\u611f\u3002\u6846\u67b6\u6839\u636e\u7528\u6237\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u7684\u540d\u79f0\u6765\u641c\u5bfb\u63d2\u4ef6\u3002 \u5341\u5206\u91cd\u8981 \u3002</li> <li><code>class</code>: \u5165\u53e3\u7c7b\u7684\u5168\u9650\u5b9a\u540d\u79f0\uff0c\u6846\u67b6\u901a\u8fc7\u53cd\u5c04\u521b\u5efa\u5165\u53e3\u7c7b\u7684\u5b9e\u4f8b\u3002\u5341\u5206\u91cd\u8981 \u3002</li> <li><code>description</code>: \u63cf\u8ff0\u4fe1\u606f\u3002</li> <li><code>developer</code>: \u5f00\u53d1\u4eba\u5458\u3002</li> </ul>"},{"location":"plugin_development/#_7","title":"\u6253\u5305\u53d1\u5e03","text":"<p><code>Addax</code> \u4f7f\u7528 <code>assembly</code> \u6253\u5305\uff0c\u6253\u5305\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>mvn clean package\nmvn package assembly:single\n</code></pre> <p><code>Addax</code> \u63d2\u4ef6\u9700\u8981\u9075\u5faa\u7edf\u4e00\u7684\u76ee\u5f55\u7ed3\u6784\uff1a</p> <pre><code>${ADDAX_HOME}\n\u251c\u2500\u2500 bin\n\u2502     \u251c\u2500\u2500 addax.sh\n\u251c\u2500\u2500 conf\n\u2502     \u251c\u2500\u2500 core.json\n\u2502     \u2514\u2500\u2500 logback.xml\n\u251c\u2500\u2500 job\n\u251c\u2500\u2500 lib\n\u2502     \u251c\u2500\u2500 addax-common-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-core-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-rdbms-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-storage-&lt;version&gt;.jar\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 plugin\n\u2502     \u251c\u2500\u2500 reader\n\u2502     \u2502     \u251c\u2500\u2500 cassandrareader\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 cassandrareader-&lt;version&gt;.jar\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 libs\n\u2502     \u2502     \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 plugin.json\n\u2502     \u2502     \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u2502     \u2514\u2500\u2500 writer\n\u2502         \u251c\u2500\u2500 cassandrawriter\n\u2502         \u2502     \u251c\u2500\u2500 cassandrawriter-&lt;version&gt;.jar\n\u2502         \u2502     \u251c\u2500\u2500 libs\n\u2502         \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502         \u2502     \u251c\u2500\u2500 plugin.json\n\u2502         \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u251c\u2500\u2500 shared\n</code></pre> <ul> <li><code>${ADDAX_HOME}/bin</code>: \u53ef\u6267\u884c\u7a0b\u5e8f\u76ee\u5f55</li> <li><code>${ADDAX_HOME}/conf</code>: \u6846\u67b6\u914d\u7f6e\u76ee\u5f55</li> <li><code>${ADDAX_HOME}/lib</code>: \u6846\u67b6\u4f9d\u8d56\u5e93\u76ee\u5f55</li> <li><code>${ADDAX_HOME}/shared</code>: \u63d2\u4ef6\u4f9d\u8d56\u76ee\u5f55</li> <li><code>${ADDAX_HOME}/plugin</code>: \u63d2\u4ef6\u76ee\u5f55</li> </ul> <p>\u63d2\u4ef6\u76ee\u5f55\u5206\u4e3a <code>reader</code> \u548c <code>writer</code> \u5b50\u76ee\u5f55\uff0c\u8bfb\u5199\u63d2\u4ef6\u5206\u522b\u5b58\u653e\u3002\u63d2\u4ef6\u76ee\u5f55\u89c4\u8303\u5982\u4e0b\uff1a</p> <ul> <li><code>${PLUGIN_HOME}/libs</code>: \u63d2\u4ef6\u7684\u4f9d\u8d56\u5e93\uff0c\u4e3a\u4e86\u51cf\u5c11\u7a0b\u5e8f\u5305\u5927\u5c0f\uff0c\u8fd9\u4e9b\u4f9d\u8d56\u5305\u90fd\u662f\u6307\u5411 <code>shared</code> \u76ee\u5f55\u7684\u7b26\u53f7\u94fe\u63a5</li> <li><code>${PLUGIN_HOME}/plugin-name-version.jar</code>: \u63d2\u4ef6\u672c\u8eab\u7684 jar\u3002</li> <li><code>${PLUGIN_HOME}/plugin.json</code>: \u63d2\u4ef6\u63cf\u8ff0\u6587\u4ef6\u3002</li> </ul> <p>\u5c3d\u7ba1\u6846\u67b6\u52a0\u8f7d\u63d2\u4ef6\u65f6\uff0c\u4f1a\u628a <code>${PLUGIN_HOME}</code> \u4e0b\u6240\u6709\u7684 jar \u5305\u6dfb\u52a0\u5230 <code>classpath</code> \u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u4f46\u8fd8\u662f\u63a8\u8350\u4f9d\u8d56\u5e93\u7684 jar \u548c\u63d2\u4ef6\u672c\u8eab\u7684 jar \u5206\u5f00\u5b58\u653e\u3002</p> <p>\u7279\u522b\u63d0\u9192</p> <p>\u63d2\u4ef6\u7684\u76ee\u5f55\u540d\u5b57\u5fc5\u987b\u548c <code>plugin.json</code> \u4e2d\u5b9a\u4e49\u7684\u63d2\u4ef6\u540d\u79f0\u4e00\u81f4\u3002</p>"},{"location":"plugin_development/#_8","title":"\u914d\u7f6e\u6587\u4ef6","text":"<p><code>Addax</code> \u4f7f\u7528 <code>json</code> \u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u7684\u683c\u5f0f\u3002\u4e00\u4e2a\u5178\u578b\u7684 <code>Addax</code> \u4efb\u52a1\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p><code>Addax</code> \u6846\u67b6\u6709 <code>core.json</code> \u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9a\u4e86\u6846\u67b6\u7684\u9ed8\u8ba4\u884c\u4e3a\u3002\u4efb\u52a1\u7684\u914d\u7f6e\u91cc\u5934\u53ef\u4ee5\u6307\u5b9a\u6846\u67b6\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u914d\u7f6e\u9879\uff0c\u800c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u4f1a\u8986\u76d6 <code>core.json</code> \u4e2d\u7684\u9ed8\u8ba4\u503c\u3002</p> <p>\u914d\u7f6e\u4e2d<code>job.content.reader.parameter</code> \u7684 <code>value</code> \u90e8\u5206\u4f1a\u4f20\u7ed9 <code>Reader.Job</code>\uff1b<code>job.content.writer.parameter</code> \u7684 <code>value</code> \u90e8\u5206\u4f1a\u4f20\u7ed9<code>Writer.Job</code> \uff0c <code>Reader.Job</code> \u548c <code>Writer.Job</code> \u53ef\u4ee5\u901a\u8fc7 <code>super.getPluginJobConf()</code> \u6765\u83b7\u53d6\u3002</p>"},{"location":"plugin_development/#_9","title":"\u5982\u4f55\u8bbe\u8ba1\u914d\u7f6e\u53c2\u6570","text":"<p>\u914d\u7f6e\u6587\u4ef6\u7684\u8bbe\u8ba1\u662f\u63d2\u4ef6\u5f00\u53d1\u7684\u7b2c\u4e00\u6b65\uff01</p> <p>\u4efb\u52a1\u914d\u7f6e\u4e2d <code>reader</code> \u548c <code>writer</code> \u4e0b <code>parameter</code> \u90e8\u5206\u662f\u63d2\u4ef6\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u63d2\u4ef6\u7684\u914d\u7f6e\u53c2\u6570\u5e94\u5f53\u9075\u5faa\u4ee5\u4e0b\u539f\u5219\uff1a</p> <ul> <li>\u9a7c\u5cf0\u547d\u540d\uff1a\u6240\u6709\u914d\u7f6e\u9879\u91c7\u7528\u5c0f\u9a7c\u5cf0\u547d\u540d\u6cd5\uff0c\u9996\u5b57\u6bcd\u5c0f\u5199\u3002</li> <li>\u6b63\u4ea4\u539f\u5219\uff1a\u914d\u7f6e\u9879\u5fc5\u987b\u6b63\u4ea4\uff0c\u529f\u80fd\u6ca1\u6709\u91cd\u590d\uff0c\u6ca1\u6709\u6f5c\u89c4\u5219\u3002</li> <li>\u5bcc\u7c7b\u578b\uff1a\u5408\u7406\u4f7f\u7528 json \u7684\u7c7b\u578b\uff0c\u51cf\u5c11\u65e0\u8c13\u7684\u5904\u7406\u903b\u8f91\uff0c\u51cf\u5c11\u51fa\u9519\u7684\u53ef\u80fd\u3002</li> <li>\u4f7f\u7528\u6b63\u786e\u7684\u6570\u636e\u7c7b\u578b\u3002\u6bd4\u5982\uff0c<code>bool</code> \u7c7b\u578b\u7684\u503c\u4f7f\u7528 <code>true</code>/<code>false</code>\uff0c\u800c\u975e <code>\"yes\"</code>/<code>\"true\"</code>/<code>0</code> \u7b49\u3002</li> <li>\u5408\u7406\u4f7f\u7528\u96c6\u5408\u7c7b\u578b\uff0c\u6bd4\u5982\uff0c\u7528\u6570\u7ec4\u66ff\u4ee3\u6709\u5206\u9694\u7b26\u7684\u5b57\u7b26\u4e32\u3002</li> <li>\u7c7b\u4f3c\u901a\u7528\uff1a\u9075\u5b88\u540c\u4e00\u7c7b\u578b\u7684\u63d2\u4ef6\u7684\u4e60\u60ef\uff0c\u6bd4\u5982\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7684 <code>connection</code> \u53c2\u6570\u90fd\u662f\u5982\u4e0b\u7ed3\u6784\uff1a</li> </ul> <pre><code>{\n  \"connection\": [\n    {\n      \"table\": [\"table_1\", \"table_2\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.1:3306/database_1\",\n        \"jdbc:mysql://127.0.0.2:3306/database_1_slave\"\n      ]\n    },\n    {\n      \"table\": [\"table_3\", \"table_4\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.3:3306/database_2\",\n        \"jdbc:mysql://127.0.0.4:3306/database_2_slave\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"plugin_development/#configuration","title":"\u5982\u4f55\u4f7f\u7528 <code>Configuration</code> \u7c7b","text":"<p>\u4e3a\u4e86\u7b80\u5316\u5bf9 <code>json</code> \u7684\u64cd\u4f5c\uff0c<code>Addax</code> \u63d0\u4f9b\u4e86\u7b80\u5355\u7684 DSL \u914d\u5408 <code>Configuration</code> \u7c7b\u4f7f\u7528\u3002</p> <p><code>Configuration</code> \u63d0\u4f9b\u4e86\u5e38\u89c1\u7684 <code>get</code>, <code>\u5e26\u7c7b\u578bget</code>\uff0c<code>\u5e26\u9ed8\u8ba4\u503cget</code>\uff0c<code>set</code> \u7b49\u8bfb\u5199\u914d\u7f6e\u9879\u7684\u64cd\u4f5c\uff0c\u4ee5\u53ca <code>clone</code>, <code>toJSON</code> \u7b49\u65b9\u6cd5\u3002\u914d\u7f6e\u9879\u8bfb\u5199\u64cd\u4f5c\u90fd\u9700\u8981\u4f20\u5165\u4e00\u4e2a <code>path</code> \u505a\u4e3a\u53c2\u6570\uff0c \u8fd9\u4e2a <code>path</code> \u5c31\u662f <code>Addax</code> \u5b9a\u4e49\u7684 DSL\u3002\u8bed\u6cd5\u6709\u4e24\u6761\uff1a</p> <ol> <li>\u5b50 map \u7528 <code>.key</code> \u8868\u793a\uff0c<code>path</code> \u7684\u7b2c\u4e00\u4e2a\u70b9\u7701\u7565\u3002</li> <li>\u6570\u7ec4\u5143\u7d20\u7528 <code>[index]</code> \u8868\u793a\u3002</li> </ol> <p>\u6bd4\u5982\u64cd\u4f5c\u5982\u4e0b json\uff1a</p> <pre><code>{\n  \"a\": {\n    \"b\": {\n      \"c\": 2\n    },\n    \"f\": [\n      1,\n      2,\n      {\n        \"g\": true,\n        \"h\": false\n      },\n      4\n    ]\n  },\n  \"x\": 4\n}\n</code></pre> <p>\u6bd4\u5982\u8c03\u7528 <code>configuration.get(path)</code> \u65b9\u6cd5\uff0c\u5f53 path \u4e3a\u5982\u4e0b\u503c\u7684\u65f6\u5019\u5f97\u5230\u7684\u7ed3\u679c\u4e3a\uff1a</p> <ul> <li><code>x</code>\uff1a<code>4</code></li> <li><code>a.b.c</code>\uff1a<code>2</code></li> <li><code>a.b.c.d</code>\uff1a<code>null</code></li> <li><code>a.b.f[0]</code>\uff1a<code>1</code></li> <li><code>a.b.f[2].g</code>\uff1a<code>true</code></li> </ul> <p>\u6ce8\u610f\uff0c\u56e0\u4e3a\u63d2\u4ef6\u770b\u5230\u7684\u914d\u7f6e\u53ea\u662f\u6574\u4e2a\u914d\u7f6e\u7684\u4e00\u90e8\u5206\u3002\u4f7f\u7528 <code>Configuration</code> \u5bf9\u8c61\u65f6\uff0c\u9700\u8981\u6ce8\u610f\u5f53\u524d\u7684\u6839\u8def\u5f84\u662f\u4ec0\u4e48\u3002</p> <p>\u66f4\u591a <code>Configuration</code> \u7684\u64cd\u4f5c\u8bf7\u53c2\u8003 Configuration.java \u3002</p>"},{"location":"plugin_development/#_10","title":"\u63d2\u4ef6\u6570\u636e\u4f20\u8f93","text":"<p>\u8ddf\u4e00\u822c\u7684 <code>\u751f\u4ea7\u8005-\u6d88\u8d39\u8005</code> \u6a21\u5f0f\u4e00\u6837\uff0c<code>Reader</code> \u63d2\u4ef6\u548c <code>Writer</code> \u63d2\u4ef6\u4e4b\u95f4\u4e5f\u662f\u901a\u8fc7 <code>channel</code> \u6765\u5b9e\u73b0\u6570\u636e\u7684\u4f20\u8f93\u7684\u3002<code>channel</code> \u53ef\u4ee5\u662f\u5185\u5b58\u7684\uff0c\u4e5f\u53ef\u80fd\u662f\u6301\u4e45\u5316\u7684\uff0c\u63d2\u4ef6\u4e0d\u5fc5\u5173\u5fc3\u3002 \u63d2\u4ef6\u901a\u8fc7 <code>RecordSender</code> \u5f80 <code>channel</code> \u5199\u5165\u6570\u636e\uff0c\u901a\u8fc7 <code>RecordReceiver</code> \u4ece <code>channel</code> \u8bfb\u53d6\u6570\u636e\u3002</p> <p><code>channel</code> \u4e2d\u7684\u4e00\u6761\u6570\u636e\u4e3a\u4e00\u4e2a <code>Record</code> \u7684\u5bf9\u8c61\uff0c<code>Record</code> \u4e2d\u53ef\u4ee5\u653e\u591a\u4e2a <code>Column</code> \u5bf9\u8c61\uff0c\u8fd9\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\u6570\u636e\u5e93\u4e2d\u7684\u8bb0\u5f55\u548c\u5217\u3002</p> <p><code>Record</code> \u6709\u5982\u4e0b\u65b9\u6cd5\uff1a</p> <pre><code>public interface Record\n{\n    // \u52a0\u5165\u4e00\u4e2a\u5217\uff0c\u653e\u5728\u6700\u540e\u7684\u4f4d\u7f6e\n    void addColumn(Column column);\n\n    // \u5728\u6307\u5b9a\u4e0b\u6807\u5904\u653e\u7f6e\u4e00\u4e2a\u5217\n    void setColumn(int i, final Column column);\n\n    // \u83b7\u53d6\u4e00\u4e2a\u5217\n    Column getColumn(int i);\n\n    // \u8f6c\u6362\u4e3ajson String\n    String toString();\n\n    // \u83b7\u53d6\u603b\u5217\u6570\n    int getColumnNumber();\n\n    // \u8ba1\u7b97\u6574\u6761\u8bb0\u5f55\u5728\u5185\u5b58\u4e2d\u5360\u7528\u7684\u5b57\u8282\u6570\n    int getByteSize();\n}\n</code></pre> <p>\u56e0\u4e3a <code>Record</code> \u662f\u4e00\u4e2a\u63a5\u53e3\uff0c<code>Reader</code> \u63d2\u4ef6\u9996\u5148\u8c03\u7528 <code>RecordSender.createRecord()</code> \u521b\u5efa\u4e00\u4e2a <code>Record</code> \u5b9e\u4f8b\uff0c\u7136\u540e\u628a <code>Column</code> \u4e00\u4e2a\u4e2a\u6dfb\u52a0\u5230 <code>Record</code> \u4e2d\u3002</p> <p><code>Writer</code> \u63d2\u4ef6\u8c03\u7528 <code>RecordReceiver.getFromReader()</code> \u65b9\u6cd5\u83b7\u53d6 <code>Record</code>\uff0c\u7136\u540e\u628a <code>Column</code> \u904d\u5386\u51fa\u6765\uff0c\u5199\u5165\u76ee\u6807\u5b58\u50a8\u4e2d\u3002\u5f53 <code>Reader</code> \u5c1a\u672a\u9000\u51fa\uff0c\u4f20\u8f93\u8fd8\u5728\u8fdb\u884c\u65f6\uff0c\u5982\u679c\u6682\u65f6\u6ca1\u6709\u6570\u636e <code>RecordReceiver.getFromReader()</code> \u65b9\u6cd5\u4f1a\u963b\u585e\u76f4\u5230\u6709\u6570\u636e\u3002 \u5982\u679c\u4f20\u8f93\u5df2\u7ecf\u7ed3\u675f\uff0c\u4f1a\u8fd4\u56de<code>null</code>\uff0c<code>Writer</code> \u63d2\u4ef6\u53ef\u4ee5\u636e\u6b64\u5224\u65ad\u662f\u5426\u7ed3\u675f <code>startWrite</code> \u65b9\u6cd5\u3002</p>"},{"location":"plugin_development/#_11","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u4e3a\u4e86\u89c4\u8303\u6e90\u7aef\u548c\u76ee\u7684\u7aef\u7c7b\u578b\u8f6c\u6362\u64cd\u4f5c\uff0c\u4fdd\u8bc1\u6570\u636e\u4e0d\u5931\u771f\uff0cAddax \u652f\u6301\u516d\u79cd\u5185\u90e8\u6570\u636e\u7c7b\u578b\uff1a</p> <ul> <li><code>Long</code>\uff1a\u5b9a\u70b9\u6570(Int\u3001Short\u3001Long\u3001BigInteger \u7b49)\u3002</li> <li><code>Double</code>\uff1a\u6d6e\u70b9\u6570(Float\u3001Double\u3001BigDecimal(\u65e0\u9650\u7cbe\u5ea6)\u7b49)\u3002</li> <li><code>String</code>\uff1a\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u5e95\u5c42\u4e0d\u9650\u957f\uff0c\u4f7f\u7528\u901a\u7528\u5b57\u7b26\u96c6(Unicode)\u3002</li> <li><code>Date</code>\uff1a\u65e5\u671f\u7c7b\u578b\u3002</li> <li><code>Timestamp</code>: \u65f6\u95f4\u6233</li> <li><code>Bool</code>\uff1a\u5e03\u5c14\u503c\u3002</li> <li><code>Bytes</code>\uff1a\u4e8c\u8fdb\u5236\uff0c\u53ef\u4ee5\u5b58\u653e\u8bf8\u5982 MP3 \u7b49\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002</li> </ul> <p>\u5bf9\u5e94\u5730\uff0c\u6709 <code>DateColumn</code>\u3001<code>LongColumn</code>\u3001<code>DoubleColumn</code>\u3001<code>BytesColumn</code>\u3001<code>StringColumn</code> \u3001<code>BoolColumn</code> \u548c <code>TimestampColumn</code> \u4e03\u79cd <code>Column</code> \u7684\u5b9e\u73b0\u3002</p> <p><code>Column</code> \u9664\u4e86\u63d0\u4f9b\u6570\u636e\u76f8\u5173\u7684\u65b9\u6cd5\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e00\u7cfb\u5217\u4ee5 <code>as</code> \u5f00\u5934\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u8f6c\u6362\u65b9\u6cd5\u3002</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\ndirection TB\nclass Column {\n    &lt;&lt;interface&gt;&gt;\n    - rawData: Object\n    - type: Type\n    + getRawData(): Object\n    + getType(): Type\n    + getByteSize(): init\n    + asLong(): Long\n    + asDouble(): Doule\n    + asString(): String\n    + asDate(): Date\n    + asBytes(): Bytes\n    + asBigDecimal(): BigDecimal\n    + asBoolean(): Boolean\n}\nColumn &lt;|-- Stringcolumn\nColumn &lt;|-- Doublecolumn\nColumn &lt;|-- Longcolumn\nColumn &lt;|-- Datecolumn\nColumn &lt;|-- Boolcolumn\nColumn &lt;|-- Bytescolumn</code></pre> <p>Addax \u7684\u5185\u90e8\u7c7b\u578b\u5728\u5b9e\u73b0\u4e0a\u4f1a\u9009\u7528\u4e0d\u540c\u7684 java \u7c7b\u578b\uff1a</p> \u5185\u90e8\u7c7b\u578b \u5b9e\u73b0\u7c7b\u578b \u5907\u6ce8 Date java.util.Date Timestamp java.sql.Timestamp \u53ef\u4ee5\u7cbe\u786e\u5230\u7eb3\u79d2 Long java.math.BigInteger \u4f7f\u7528\u65e0\u9650\u7cbe\u5ea6\u7684\u5927\u6574\u6570\uff0c\u4fdd\u8bc1\u4e0d\u5931\u771f Double java.lang.String \u7528 String \u8868\u793a\uff0c\u4fdd\u8bc1\u4e0d\u5931\u771f Bytes byte[] String java.lang.String Bool java.lang.Boolean <p>\u7c7b\u578b\u4e4b\u95f4\u76f8\u4e92\u8f6c\u6362\u7684\u5173\u7cfb\u5982\u4e0b\uff1a</p> from/to Date Long Double Bytes String Bool Date - \u4f7f\u7528\u6beb\u79d2\u65f6\u95f4\u6233 \u4e0d\u652f\u6301 \u4e0d\u652f\u6301 \u6309\u914d\u7f6e\u683c\u5f0f\u8f6c\u6362 \u4e0d\u652f\u6301 Long \u4f5c\u4e3a\u6beb\u79d2\u65f6\u95f4\u6233\u6784\u9020 Date - <code>BigDecimal.doubleValue()</code> \u4e0d\u652f\u6301 <code>BigInteger.toString()</code> 0 \u4e3a <code>false</code>\uff0c\u5176\u4ed6\u4e3a <code>true</code> Double \u4e0d\u652f\u6301 <code>BigDecimal.longValue()</code> - \u4e0d\u652f\u6301 \u76f4\u63a5\u8fd4\u56de\u5185\u90e8 String \u4e0d\u652f\u6301 Bytes \u4e0d\u652f\u6301 \u4e0d\u652f\u6301 \u4e0d\u652f\u6301 - \u6309utf-8\u7f16\u7801\u8f6c\u6362\u4e3a <code>byte[]</code> \u4e0d\u652f\u6301 String \u6309\u914d\u7f6e\u7684 \u683c\u5f0f\u89e3\u6790 <code>BigDecimal.longValue</code> <code>BigDecimal.doubleValue</code><sup>1</sup> \u6309 utf-8 \u7f16\u7801<sup>2</sup>\u8f6c\u6362\u4e3a <code>byte[]</code> - \"true\"\u4e3a<code>true</code>, \"false\"\u4e3a<code>false</code>\uff0c\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u3002\u5176\u4ed6\u4e0d\u652f\u6301 Bool \u4e0d\u652f\u6301 <code>true</code>\u4e3a<code>1L</code>\uff0c\u5426\u5219<code>0L</code> \u4e0d\u652f\u6301 <code>true</code>\u4e3a<code>1.0</code>\uff0c\u5426\u5219<code>0.0</code> \u4e0d\u652f\u6301 -"},{"location":"plugin_development/#_12","title":"\u810f\u6570\u636e\u5904\u7406","text":""},{"location":"plugin_development/#_13","title":"\u4ec0\u4e48\u662f\u810f\u6570\u636e","text":"<p>\u76ee\u524d\u4e3b\u8981\u6709\u4e09\u7c7b\u810f\u6570\u636e\uff1a</p> <ol> <li>Reader \u8bfb\u5230\u4e0d\u652f\u6301\u7684\u7c7b\u578b\u3001\u4e0d\u5408\u6cd5\u7684\u503c\u3002</li> <li>\u4e0d\u652f\u6301\u7684\u7c7b\u578b\u8f6c\u6362\uff0c\u6bd4\u5982\uff1a<code>Bytes</code> \u8f6c\u6362\u4e3a <code>Date</code>\u3002</li> <li>\u5199\u5165\u76ee\u6807\u7aef\u5931\u8d25\uff0c\u6bd4\u5982\uff1a\u5199 MySQL \u6574\u578b\u957f\u5ea6\u8d85\u957f\u3002</li> </ol>"},{"location":"plugin_development/#_14","title":"\u5982\u4f55\u5904\u7406\u810f\u6570\u636e","text":"<p>\u5728 <code>Reader.Task</code> \u548c <code>Writer.Task</code> \u4e2d\uff0c\u901a\u8fc7 <code>AbstractTaskPlugin.getPluginCollector()</code> \u53ef\u4ee5\u62ff\u5230\u4e00\u4e2a <code>TaskPluginCollector</code>\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217 <code>collectDirtyRecord</code> \u7684\u65b9\u6cd5\u3002 \u5f53\u810f\u6570\u636e\u51fa\u73b0\u65f6\uff0c\u53ea\u9700\u8981\u8c03\u7528\u5408\u9002\u7684 <code>collectDirtyRecord</code> \u65b9\u6cd5\uff0c\u628a\u88ab\u8ba4\u4e3a\u662f\u810f\u6570\u636e\u7684 <code>Record</code> \u4f20\u5165\u5373\u53ef\u3002</p> <p>\u7528\u6237\u53ef\u4ee5\u5728\u4efb\u52a1\u7684\u914d\u7f6e\u4e2d\u6307\u5b9a\u810f\u6570\u636e\u9650\u5236\u6761\u6570\u6216\u8005\u767e\u5206\u6bd4\u9650\u5236\uff0c\u5f53\u810f\u6570\u636e\u8d85\u51fa\u9650\u5236\u65f6\uff0c\u6846\u67b6\u4f1a\u7ed3\u675f\u540c\u6b65\u4efb\u52a1\uff0c\u9000\u51fa\u3002\u63d2\u4ef6\u9700\u8981\u4fdd\u8bc1\u810f\u6570\u636e\u90fd\u88ab\u6536\u96c6\u5230\uff0c\u5176\u4ed6\u5de5\u4f5c\u4ea4\u7ed9\u6846\u67b6\u5c31\u597d\u3002</p>"},{"location":"plugin_development/#_15","title":"\u52a0\u8f7d\u539f\u7406","text":"<ol> <li>\u6846\u67b6\u626b\u63cf <code>plugin/reader</code> \u548c <code>plugin/writer</code>\u76ee\u5f55\uff0c\u52a0\u8f7d\u6bcf\u4e2a\u63d2\u4ef6\u7684 <code>plugin.json</code> \u6587\u4ef6\u3002</li> <li>\u4ee5 <code>plugin.json</code> \u6587\u4ef6\u4e2d <code>name</code> \u4e3a key\uff0c\u7d22\u5f15\u6240\u6709\u7684\u63d2\u4ef6\u914d\u7f6e\u3002\u5982\u679c\u53d1\u73b0\u91cd\u540d\u7684\u63d2\u4ef6\uff0c\u6846\u67b6\u4f1a\u5f02\u5e38\u9000\u51fa\u3002</li> <li>\u7528\u6237\u5728\u63d2\u4ef6\u4e2d\u5728 <code>reader</code>/<code>writer</code> \u914d\u7f6e\u7684 <code>name</code> \u5b57\u6bb5\u6307\u5b9a\u63d2\u4ef6\u540d\u5b57\u3002\u6846\u67b6\u6839\u636e\u63d2\u4ef6\u7684\u7c7b\u578b\uff08<code>reader</code>/<code>writer</code>\uff09\u548c\u63d2\u4ef6\u540d\u79f0\u53bb\u63d2\u4ef6\u7684\u8def\u5f84\u4e0b\u626b\u63cf\u6240\u6709\u7684 jar\uff0c\u52a0\u5165 <code>classpath</code>\u3002</li> <li>\u6839\u636e\u63d2\u4ef6\u914d\u7f6e\u4e2d\u5b9a\u4e49\u7684\u5165\u53e3\u7c7b\uff0c\u6846\u67b6\u901a\u8fc7\u53cd\u5c04\u5b9e\u4f8b\u5316\u5bf9\u5e94\u7684 <code>Job</code> \u548c <code>Task</code> \u5bf9\u8c61\u3002</li> </ol> <ol> <li> <p>\u5904\u7406 <code>NaN</code>, <code>Infinity</code>, <code>-Infinity</code> \u7b49\u6570\u503c\u00a0\u21a9</p> </li> <li> <p>\u9664\u975e\u53e6\u6709\u6307\u5b9a\u7f16\u7801\u683c\u5f0f\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"\u5feb\u901f\u4f7f\u7528","text":""},{"location":"quickstart/#addax","title":"\u5b89\u88c5 Addax","text":""},{"location":"quickstart/#docker","title":"\u4f7f\u7528 Docker \u955c\u50cf","text":"<p>\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 Docker \u955c\u50cf\uff0c\u53ea\u9700\u8981\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u5373\u53ef</p> <pre><code>docker run -it --rm quay.io/wgzhao/addax:latest /opt/addax/bin/addax.sh /opt/addax/job/job.json\n</code></pre>"},{"location":"quickstart/#_2","title":"\u4e00\u952e\u5b89\u88c5","text":"<p>\u5982\u679c\u4f60\u4e0d\u60f3\u7f16\u8bd1\uff0c\u4f60\u53ef\u4ee5\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u4e00\u952e\u5b89\u88c5\uff08\u5f53\u524d\u4ec5\u652f\u6301 Linux \u548c macOS \uff09</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/wgzhao/Addax/master/install.sh)\"\n</code></pre> <p>\u5982\u679c\u662f macOS \uff0c\u9ed8\u8ba4\u5b89\u88c5\u5728 <code>/usr/local/addax</code> \u76ee\u5f55\u4e0b\uff0c \u5982\u679c\u662f Linux\uff0c \u5219\u5b89\u88c5\u5728 <code>/opt/addax</code> \u76ee\u5f55\u4e0b</p>"},{"location":"quickstart/#_3","title":"\u6e90\u4ee3\u7801\u7f16\u8bd1\u5b89\u88c5","text":"<p>\u4f60\u53ef\u4ee5\u9009\u62e9\u4ece\u6e90\u4ee3\u7801\u7f16\u8bd1\u5b89\u88c5\uff0c\u57fa\u672c\u64cd\u4f5c\u5982\u4e0b\uff1a</p> <pre><code>git clone https://github.com/wgzhao/addax.git\ncd addax\nmvn clean package\nmvn package assembly:single\ncd target/addax/addax-&lt;version&gt;\n</code></pre>"},{"location":"quickstart/#_4","title":"\u5f00\u59cb\u7b2c\u4e00\u4e2a\u91c7\u96c6\u4efb\u52a1","text":"<p>\u8981\u4f7f\u7528 <code>Addax</code> \u8fdb\u884c\u6570\u636e\u91c7\u96c6\uff0c\u53ea\u9700\u8981\u7f16\u5199\u4e00\u4e2a\u4efb\u52a1\u91c7\u96c6\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u4e3a JSON \u683c\u5f0f\uff0c\u4ee5\u4e0b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u8be5\u4efb\u52a1\u7684\u76ee\u7684\u662f\u4ece\u5185\u5b58\u8bfb\u53d6\u8bfb\u53d6\u6307\u5b9a\u5185\u5bb9\u7684\u6570\u636e\uff0c\u5e76\u5c06\u5176\u6253\u5370\u51fa\u6765\uff0c\u6587\u4ef6\u4fdd\u5b58\u5728 <code>job/test.json</code> \u4e2d</p> job/test.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/test.json</code></p> <p>\u7136\u540e\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff1a</p> <pre><code>bin/addax.sh job/test.json\n</code></pre> <p>\u5982\u679c\u6ca1\u6709\u62a5\u9519\uff0c\u5e94\u8be5\u4f1a\u6709\u7c7b\u4f3c\u8fd9\u6837\u7684\u8f93\u51fa</p> <pre><code>  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3-SNAPSHOT)\n\n2021-08-23 13:45:17.199 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-23 13:45:17.223 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"addax\"\n                        },\n                        {\n                            \"type\":\"long\",\n                            \"value\":19890604\n                        },\n                        {\n                            \"type\":\"date\",\n                            \"value\":\"1989-06-04 00:00:00\"\n                        },\n                        {\n                            \"type\":\"bool\",\n                            \"value\":true\n                        }\n                    ],\n                    \"sliceRecordCount\":10\n                },\n                \"name\":\"streamreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n        }\n}\n\n2021-08-23 13:45:17.238 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-23 13:45:17.239 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-23 13:45:17.240 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-23 13:45:17.250 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-08-23 13:45:17.250 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-23 13:45:17.251 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-23 13:45:17.251 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-08-23 13:45:17.252 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-23 13:45:17.276 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-23 13:45:17.282 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-23 13:45:17.287 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-23 13:45:17.288 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\naddax   19890604    1989-06-04 00:00:00 true\n2021-08-23 13:45:20.295 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-23 13:45:20.296 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-23 13:45:20.297 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-08-23 13:45:20.302 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-23 13:45:20.305 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 220 bytes | Speed 73B/s, 3 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.011s | Percentage 100.00%\n2021-08-23 13:45:20.307 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-23 13:45:17\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-23 13:45:20\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               73B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"server/","title":"Server\u6a21\u5757","text":"<p>Server\u6a21\u5757\u7528\u4e8e\u901a\u8fc7HTTP\u63a5\u53e3\u63d0\u4ea4\u548c\u7ba1\u7406\u6570\u636e\u91c7\u96c6\u4efb\u52a1\u3002\u7528\u6237\u53ef\u901a\u8fc7POST\u65b9\u5f0f\u63d0\u4ea4JSON\u4efb\u52a1\u914d\u7f6e\uff0c\u670d\u52a1\u7aef\u5f02\u6b65\u6267\u884c\u91c7\u96c6\u4efb\u52a1\u5e76\u8fd4\u56de\u552f\u4e00\u4efb\u52a1ID\uff0c\u968f\u540e\u53ef\u901a\u8fc7\u4efb\u52a1ID\u67e5\u8be2\u4efb\u52a1\u8fdb\u5ea6\u548c\u7ed3\u679c\u3002</p>"},{"location":"server/#_1","title":"\u529f\u80fd\u7b80\u4ecb","text":"<ul> <li>\u63d0\u4f9bRESTful\u63a5\u53e3\uff0c\u652f\u6301\u4efb\u52a1\u63d0\u4ea4\u4e0e\u72b6\u6001\u67e5\u8be2</li> <li>\u652f\u6301\u6700\u5927\u5e76\u53d1\u4efb\u52a1\u6570\u9650\u5236\uff08\u9ed8\u8ba430\uff0c\u53ef\u914d\u7f6e\uff09</li> <li>\u96c6\u6210Addax\u6838\u5fc3Engine\uff0c\u76f4\u63a5\u6267\u884c\u91c7\u96c6\u4efb\u52a1</li> <li>\u652f\u6301\u547d\u4ee4\u884c\u548c\u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u5e76\u53d1\u6570</li> <li>\u63d0\u4f9b\u542f\u52a8/\u505c\u6b62\u811a\u672c\uff0c\u652f\u6301\u540e\u53f0\u8fd0\u884c</li> </ul>"},{"location":"server/#http","title":"HTTP\u63a5\u53e3\u8bf4\u660e","text":""},{"location":"server/#1","title":"1. \u63d0\u4ea4\u4efb\u52a1","text":"<ul> <li>URL: <code>/api/submit?k1=v1&amp;k2=v2</code></li> <li>\u65b9\u6cd5: POST</li> <li>\u8bf7\u6c42\u4f53\u793a\u4f8b\uff1a <pre><code>curl 'http://localhost:10601/api/submit?jobName=example-job' \\\n-H 'Content-Type: application/json' \\\n-d @job/job.json\n</code></pre></li> <li>\u8fd4\u56de\u793a\u4f8b\uff1a <pre><code>{\n    \"taskId\": \"xxxx-xxxx-xxxx\"\n}\n</code></pre></li> <li>\u5f53\u5e76\u53d1\u6570\u8fbe\u5230\u4e0a\u9650\u65f6\uff1a <pre><code>{\n    \"error\": \"ERROR: Maximum number of concurrent tasks reached.\"\n}\n</code></pre></li> </ul>"},{"location":"server/#2","title":"2. \u67e5\u8be2\u4efb\u52a1\u72b6\u6001","text":"<ul> <li>URL: <code>/api/status?taskId={taskId}</code></li> <li>\u65b9\u6cd5: GET</li> <li>\u8fd4\u56de\u793a\u4f8b\uff1a <pre><code>{\n    \"taskId\": \"xxxx-xxxx-xxxx\",\n    \"status\": \"SUCCESS\",\n    \"result\": \"Job example-job executed.\",\n    \"error\": null\n}\n</code></pre></li> </ul>"},{"location":"server/#_2","title":"\u542f\u52a8\u4e0e\u505c\u6b62","text":"<p>\u63a8\u8350\u4f7f\u7528\u811a\u672c <code>core/src/main/bin/addax-server.sh</code> \u542f\u52a8\u548c\u505c\u6b62\u670d\u52a1\u3002</p>"},{"location":"server/#_3","title":"\u542f\u52a8\u670d\u52a1","text":"<pre><code>./addax-server.sh start\n</code></pre>"},{"location":"server/#50","title":"\u8bbe\u7f6e\u6700\u5927\u5e76\u53d1\u6570\uff08\u598250\uff09\u5e76\u540e\u53f0\u8fd0\u884c","text":"<pre><code>./addax-server.sh start -p 50 --daemon\n</code></pre>"},{"location":"server/#_4","title":"\u505c\u6b62\u670d\u52a1","text":"<pre><code>./addax-server.sh stop\n</code></pre>"},{"location":"server/#_5","title":"\u5e76\u53d1\u6570\u914d\u7f6e","text":"<ul> <li>\u547d\u4ee4\u884c\u53c2\u6570 <code>-p</code> \u6216 <code>--parallel</code> \u4f18\u5148</li> <li>\u73af\u5883\u53d8\u91cf <code>ADDAX_SERVER_PARALLEL</code> \u5176\u6b21</li> <li>\u9ed8\u8ba4\u5e76\u53d1\u6570\u4e3a30</li> </ul>"},{"location":"server/#_6","title":"\u4f9d\u8d56\u8bf4\u660e","text":"<ul> <li>\u4ec5\u4f9d\u8d56Spring Boot\u6700\u5c0f\u5316Web\u7ec4\u4ef6</li> <li>\u4f9d\u8d56core\u6a21\u5757\u7684Engine\u7c7b</li> </ul>"},{"location":"server/#_7","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u4efb\u52a1\u63d0\u4ea4\u65f6\u8bf7\u786e\u4fddjob\u53c2\u6570\u4e3a\u5408\u6cd5\u7684Addax\u4efb\u52a1JSON</li> <li>\u5e76\u53d1\u6570\u8fc7\u9ad8\u53ef\u80fd\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd</li> </ul> <p>\u5982\u9700\u66f4\u591a\u5e2e\u52a9\uff0c\u8bf7\u53c2\u8003\u5176\u4ed6\u6587\u6863\u6216\u8054\u7cfb\u9879\u76ee\u7ef4\u62a4\u8005\u3002</p>"},{"location":"setupJob/","title":"\u4efb\u52a1\u914d\u7f6e","text":"<p>\u4e00\u4e2a\u91c7\u96c6\u4efb\u52a1\u5c31\u662f\u4e00\u4e2a JSON \u683c\u5f0f\u914d\u7f6e\u6587\u4ef6\uff0c\u8be5\u914d\u7f6e\u6587\u4ef6\u7684\u6a21\u677f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"job\": {\n    \"settings\": {},\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {},\n      \"transformer\": []\n    }\n  }\n}\n</code></pre> <p>\u4efb\u52a1\u914d\u7f6e\u7531 key \u4e3a <code>job</code> \u7684\u5b57\u5178\u7ec4\u6210\uff0c\u5176\u5b57\u5178\u5143\u7d20\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff1a</p> <ul> <li><code>settings</code>:  \u7528\u6765\u5b9a\u4e49\u672c\u6b21\u4efb\u52a1\u7684\u4e00\u4e9b\u63a7\u5236\u53c2\u6570\uff0c\u6bd4\u5982\u6307\u5b9a\u591a\u5c11\u7ebf\u7a0b\uff0c\u6700\u5927\u9519\u8bef\u7387\uff0c\u6700\u5927\u9519\u8bef\u8bb0\u5f55\u6761\u6570\u7b49\uff0c\u8fd9\u662f\u53ef\u9009\u914d\u7f6e\u3002</li> <li><code>reader</code>: \u7528\u6765\u914d\u7f6e\u6570\u636e\u8bfb\u53d6\u6240\u9700\u8981\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8fd9\u662f\u5fc5\u586b\u5185\u5bb9</li> <li><code>writer</code>: \u7528\u6765\u914d\u7f6e\u5199\u5165\u6570\u636e\u6240\u9700\u8981\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8fd9\u662f\u5fc5\u586b\u5185\u5bb9</li> <li><code>transformer</code>: \u6570\u636e\u8f6c\u6362\u89c4\u5219\uff0c\u5982\u679c\u9700\u8981\u5bf9\u8bfb\u53d6\u7684\u6570\u636e\u5728\u5199\u5165\u4e4b\u524d\u505a\u4e00\u4e9b\u53d8\u6362\uff0c\u53ef\u4ee5\u914d\u7f6e\u8be5\u9879\uff0c\u5426\u5219\u53ef\u4ee5\u4e0d\u914d\u7f6e</li> </ul>"},{"location":"setupJob/#reader","title":"reader \u914d\u7f6e\u9879","text":"<p><code>reader</code> \u914d\u7f6e\u9879\u4f9d\u636e\u4e0d\u540c\u7684 reader \u63d2\u4ef6\u800c\u6709\u4e9b\u5fae\u4e0d\u540c\uff0c\u4f46\u5927\u90e8\u5206\u7684\u914d\u7f6e\u57fa\u672c\u76f8\u540c\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5173\u7cfb\u578b\u6570\u636e\u5e93\u800c\u8a00\uff0c\u5176\u57fa\u672c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"name\": \"mysqlreader\",\n  \"parameter\": {\n    \"username\": \"\",\n    \"password\": \"\",\n    \"column\": [],\n    \"autoPk\": false,\n    \"splitPk\": \"\",\n    \"connection\": [\n      {\n        \"jdbcUrl\": [],\n        \"table\": []\n      }\n    ],\n    \"where\": \"\"\n  }\n}\n</code></pre> <p>\u5176\u4e2d <code>name</code> \u662f\u63d2\u4ef6\u7684\u540d\u79f0\uff0c\u6bcf\u4e2a\u63d2\u4ef6\u7684\u540d\u79f0\u90fd\u662f\u552f\u4e00\u7684\uff0c\u6bcf\u4e2a\u63d2\u4ef6\u66f4\u8be6\u7ec6\u7684\u914d\u7f6e\u53ef\u4ee5\u53c2\u8003\u8bfb\u53d6\u63d2\u4ef6\u7ae0\u8282\u7684\u5404\u63d2\u4ef6\u5185\u5bb9</p>"},{"location":"setupJob/#writer","title":"writer \u914d\u7f6e\u9879","text":"<p><code>writer</code> \u914d\u7f6e\u9879\u548c <code>reader</code> \u914d\u7f6e\u9879\u5dee\u4e0d\u591a\uff0c\u5176\u57fa\u672c\u6a21\u677f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"name\": \"mysqlwriter\",\n  \"parameter\": {\n    \"username\": \"\",\n    \"password\": \"\",\n    \"writeMode\": \"\",\n    \"column\": [],\n    \"session\": [],\n    \"preSql\": [],\n    \"postSql\": [],\n    \"connection\": [\n      {\n        \"jdbcUrl\": \"\",\n        \"table\": []\n      }\n    ]\n  }\n}\n</code></pre> <p>\u540c\u6837\u7684\uff0c\u8fd9\u91cc\u7684 <code>name</code> \u4e5f\u662f\u552f\u4e00\u7684\uff0c\u6bcf\u4e2a\u63d2\u4ef6\u66f4\u8be6\u7ec6\u7684\u914d\u7f6e\u53ef\u4ee5\u53c2\u8003\u5199\u5165\u63d2\u4ef6\u7ae0\u8282\u7684\u5404\u63d2\u4ef6\u5185\u5bb9</p>"},{"location":"setupJob/#settings","title":"settings \u914d\u7f6e\u9879","text":"<p><code>settings</code> \u53ef\u914d\u7f6e\u7684\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"speed\": {\n    \"byte\": -1,\n    \"record\": 100,\n    \"channel\": 1\n  },\n  \"errorLimit\": {\n    \"record\": 0,\n    \"percentage\": 0.02\n  }\n}\n</code></pre> <p>\u89e3\u91ca\u5982\u4e0b\uff1a</p>"},{"location":"setupJob/#speed","title":"<code>speed</code>","text":"<p>\u987e\u540d\u601d\u4e49\uff0c\u8fd9\u91cc\u662f\u7528\u6765\u505a\u6d41\u63a7\u7684\u914d\u7f6e\u9879\uff0c\u53ef\u4ee5\u4ece\u7f51\u7edc\u4f20\u8f93\u901f\u5ea6\uff0c\u6bcf\u79d2\u7684\u8bb0\u5f55\u6570\u4ee5\u53ca\u7ebf\u7a0b\u6570\u4e0a\u505a\u63a7\u5236\uff0c\u5206\u522b\u63cf\u8ff0\u5982\u4e0b\uff1a</p>"},{"location":"setupJob/#speedbyte","title":"<code>speed.byte</code>","text":"<p>\u8bbe\u7f6e\u6bcf\u79d2\u53ef\u83b7\u53d6\u7684\u5b57\u8282\u6570(Bps)\uff0c\u4e00\u822c\u662f\u4e3a\u4e86\u9632\u6b62\u6267\u884c\u4efb\u52a1\u65f6\u5c06\u6574\u4e2a\u5e26\u5bbd\u8dd1\u6ee1\uff0c\u4ece\u800c\u5f71\u54cd\u5230\u5176\u4ed6\u670d\u52a1\u3002\u5982\u679c\u4e0d\u505a\u9650\u5236\uff0c\u53ef\u8bbe\u7f6e\u4e3a <code>-1</code></p>"},{"location":"setupJob/#speedrecord","title":"<code>speed.record</code>","text":"<p>\u8bbe\u7f6e\u8bb0\u5f55\u6bcf\u79d2\u53ef\u83b7\u53d6\u7684\u6700\u5927\u8bb0\u5f55\u6761\u6570\uff0c\u8be5\u53c2\u6570\u9700\u8981\u548c <code>speed.byte</code> \u914d\u5408\u4f7f\u7528</p>"},{"location":"setupJob/#speedchannel","title":"<code>speed.channel</code>","text":"<p>\u8bbe\u7f6e\u901a\u9053\u6570\uff0c\u8be5\u901a\u9053\u8def\u786e\u5b9a\u4e86\u603b\u7684 Task \u7ebf\u7a0b\u6570\uff0c\u5047\u5b9a\u8bbe\u5b9a <code>speed.channel</code> \u4e3a 13\uff0c \u5219\u4e00\u5171\u6709 13 \u4e2a Task. \u7136\u540e\u6839\u636e <code>conf/core.json</code> \u914d\u7f6e\u4e2d\u7684 <code>taskGroup.channel</code> \u914d\u7f6e\u6765\u786e\u5b9a\u8981\u521b\u5efa\u7684 taskGroup \u6570\u91cf\u3002\u5373 <code>taskGroup = speed.channel / taskGroup.channel</code>.</p>"},{"location":"setupJob/#errorlimit","title":"<code>errorLimit</code>","text":"<p><code>errorLimit</code> \u7528\u6765\u914d\u7f6e\u5728\u6570\u636e\u5199\u5165\u62a5\u9519\u65f6\u7684\u884c\u4e3a\uff0c\u5177\u4f53\u5982\u4e0b</p>"},{"location":"setupJob/#errorlimitrecord","title":"<code>errorLimit.record</code>","text":"<p>\u5141\u8bb8\u9519\u8bef\u7684\u8bb0\u5f55\u6761\u6570\uff0c\u5982\u679c\u8d85\u8fc7\u8fd9\u4e2a\u6570\uff0c\u5219\u8ba4\u4e3a\u672c\u6b21\u4efb\u52a1\u5931\u8d25\uff0c\u5426\u5219\u8ba4\u4e3a\u6210\u529f</p>"},{"location":"setupJob/#errorlimitpercentage","title":"<code>errorLimit.percentage</code>","text":"<p>\u5141\u8bb8\u9519\u8bef\u8bb0\u5f55\u7684\u6bd4\u7387\uff0c\u8d85\u8fc7\u8fd9\u4e2a\u6bd4\u7387\uff0c\u5219\u8ba4\u4e3a\u672c\u6b21\u4efb\u52a1\u5931\u8d25\uff0c\u5426\u5219\u8ba4\u4e3a\u6210\u529f</p> <p>\u6ce8\u610f\uff0c\u4e0a\u8ff0\u53c2\u6570\u5728 <code>conf/core.json</code> \u914d\u7f6e\u6587\u4ef6\u5747\u6709\u9ed8\u8ba4\u914d\u7f6e\uff0c\u7528\u6765\u63a7\u5236\u5168\u5c40\u7684\u8bbe\u7f6e\u3002</p>"},{"location":"statsreport/","title":"\u4efb\u52a1\u7ed3\u679c\u4e0a\u62a5","text":""},{"location":"statsreport/#_2","title":"\u5feb\u901f\u4ecb\u7ecd","text":"<p>\u4e3b\u8981\u7528\u4e8e\u5c06\u4efb\u52a1\u6267\u884c\u7684\u7ed3\u679c\u4e0a\u62a5\u7ed9\u6307\u5b9a\u670d\u52a1\u5668\uff0cAddax \u4f7f\u7528 HTTP \u534f\u8bae\u4ee5 POST \u65b9\u5f0f\u628a\u4e0a\u62a5\u6570\u636e\u4ee5 JSON \u683c\u5f0f\u53d1\u9001\u7ed9\u6307\u5b9a\u7684\u670d\u52a1\u63a5\u53e3\u3002</p> <p>\u53d1\u9001\u7684\u6570\u636e\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"jobName\": \"test\",\n  \"startTimeStamp\": 1587971621,\n  \"endTimeStamp\": 1587971621,\n  \"totalCosts\": 10,\n  \"totalBytes\": 330,\n  \"byteSpeedPerSecond\": 33,\n  \"recordSpeedPerSecond\": 1,\n  \"totalReadRecords\": 6,\n  \"totalErrorRecords\": 0,\n  \"jobContent\": {\n    \"\u914d\u7f6e\u5185\u5bb9\u7701\u7565\": \"\u6b64\u5904\u4e3a\u5b9e\u9645\u4efb\u52a1\u914d\u7f6e\"\n  }\n}\n</code></pre> <p>\u670d\u52a1\u63a5\u53e3\u5728 <code>$ADDAX/conf/core.json</code> \u6587\u4ef6\u4e2d\u7684 <code>core.server.address</code> \u4e2d\u5b9a\u4e49\uff0c\u6bd4\u5982\uff1a</p> <pre><code>{\n  \"core\": {\n    \"server\": {\n      \"address\": \"http://localhost:9090/api/v1/addax/jobReport\",\n      \"timeout\": 5\n    }\n  }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684 http://localhost:9090/api/v1/addax/jobReport \u63a5\u53e3\u670d\u52a1\u9700\u8981\u81ea\u884c\u5f00\u53d1\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Python \u7684 <code>flask</code> \u5feb\u901f\u5f00\u53d1\u8fd9\u6837\u7684\u4e00\u4e2a\u63a5\u53e3\u670d\u52a1\uff1a</p> <pre><code>#!/bin/env python3\n# pip install flask\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n# \u5b9a\u4e49 POST \u63a5\u53e3\n@app.route('/api/v1/addax/jobReport', methods=['POST'])\ndef process_job():\n    # \u68c0\u67e5\u8bf7\u6c42\u662f\u5426\u4e3a JSON \u683c\u5f0f\n    if not request.is_json:\n        return jsonify({\"error\": \"Invalid request. JSON data is expected.\"}), 400\n\n    data = request.get_json()  # \u83b7\u53d6 JSON \u6570\u636e\n\n    # \u6253\u5370\u63a5\u6536\u5230\u7684 JSON \u6570\u636e\n    print(\"Received JSON data:\", data)\n\n    # \u53ef\u4ee5\u5728\u8fd9\u91cc\u6dfb\u52a0\u5177\u4f53\u7684\u6570\u636e\u5904\u7406\u903b\u8f91\n    # \u6bd4\u5982\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u8868\n\n    # \u8fd4\u56de\u6210\u529f\u54cd\u5e94\n    return jsonify({\"message\": \"Job data received successfully.\", \"received_data\": data}), 200\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=9090, debug=True)\n</code></pre> <p>Java \u4ee3\u7801\u793a\u4f8b\u53ef\u4ee5\u53c2\u8003 AddaxReportController.java</p> <p>\u4e0a\u8ff0\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\uff1a</p> \u53c2\u6570 \u63cf\u8ff0 \u5fc5\u9009 \u9ed8\u8ba4\u503c jobName \u4efb\u52a1\u540d \u662f jobName startTimeStamp \u4efb\u52a1\u6267\u884c\u7684\u5f00\u59cb\u65f6\u95f4 \u662f \u65e0 endTimeStamp \u4efb\u52a1\u6267\u884c\u7684\u7ed3\u675f\u65f6\u95f4 \u662f \u65e0 totalCosts \u4efb\u52a1\u603b\u8ba1\u8017\u65f6(s) \u662f \u65e0 totalBytes \u4efb\u52a1\u8bfb\u5199\u603b\u5b57\u8282\u6570 \u662f \u65e0 byteSpeedPerSecond \u4efb\u52a1\u5e73\u5747\u6d41\u91cf \u662f \u65e0 recordSpeedPerSecond \u8bb0\u5f55\u5199\u5165\u901f\u5ea6 \u662f \u65e0 totalReadRecords \u8bfb\u51fa\u8bb0\u5f55\u603b\u6570 \u662f 0 totalErrorRecords \u8bfb\u5199\u5931\u8d25\u603b\u6570 \u662f 0 jobContent \u672c\u6b21\u4efb\u52a1\u7684json\u6587\u4ef6 \u662f \u65e0 <p>\u4e0a\u8ff0\u53c2\u6570\u53ea\u6709 <code>jobName</code> \u53ef\u4ee5\u901a\u8fc7\u81ea\u884c\u4f20\u9012\u53c2\u6570\u7684\u65f6\u5019\u8bbe\u5b9a\uff0c\u5f53\u4f60\u4ee5</p> <pre><code>bin/addax.sh -p \"-DjobName=test\" job/job.json\n</code></pre> <p>\u6267\u884c\u91c7\u96c6\u4efb\u52a1\u65f6\uff0cPOST \u4f20\u9012\u7ed9\u63a5\u53e3\u7684 <code>jobName</code> \u5c31\u662f\u4e0a\u8ff0\u6307\u5b9a\u7684 <code>test</code> \u503c\u3002</p> <p>\u5982\u679c\u4e0d\u6307\u5b9a\uff0c\u5219 <code>Addax</code> \u7a0b\u5e8f\u5185\u90e8\u4f1a\u751f\u6210 <code>jobName</code> \u503c\uff0c\u4f46\u751f\u6210\u7684\u903b\u8f91\u662f\u5047\u5b9a\u4f60\u7684\u91c7\u96c6\u4efb\u52a1\u662f\u5199\u6570\u636e\u5230 Hadoop HDFS \u6587\u4ef6\u7cfb\u7edf\u4e0a\u3002 \u5177\u4f53\u903b\u8f91\u5982\u4e0b\uff1a</p> <ol> <li>json \u6587\u4ef6\u662f\u5426 <code>writer</code> \u63d2\u4ef6\u662f\u5426\u6709 <code>writer.parameters.path</code> \u503c\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u8bbe\u5b9a\u503c\u4e3a <code>jobName</code>\uff0c\u5426\u5219</li> <li>\u53d6 <code>writer.parameters.path</code> \u503c\uff0c\u6309 <code>/</code> \u5206\u5272\u540e\u53d6\u7b2c2\uff0c3\u5217\u7528\u70b9(.)\u62fc\u63a5\u800c\u6210\uff0c\u5176\u542b\u4e49\u662f\u4e3a\u5e93\u540d\u53ca\u8868\u540d</li> </ol> <p>\u5047\u5b9a\u4f60\u7684 json \u4efb\u52a1\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mysqlreader\",\n        \"parameter\": {\n          \"username\": \"username\",\n          \"password\": \"password\",\n          \"column\": [\n            \"*\"\n          ],\n          \"autoPk\": \"true\",\n          \"connection\": {\n            \"table\": [\n              \"tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:mysql://example.com:3306/example_db\"\n          },\n          \"where\": \"\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hdfswriter\",\n        \"parameter\": {\n          \"defaultFS\": \"hdfs://yytz\",\n          \"fileType\": \"orc\",\n          \"path\": \"/ods/odstl/tbl/logdate=${logdate}\",\n          \"fileName\": \"addax\",\n          \"column\": [\n            \"\u7701\u7565\u7684\u5b57\u6bb5\u914d\u7f6e\"\n          ],\n          \"writeMode\": \"overwrite\",\n          \"fieldDelimiter\": \"\\u0001\",\n          \"compress\": \"lz4\",\n          \"haveKerberos\": \"false\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u90a3\u4e48\u5148\u53d6\u51fa <code>/ods/odstl/tbl/logdate=${logdate}</code>\uff0c\u7136\u540e\u6309\u7167 <code>/</code> \u5207\u5206\uff0c\u83b7\u53d6\u7b2c\u4e8c\u9879 <code>odstl</code>\uff0c\u7b2c\u4e09\u9879 <code>tbl</code>\uff0c\u7136\u540e\u62fc\u63a5\u6210 <code>odstl.tbl</code> \u8fd9\u4e2a\u503c\u5c31\u662f <code>jobName</code> \u503c</p>"},{"location":"transformer/","title":"\u6570\u636e\u8f6c\u6362","text":""},{"location":"transformer/#transformer","title":"Transformer \u5b9a\u4e49","text":"<p>\u5728\u6570\u636e\u540c\u6b65\u3001\u4f20\u8f93\u8fc7\u7a0b\u4e2d\uff0c\u5b58\u5728\u7528\u6237\u5bf9\u4e8e\u6570\u636e\u4f20\u8f93\u8fdb\u884c\u7279\u6b8a\u5b9a\u5236\u5316\u7684\u9700\u6c42\u573a\u666f\uff0c\u5305\u62ec\u88c1\u526a\u5217\u3001\u8f6c\u6362\u5217\u7b49\u5de5\u4f5c\uff0c\u53ef\u4ee5\u501f\u52a9ETL\u7684T\u8fc7\u7a0b\u5b9e\u73b0(Transformer)\u3002Addax\u5305\u542b\u4e86\u5b8c\u6210\u7684E(Extract)\u3001T(Transformer)\u3001L(Load)\u652f\u6301\u3002</p>"},{"location":"transformer/#_2","title":"\u8fd0\u884c\u6a21\u578b","text":"<pre><code>graph LR\nsource((\"source\"))\nsubgraph fr[\"Addax Framework\"]\n    direction LR\n    Reader ==&gt; Transformer ==&gt;Writer\nend\ntarget((\"target\"))\nsource ==&gt; fr ==&gt; target</code></pre>"},{"location":"transformer/#udf","title":"UDF \u51fd\u6570","text":""},{"location":"transformer/#dx_substr","title":"dx_substr","text":"<p><code>dx_substr(idx, pos, length) -&gt; str</code></p> <p>\u53c2\u6570</p> <ul> <li><code>idx</code>: \u5b57\u6bb5\u7f16\u53f7\uff0c\u5bf9\u5e94record\u4e2d\u7b2c\u51e0\u4e2a\u5b57\u6bb5</li> <li><code>pos</code>: \u5b57\u6bb5\u503c\u7684\u5f00\u59cb\u4f4d\u7f6e</li> <li><code>length</code>: \u76ee\u6807\u5b57\u6bb5\u957f\u5ea6</li> </ul> <p>\u8fd4\u56de\uff1a \u4ece\u5b57\u7b26\u4e32\u7684\u6307\u5b9a\u4f4d\u7f6e\uff08\u5305\u542b\uff09\u622a\u53d6\u6307\u5b9a\u957f\u5ea6\u7684\u5b57\u7b26\u4e32\u3002\u5982\u679c\u5f00\u59cb\u4f4d\u7f6e\u975e\u6cd5\u629b\u51fa\u5f02\u5e38\u3002\u5982\u679c\u5b57\u6bb5\u4e3a\u7a7a\u503c\uff0c\u76f4\u63a5\u8fd4\u56de\uff08\u5373\u4e0d\u53c2\u4e0e\u672ctransformer\uff09</p>"},{"location":"transformer/#dx_pad","title":"dx_pad","text":"<p><code>dx_pad(idx, flag, length, chr)</code></p> <p>\u53c2\u6570</p> <ul> <li><code>idx</code>: \u5b57\u6bb5\u7f16\u53f7\uff0c\u5bf9\u5e94record\u4e2d\u7b2c\u51e0\u4e2a\u5b57\u6bb5</li> <li><code>flag</code>: \"l\",\"r\", \u6307\u793a\u662f\u5728\u5934\u8fdb\u884c\u586b\u5145\uff0c\u8fd8\u662f\u5c3e\u8fdb\u884c\u586b\u5145</li> <li><code>length</code>: \u76ee\u6807\u5b57\u6bb5\u957f\u5ea6</li> <li><code>chr</code>: \u9700\u8981\u586b\u5145\u7684\u5b57\u7b26</li> </ul> <p>\u8fd4\u56de\uff1a \u5982\u679c\u6e90\u5b57\u7b26\u4e32\u957f\u5ea6\u5c0f\u4e8e\u76ee\u6807\u5b57\u6bb5\u957f\u5ea6\uff0c\u6309\u7167\u4f4d\u7f6e\u6dfb\u52a0pad\u5b57\u7b26\u540e\u8fd4\u56de\u3002\u5982\u679c\u957f\u4e8e\uff0c\u76f4\u63a5\u622a\u65ad\uff08\u90fd\u622a\u53f3\u8fb9\uff09\u3002\u5982\u679c\u5b57\u6bb5\u4e3a\u7a7a\u503c\uff0c\u8f6c\u6362\u4e3a\u7a7a\u5b57\u7b26\u4e32\u8fdb\u884cpad\uff0c\u5373\u6700\u540e\u7684\u5b57\u7b26\u4e32\u5168\u662f\u9700\u8981pad\u7684\u5b57\u7b26</p> <p>\u4e3e\u4f8b\uff1a</p> <ul> <li><code>dx_pad(1,\"l\",\"4\",\"A\")</code>: \u5982\u679c <code>column 1</code> \u7684\u503c\u4e3a <code>xyz=&gt; Axyz</code>\uff0c \u5219\u8f6c\u6362\u540e\u7684\u503c\u4e3a <code>xyzzzzz =&gt; xyzz</code></li> <li><code>dx_pad(1,\"r\",\"4\",\"A\")</code>, \u5982\u679c <code>column 1</code> \u7684\u503c\u4e3a <code>xyz=&gt; xyzA</code>\uff0c \u503c\u4e3a <code>xyzzzzz =&gt; xyzz</code></li> </ul>"},{"location":"transformer/#dx_replace","title":"dx_replace","text":"<p><code>dx_replace(idx, pos, length, str) -&gt; str</code></p> <p>\u53c2\u6570</p> <ul> <li><code>idx</code>: \u5b57\u6bb5\u7f16\u53f7\uff0c\u5bf9\u5e94record\u4e2d\u7b2c\u51e0\u4e2a\u5b57\u6bb5</li> <li><code>pos</code>: \u5b57\u6bb5\u503c\u7684\u5f00\u59cb\u4f4d\u7f6e</li> <li><code>length</code>: \u9700\u8981\u66ff\u6362\u7684\u5b57\u6bb5\u957f\u5ea6</li> <li><code>str</code>: \u8981\u66ff\u6362\u7684\u5b57\u7b26\u4e32</li> </ul> <p>\u8fd4\u56de\uff1a \u4ece\u5b57\u7b26\u4e32\u7684\u6307\u5b9a\u4f4d\u7f6e\uff08\u5305\u542b\uff09\u66ff\u6362\u6307\u5b9a\u957f\u5ea6\u7684\u5b57\u7b26\u4e32\u3002\u5982\u679c\u5f00\u59cb\u4f4d\u7f6e\u975e\u6cd5\u629b\u51fa\u5f02\u5e38\u3002\u5982\u679c\u5b57\u6bb5\u4e3a\u7a7a\u503c\uff0c\u76f4\u63a5\u8fd4\u56de\uff08\u5373\u4e0d\u53c2\u4e0e\u672ctransformer\uff09</p> <p>\u4e3e\u4f8b\uff1a</p> <ul> <li><code>dx_replace(1,\"2\",\"4\",\"****\")</code>:  \u5982\u679c <code>column 1</code> \u7684\u503c\u4e3a <code>addaxTest</code>, \u5219\u8f6c\u6362\u4e3a <code>da****est</code></li> <li><code>dx_replace(1,\"5\",\"10\",\"****\")</code>  \u5982\u679c <code>column 1</code> \u7684\u503c\u4e3a <code>addaxTest</code> \u5219\u8f6c\u6362\u4e3a <code>data****</code></li> </ul>"},{"location":"transformer/#dx_filter","title":"dx_filter","text":"<p><code>dx_filter(idx, operator, expr) -&gt; str</code></p> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>idx</code>: \u5b57\u6bb5\u7f16\u53f7\uff0c\u5bf9\u5e94record\u4e2d\u7b2c\u51e0\u4e2a\u5b57\u6bb5</li> <li><code>operator</code>: \u8fd0\u7b97\u7b26, \u652f\u6301 <code>like</code>, <code>not like</code>, <code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code></li> <li><code>expr</code>: \u6b63\u5219\u8868\u8fbe\u5f0f\uff08java\u6b63\u5219\u8868\u8fbe\u5f0f\uff09\u3001\u503c</li> <li><code>str</code>: \u8981\u66ff\u6362\u7684\u5b57\u7b26\u4e32</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u5982\u679c\u5339\u914d\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u8fd4\u56deNull\uff0c\u8868\u793a\u8fc7\u6ee4\u8be5\u884c\u3002\u4e0d\u5339\u914d\u8868\u8fbe\u5f0f\u65f6\uff0c\u8868\u793a\u4fdd\u7559\u8be5\u884c\u3002\uff08\u6ce8\u610f\u662f\u8be5\u884c\uff09\u3002\u5bf9\u4e8e <code>&gt;</code>, <code>=</code>, <code>&lt;</code>\u90fd\u662f\u5bf9\u5b57\u6bb5\u76f4\u63a5compare\u7684\u7ed3\u679c.</li> <li><code>like</code> \uff0c <code>not like</code> \u662f\u5c06\u5b57\u6bb5\u8f6c\u6362\u6210\u5b57\u7b26\u7c7b\u578b\uff0c\u7136\u540e\u548c\u76ee\u6807\u6b63\u5219\u8868\u8fbe\u5f0f\u8fdb\u884c\u5168\u5339\u914d\u3002</li> <li><code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code> ,\u6309\u7167\u7c7b\u578b\u8fdb\u884c\u6bd4\u8f83, \u6570\u503c\u7c7b\u578b\u6309\u5927\u5c0f\u6bd4\u8f83,\u5b57\u7b26\u53ca\u5e03\u5c14\u7c7b\u578b\u6309\u7167\u5b57\u5178\u5e8f\u6bd4\u8f83</li> <li>\u5982\u679c\u76ee\u6807\u5b57\u6bb5\u4e3a\u7a7a\uff08null\uff09\uff0c\u5bf9\u4e8e <code>= null</code> \u7684\u8fc7\u6ee4\u6761\u4ef6\uff0c\u5c06\u6ee1\u8db3\u6761\u4ef6\uff0c\u88ab\u8fc7\u6ee4\u3002<code>\uff01=null</code> \u7684\u8fc7\u6ee4\u6761\u4ef6\uff0cnull\u4e0d\u6ee1\u8db3\u8fc7\u6ee4\u6761\u4ef6\uff0c\u4e0d\u88ab\u8fc7\u6ee4\u3002 <code>like</code>\uff0c\u5b57\u6bb5\u4e3anull\u4e0d\u6ee1\u8db3\u6761\u4ef6\uff0c\u4e0d\u88ab\u8fc7\u6ee4\uff0c\u548c <code>not like</code>\uff0c\u5b57\u6bb5\u4e3anull\u6ee1\u8db3\u6761\u4ef6\uff0c\u88ab\u8fc7\u6ee4\u3002</li> </ul> <p>\u4e3e\u4f8b</p> <ul> <li><code>dx_filter(1,\"like\",\"dataTest\")</code> </li> <li><code>dx_filter(1,\"&gt;=\",\"10\")</code></li> </ul> <p>\u5173\u8054filter\u6682\u4e0d\u652f\u6301\uff0c\u5373\u591a\u4e2a\u5b57\u6bb5\u7684\u8054\u5408\u5224\u65ad\uff0c\u51fd\u53c2\u592a\u8fc7\u590d\u6742\uff0c\u7528\u6237\u96be\u4ee5\u4f7f\u7528\u3002</p>"},{"location":"transformer/#dx_groovy","title":"dx_groovy","text":"<p><code>dx_groovy(code, package) -&gt; record</code></p> <p>\u53c2\u6570</p> <ul> <li><code>coee</code>: \u7b26\u5408 groovy \u7f16\u7801\u8981\u6c42\u7684\u4ee3\u7801</li> <li><code>package</code>: extraPackage, \u5217\u8868\u6216\u8005\u4e3a\u7a7a</li> </ul> <p>\u8fd4\u56de</p> <p>Record \u6570\u636e\u7c7b\u578b</p> <p>\u6ce8\u610f\uff1a</p> <ul> <li><code>dx_groovy</code> \u53ea\u80fd\u8c03\u7528\u4e00\u6b21\u3002\u4e0d\u80fd\u591a\u6b21\u8c03\u7528\u3002</li> <li><code>groovy code</code> \u4e2d\u652f\u6301 <code>java.lang</code>, <code>java.util</code> \u7684\u5305\uff0c\u53ef\u76f4\u63a5\u5f15\u7528\u7684\u5bf9\u8c61\u6709 <code>record</code>   \uff0c\u4ee5\u53caelement\u4e0b\u7684\u5404\u79cdcolumn\uff08BoolColumn.class,BytesColumn.class,DateColumn.class,DoubleColumn.class,LongColumn.class,StringColumn.class\uff09\u3002   \u4e0d\u652f\u6301\u5176\u4ed6\u5305\uff0c\u5982\u679c\u7528\u6237\u6709\u9700\u8981\u7528\u5230\u5176\u4ed6\u5305\uff0c\u53ef\u8bbe\u7f6eextraPackage\uff0c\u6ce8\u610fextraPackage\u4e0d\u652f\u6301\u7b2c\u4e09\u65b9jar\u5305\u3002</li> <li><code>groovy code</code> \u4e2d\uff0c\u8fd4\u56de\u66f4\u65b0\u8fc7\u7684 <code>Record</code>\uff08\u6bd4\u5982record.setColumn(columnIndex, new StringColumn(newValue));\uff09\uff0c\u6216\u8005null\u3002\u8fd4\u56denull\u8868\u793a\u8fc7\u6ee4\u6b64\u884c\u3002</li> <li>\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528\u9759\u6001\u7684Util\u65b9\u5f0f\uff08GroovyTransformerStaticUtil)</li> </ul> <p>\u4e3e\u4f8b:</p> <p>groovy \u5b9e\u73b0\u7684 subStr</p> <pre><code>String code=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = oriValue.substring(0, 3);\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n        dx_groovy(record);\n</code></pre> <p>groovy \u5b9e\u73b0\u7684Replace</p> <pre><code>String code2=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = \\\"****\\\" + oriValue.substring(3, oriValue.length());\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>groovy \u5b9e\u73b0\u7684Pad</p> <pre><code>String code3=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String padString = \\\"12345\\\";\\n\"+\n        \" String finalPad = \\\"\\\";\\n\"+\n        \" int NeedLength = 8 - oriValue.length();\\n\"+\n        \"        while (NeedLength &gt; 0) {\\n\"+\n        \"\\n\"+\n        \"            if (NeedLength &gt;= padString.length()) {\\n\"+\n        \"                finalPad += padString;\\n\"+\n        \"                NeedLength -= padString.length();\\n\"+\n        \"            } else {\\n\"+\n        \"                finalPad += padString.substring(0, NeedLength);\\n\"+\n        \"                NeedLength = 0;\\n\"+\n        \"            }\\n\"+\n        \"        }\\n\"+\n        \" String newValue= finalPad + oriValue;\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>\u4ece  <code>4.1.2</code> \u7248\u672c\u5f00\u59cb\uff0c <code>dx_groovy</code> \u652f\u6301\u4ece\u5916\u90e8\u6587\u4ef6\u52a0\u8f7d groovy \u4ee3\u7801\uff0c\u8bfb\u53d6\u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u4e3a <code>$ADDAX_HOME</code> \u53d8\u91cf\u6240\u5728\u7684\u76ee\u5f55\uff0c\u4e5f\u5c31\u662f Addax \u7684\u5b89\u88c5\u76ee\u5f55\u3002</p> <p>\u4ee5\u5b9e\u73b0 <code>subStr</code> \u4e3a\u4f8b\uff0c\u6211\u4eec\u53ef\u4ee5\u521b\u5efa <code>job/substr.groovy</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>Column column = record.getColumn(1)\nString oriValue = column.asString()\nString newValue = oriValue.substring(0, 3)\nrecord.setColumn(1, new StringColumn(newValue))\nreturn record\n</code></pre> <p>\u7136\u540e\u5728 <code>job</code> \u6587\u4ef6\u4e2d\u8fd9\u6837\u53bb\u5b9a\u4e49\uff1a</p> <pre><code>{\n  \"transformer\": [\n    {\n      \"name\": \"dx_groovy\",\n      \"parameter\": {\n        \"codeFile\": \"job/substr.groovy\"\n      }\n    }\n  ]\n}\n</code></pre> <p>\u6587\u4ef6\u4e5f\u53ef\u4ee5\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\u6765\u6307\u5b9a\u3002</p>"},{"location":"transformer/#job","title":"Job\u5b9a\u4e49","text":"<p>\u672c\u4f8b\u4e2d\uff0c\u914d\u7f6e4\u4e2aUDF\u3002</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"My name is xxxx\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"password is Passw0rd\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"random\": \"0,10\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      },\n      \"transformer\": [\n        {\n          \"name\": \"dx_replace\",\n          \"parameter\": {\n            \"columnIndex\": 0,\n            \"paras\": [\n              \"11\",\n              \"6\",\n              \"wgzhao\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_substr\",\n          \"parameter\": {\n            \"columnIndex\": 1,\n            \"paras\": [\n              \"0\",\n              \"12\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_map\",\n          \"parameter\": {\n            \"columnIndex\": 2,\n            \"paras\": [\n              \"^\",\n              \"2\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_filter\",\n          \"parameter\": {\n            \"columnIndex\": 6,\n            \"paras\": [\n              \"&lt;\",\n              \"5\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"transformer/#_3","title":"\u81ea\u5b9a\u4e49\u51fd\u6570","text":"<p>\u5982\u679c\u81ea\u5e26\u7684\u51fd\u6570\u4e0d\u6ee1\u8db3\u6570\u636e\u8f6c\u6362\u8981\u6c42\uff0c\u6211\u4eec\u53ef\u4ee5\u5728 <code>transformer</code> \u7f16\u5199\u6ee1\u8db3 <code>groovy</code> \u89c4\u8303\u8981\u6c42\u7684\u4ee3\u7801\uff0c\u4e0b\u9762\u7ed9\u51fa\u4e00\u4e2a\u5b8c\u6574\u7684\u4f8b\u5b50</p> <pre><code>\n</code></pre> <p>\u4e0a\u8ff0 <code>transformer</code> \u4ee3\u7801\u9488\u5bf9\u6bcf\u6761\u8bb0\u5f55\u7684\u524d\u9762\u4e24\u4e2a\u5b57\u6bb5\u505a\u4e86\u4fee\u6539\uff0c\u5bf9\u7b2c\u4e00\u4e2a\u5b57\u6bb5\u7684\u5b57\u7b26\u4e32\uff0c\u5728\u5b57\u7b26\u4e32\u524d\u9762\u589e\u52a0 <code>Header_</code> \u5b57\u7b26\uff1b  \u7b2c\u4e8c\u4e2a\u6574\u6570\u5b57\u6bb5\u503c\u8fdb\u884c\u500d\u589e\u5904\u7406\u3002\u6700\u540e\u6267\u884c\u7684\u7ed3\u679c\u5982\u4e0b\uff1a</p> <pre><code>$ bin/addax.sh job/transformer_demo.json \n\n  ___      _     _            \n / _ \\    | |   | |           \n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt; \n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-04 15:45:56.421 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-04 15:45:56.443 [        main] INFO  Engine               - \n\n.....\n\n2021-08-04 15:45:56.458 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-04 15:45:56.459 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-04 15:45:56.460 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-04 15:45:56.470 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-04 15:45:56.498 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-04 15:45:56.505 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-08-04 15:45:56.520 [ taskGroup-0] INFO  TransformerUtil      -  user config transformers [[dx_groovy]], loading...\n2021-08-04 15:45:56.531 [ taskGroup-0] INFO  TransformerUtil      -  1 of transformer init success. name=dx_groovy, isNative=true parameter = \n  {\"code\":\"record.setColumn(0, new StringColumn('Header_' + record.getColumn(0).asString()));record.setColumn(1, new LongColumn(record.getColumn(1).asLong() * 2));return record;\"}\n\nHeader_Addax    2       1989-06-04 00:00:01     true    test\nHeader_Addax    4       1989-06-03 00:00:01     true    test\nHeader_Addax    6       1989-06-02 00:00:01     true    test\nHeader_Addax    8       1989-06-01 00:00:01     true    test\nHeader_Addax    10      1989-05-31 00:00:01     true    test\nHeader_Addax    12      1989-05-30 00:00:01     true    test\nHeader_Addax    14      1989-05-29 00:00:01     true    test\nHeader_Addax    16      1989-05-28 00:00:01     true    test\nHeader_Addax    18      1989-05-27 00:00:01     true    test\nHeader_Addax    20      1989-05-26 00:00:01     true    test\n\n2021-08-04 15:45:59.515 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-04 15:45:59.517 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-04 15:45:59.518 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-08-04 15:45:59.521 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-04 15:45:59.524 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 330 bytes | Speed 110B/s, 3 records/s | Error 0 records, 0 bytes |  \n  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Transformer Success 10 records | Transformer Error 0 records | Transformer Filter 0 records \n  | Transformer usedTime 0.383s | Percentage 100.00%\n2021-08-04 15:45:59.527 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-04 15:45:56\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-04 15:45:59\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              110B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n\n2021-08-04 15:45:59.528 [       job-0] INFO  JobContainer         - \nTransformer\u6210\u529f\u8bb0\u5f55\u603b\u6570         :                  10\nTransformer\u5931\u8d25\u8bb0\u5f55\u603b\u6570         :                   0\nTransformer\u8fc7\u6ee4\u8bb0\u5f55\u603b\u6570         :                   0\n</code></pre>"},{"location":"transformer/#_4","title":"\u8ba1\u91cf\u548c\u810f\u6570\u636e","text":"<p>Transform\u8fc7\u7a0b\u6d89\u53ca\u5230\u6570\u636e\u7684\u8f6c\u6362\uff0c\u53ef\u80fd\u9020\u6210\u6570\u636e\u7684\u589e\u52a0\u6216\u51cf\u5c11\uff0c\u56e0\u6b64\u66f4\u52a0\u9700\u8981\u7cbe\u786e\u5ea6\u91cf\uff0c\u5305\u62ec\uff1a</p> <ul> <li>Transform\u7684\u5165\u53c2Record\u6761\u6570\u3001\u5b57\u8282\u6570\u3002</li> <li>Transform\u7684\u51fa\u53c2Record\u6761\u6570\u3001\u5b57\u8282\u6570\u3002</li> <li>Transform\u7684\u810f\u6570\u636eRecord\u6761\u6570\u3001\u5b57\u8282\u6570\u3002</li> <li>\u5982\u679c\u662f\u591a\u4e2aTransform\uff0c\u67d0\u4e00\u4e2a\u53d1\u751f\u810f\u6570\u636e\uff0c\u5c06\u4e0d\u4f1a\u518d\u8fdb\u884c\u540e\u9762\u7684transform\uff0c\u76f4\u63a5\u7edf\u8ba1\u4e3a\u810f\u6570\u636e\u3002</li> <li>\u76ee\u524d\u53ea\u63d0\u4f9b\u4e86\u6240\u6709Transform\u7684\u8ba1\u91cf\uff08\u6210\u529f\uff0c\u5931\u8d25\uff0c\u8fc7\u6ee4\u7684count\uff0c\u4ee5\u53catransform\u7684\u6d88\u8017\u65f6\u95f4\uff09\u3002</li> </ul> <p>\u6d89\u53ca\u5230\u8fd0\u884c\u8fc7\u7a0b\u7684\u8ba1\u91cf\u6570\u636e\u5c55\u73b0\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <pre><code>Total 1000000 records, 22000000 bytes | Transform 100000 records(in), 10000 records(out) | Speed 2.10MB/s, 100000 records/s | Error 0 records, 0 bytes | Percentage 100.00%\n</code></pre> <p>\u6ce8\u610f\uff0c\u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u8f6c\u6362\u7684\u8f93\u5165\u8f93\u51fa\uff0c\u9700\u8981\u68c0\u6d4b\u6570\u636e\u8f93\u5165\u8f93\u51fa\u7684\u8bb0\u5f55\u6570\u91cf\u53d8\u5316\u3002</p> <p>\u6d89\u53ca\u5230\u6700\u7ec8\u4f5c\u4e1a\u7684\u8ba1\u91cf\u6570\u636e\u5c55\u73b0\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <pre><code>\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2015-03-10 17:34:21\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2015-03-10 17:34:31\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                 10s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.10MB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :         100000rec/s\n\u8f6c\u6362\u8f93\u5165\u603b\u6570                    :             1000000\n\u8f6c\u6362\u8f93\u51fa\u603b\u6570                    :             1000000\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :             1000000\n\u540c\u6b65\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre> <p>\u6ce8\u610f\uff0c\u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u8f6c\u6362\u7684\u8f93\u5165\u8f93\u51fa\uff0c\u9700\u8981\u68c0\u6d4b\u6570\u636e\u8f93\u5165\u8f93\u51fa\u7684\u8bb0\u5f55\u6570\u91cf\u53d8\u5316\u3002</p>"},{"location":"en/","title":"Addax Introduction","text":""},{"location":"en/#overview","title":"Overview","text":"<p>Addax is a heterogeneous data source offline synchronization tool originally derived from Alibaba's DataX, dedicated to implementing stable and efficient data synchronization between various heterogeneous data sources including relational databases (MySQL, Oracle, etc.), HDFS, Hive, HBase, FTP, and more.</p> <p></p> <p>To solve the problem of heterogeneous data source synchronization, Addax transforms complex network synchronization links into star-shaped data links. Addax serves as the intermediate transmission carrier responsible for connecting various data sources. When a new data source needs to be integrated, you only need to connect this data source to Addax to achieve seamless data synchronization with existing data sources.</p>"},{"location":"en/#framework-design","title":"Framework Design","text":"<pre><code>graph LR\nMySQL\nsubgraph Addax\n    direction LR\n    subgraph reader[\"Reader Plugin\"]\n        mr[\"MySQLReader\"]\n    end\n    subgraph writer[\"Writer Plugin\"]\n    hw[\"HDFSWriter\"]\n    end\n    Framework\n    mr --&gt; Framework --&gt; writer\nend\n\nMySQL ==&gt; Addax ==&gt; HDFS\n</code></pre> <p>Addax serves as an offline data synchronization framework, built with a Framework + plugin architecture. It abstracts data source reading and writing into Reader/Writer plugins, which are integrated into the entire synchronization framework.</p> <ul> <li>Reader: The Reader is the data collection module, responsible for collecting data from data sources and sending it to the Framework.</li> <li>Writer: The Writer is the data writing module, responsible for continuously fetching data from the Framework and writing it to the destination.</li> <li>Framework: The Framework connects Reader and Writer, serving as the data transmission channel between them, handling buffering, flow control, concurrency, data transformation, and other core technical issues.</li> </ul> <p>Addax Framework provides simple interfaces for plugin interaction and a simple plugin integration mechanism. By simply adding a plugin, you can seamlessly connect to other data sources.</p>"},{"location":"en/#core-architecture","title":"Core Architecture","text":"<p>This section uses a sequence diagram of an Addax job lifecycle to briefly explain the relationships between various modules from an overall architectural design perspective.</p> <pre><code>graph TB\nsubgraph Job\nend\nsubgraph task\n  direction TB\n  t1[\"Task\"]\n  t2[\"Task\"]\n  t3[\"Task\"]\n  t4[\"Task\"]\n  t5[\"Task\"]\n  t6[\"Task\"]\nend\nsubgraph taskgroup[\" \"]\n    direction TB\n  subgraph tg1[\"TaskGroup\"]\n    subgraph tg1_Task[\"Task\"]\n      tg1_r[\"Reader\"]\n      tg1_c[\"Channel\"]\n      tg1_w[\"Writer\"]\n    end\n    t7[\"Task\"]\n    t8[\"Task\"]\n  end\n\n  subgraph tg2[\"TaskGroup\"]\n    subgraph tg2_Task[\"Task\"]\n      direction LR\n      tg2_r[\"Reader\"]\n      tg2_c[\"Channel\"]\n      tg2_w[\"Writer\"]\n    end\n    t9[\"Task\"]\n    t10[\"Task\"]\n  end\n\n  subgraph tg3[\"TaskGroup\"]\n    direction LR\n    subgraph tg3_Task[\"Task\"]\n      tg3_r[\"Reader\"]\n      tg3_c[\"Channel\"]\n      tg3_w[\"Writer\"]\n    end\n    t11[\"Task\"]\n    t12[\"Task\"]\n  end\nend\n\nJob == split ==&gt; task\ntask == Schedule ==&gt; taskgroup</code></pre>"},{"location":"en/#core-module-introduction","title":"Core Module Introduction","text":"<ol> <li>Addax completes a single data synchronization job, which we call a Job. After Addax receives a Job, it will start a process to complete the entire job synchronization process. The Addax Job module is the central management node for a single job, responsible for data cleaning, sub-task splitting (converting a single job calculation into multiple sub-Tasks), TaskGroup management, and other functions.</li> <li>After the Addax Job starts, it will split the Job into multiple small Tasks (sub-tasks) according to different source-side splitting strategies for concurrent execution. Task is the smallest unit of Addax jobs, and each Task is responsible for a portion of the data synchronization work.</li> <li>After splitting multiple Tasks, the Addax Job will call the Scheduler module, which reorganizes the split Tasks into TaskGroups according to the configured concurrency. Each TaskGroup is responsible for running all assigned Tasks with a certain degree of concurrency. The default concurrency of a single task group is 5.</li> <li>Each Task is started by the TaskGroup. After the Task starts, it will start the <code>Reader\u2014&gt;Channel\u2014&gt;Writer</code> thread to complete the task synchronization work.</li> <li>After the Addax job runs, the Job monitors and waits for multiple TaskGroup modules to complete tasks. After all TaskGroup tasks are completed, the Job exits successfully. Otherwise, it exits abnormally with a non-zero exit code.</li> </ol>"},{"location":"en/#scheduling-process","title":"Scheduling Process","text":"<p>For example, a user submitted a job configured with 20 concurrent threads, aiming to synchronize data from 100 MySQL sub-tables to Oracle. The scheduling decision logic is:</p> <ol> <li>Addax Job splits into 100 Tasks based on database and table partitioning.</li> <li>Based on 20 concurrent threads, it calculates that <code>20/5 = 4</code> TaskGroups need to be allocated.</li> <li>The 4 TaskGroups evenly distribute the 100 split Tasks, with each TaskGroup responsible for running 25 Tasks with 5 concurrent threads.</li> </ol>"},{"location":"en/#core-advantages","title":"Core Advantages","text":""},{"location":"en/#reliable-data-quality-monitoring","title":"Reliable Data Quality Monitoring","text":"<ul> <li>Perfect solution for data transmission type distortion issues</li> </ul> <p>Supports all strong data types, each plugin has its own data type conversion strategy, allowing data to be transmitted to the destination completely and without loss.</p> <ul> <li>Provides full-link traffic and data volume runtime monitoring for jobs</li> </ul> <p>During operation, comprehensive displays of job status, data traffic, data speed, execution progress, and other information can be provided, allowing users to understand job status in real-time. It can also intelligently compare the speed between source and destination during job execution, providing users with more performance troubleshooting information.</p> <ul> <li>Provides dirty data detection</li> </ul> <p>In the process of large-scale data transmission, many data transmission errors (such as type conversion errors) will inevitably occur due to various reasons. Addax considers such data as dirty data. Addax can currently achieve precise filtering, identification, collection, and display of dirty data, providing users with multiple dirty data processing modes to accurately control data quality!</p>"},{"location":"en/#rich-data-transformation-functions","title":"Rich Data Transformation Functions","text":"<p>As an ETL tool serving big data, in addition to providing data snapshot migration functions, it also provides rich data transformation functions, allowing data to easily complete data desensitization, completion, filtering, and other data transformation functions during transmission. It also provides automatic <code>groovy</code> functions, allowing users to customize transformation functions. For details, please see the transformer detailed introduction.</p>"},{"location":"en/#precise-speed-control","title":"Precise Speed Control","text":"<p>Provides three flow control modes including channel (concurrency), record flow, and byte flow, allowing you to control your job speed at will, letting your job achieve optimal synchronization speed within the range that the database can bear.</p> <pre><code>{\n  \"speed\": {\n    \"channel\": 5,\n    \"byte\": 1048576,\n    \"record\": 10000\n  }\n}\n</code></pre>"},{"location":"en/#strong-synchronization-performance","title":"Strong Synchronization Performance","text":"<p>Each reader plugin has one or more splitting strategies, all of which can reasonably split jobs into multiple Tasks for parallel execution. The single-machine multi-threaded execution model can make speed increase linearly with concurrency. When both source and destination performance are sufficient, a single job can definitely saturate the network card.</p>"},{"location":"en/#robust-error-tolerance-mechanism","title":"Robust Error Tolerance Mechanism","text":"<p>Jobs are extremely susceptible to interference from external factors, and factors such as network interruptions and unstable data sources can easily cause synchronization jobs to report errors and stop halfway. Therefore, stability is a basic requirement for Addax. In the design of Addax, the stability of both framework and plugins has been improved. Currently, Addax can achieve multi-level local/global retries at the thread level and job level, ensuring stable operation of user jobs.</p>"},{"location":"en/commandline/","title":"Command Line Usage","text":"<p>Addax provides a simple command-line interface for executing data synchronization jobs.</p>"},{"location":"en/commandline/#basic-syntax","title":"Basic Syntax","text":"<pre><code>bin/addax.sh [options] job_config_file\n</code></pre>"},{"location":"en/commandline/#command-line-options","title":"Command Line Options","text":""},{"location":"en/commandline/#-h-help","title":"<code>-h, --help</code>","text":"<p>Display help information and exit.</p> <pre><code>bin/addax.sh -h\n</code></pre>"},{"location":"en/commandline/#-v-version","title":"<code>-v, --version</code>","text":"<p>Display version information and exit.</p> <pre><code>bin/addax.sh -v\n</code></pre>"},{"location":"en/commandline/#-m-mode","title":"<code>-m, --mode</code>","text":"<p>Set execution mode. Available options:</p> <ul> <li><code>standalone</code>: Standalone mode (default)</li> <li><code>local</code>: Local mode</li> </ul> <pre><code>bin/addax.sh -m standalone job.json\n</code></pre>"},{"location":"en/commandline/#-j-jvm","title":"<code>-j, --jvm</code>","text":"<p>Set JVM parameters.</p> <pre><code>bin/addax.sh -j \"-Xms1g -Xmx4g\" job.json\n</code></pre>"},{"location":"en/commandline/#-p-params","title":"<code>-p, --params</code>","text":"<p>Pass runtime parameters for variable substitution in job configuration.</p> <pre><code>bin/addax.sh -p \"-Dhost=localhost -Dport=3306\" job.json\n</code></pre>"},{"location":"en/commandline/#-reader-plugin","title":"<code>--reader-plugin</code>","text":"<p>Display information about a specific reader plugin.</p> <pre><code>bin/addax.sh --reader-plugin mysqlreader\n</code></pre>"},{"location":"en/commandline/#-writer-plugin","title":"<code>--writer-plugin</code>","text":"<p>Display information about a specific writer plugin.</p> <pre><code>bin/addax.sh --writer-plugin postgresqlwriter\n</code></pre>"},{"location":"en/commandline/#usage-examples","title":"Usage Examples","text":""},{"location":"en/commandline/#basic-job-execution","title":"Basic Job Execution","text":"<p>Execute a simple synchronization job:</p> <pre><code>bin/addax.sh job/mysql_to_postgres.json\n</code></pre>"},{"location":"en/commandline/#job-with-custom-jvm-settings","title":"Job with Custom JVM Settings","text":"<p>Execute job with custom memory settings:</p> <pre><code>bin/addax.sh -j \"-Xms2g -Xmx8g -XX:+UseG1GC\" job/large_table_sync.json\n</code></pre>"},{"location":"en/commandline/#job-with-runtime-parameters","title":"Job with Runtime Parameters","text":"<p>Execute job with variable substitution:</p> <pre><code>bin/addax.sh -p \"-Dsource.host=db1.example.com -Dtarget.host=db2.example.com\" job/template.json\n</code></pre> <p>Where <code>template.json</code> contains variables like:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"parameter\": {\n            \"jdbcUrl\": \"jdbc:mysql://${source.host}:3306/mydb\"\n          }\n        },\n        \"writer\": {\n          \"parameter\": {\n            \"jdbcUrl\": \"jdbc:postgresql://${target.host}:5432/mydb\"\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/commandline/#debug-mode","title":"Debug Mode","text":"<p>Run job with debug output:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true\" job.json\n</code></pre>"},{"location":"en/commandline/#performance-monitoring","title":"Performance Monitoring","text":"<p>Run job with performance monitoring enabled:</p> <pre><code>bin/addax.sh -j \"-Daddax.monitor=true\" job.json\n</code></pre>"},{"location":"en/commandline/#exit-codes","title":"Exit Codes","text":"<p>Addax uses the following exit codes:</p> <ul> <li><code>0</code>: Job completed successfully</li> <li><code>1</code>: Job failed due to configuration error</li> <li><code>2</code>: Job failed due to runtime error</li> <li><code>3</code>: Job killed by user or system</li> </ul>"},{"location":"en/commandline/#configuration-override","title":"Configuration Override","text":"<p>You can override configuration settings via command line parameters:</p>"},{"location":"en/commandline/#override-speed-settings","title":"Override Speed Settings","text":"<pre><code>bin/addax.sh -p \"-Djob.setting.speed.channel=5\" job.json\n</code></pre>"},{"location":"en/commandline/#override-error-limits","title":"Override Error Limits","text":"<pre><code>bin/addax.sh -p \"-Djob.setting.errorLimit.record=100\" job.json\n</code></pre>"},{"location":"en/commandline/#plugin-information","title":"Plugin Information","text":""},{"location":"en/commandline/#list-available-plugins","title":"List Available Plugins","text":"<pre><code># List all reader plugins\nbin/addax.sh --reader-plugin\n\n# List all writer plugins  \nbin/addax.sh --writer-plugin\n</code></pre>"},{"location":"en/commandline/#get-plugin-details","title":"Get Plugin Details","text":"<pre><code># Get MySQL reader details\nbin/addax.sh --reader-plugin mysqlreader\n\n# Get PostgreSQL writer details\nbin/addax.sh --writer-plugin postgresqlwriter\n</code></pre>"},{"location":"en/commandline/#environment-variables","title":"Environment Variables","text":"<p>You can set the following environment variables to customize Addax behavior:</p>"},{"location":"en/commandline/#addax_home","title":"<code>ADDAX_HOME</code>","text":"<p>Set the Addax installation directory:</p> <pre><code>export ADDAX_HOME=/opt/addax\n</code></pre>"},{"location":"en/commandline/#java_home","title":"<code>JAVA_HOME</code>","text":"<p>Set the Java installation directory:</p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk\n</code></pre>"},{"location":"en/commandline/#addax_opts","title":"<code>ADDAX_OPTS</code>","text":"<p>Set default JVM options:</p> <pre><code>export ADDAX_OPTS=\"-Xms1g -Xmx4g\"\n</code></pre>"},{"location":"en/commandline/#logging-configuration","title":"Logging Configuration","text":"<p>Addax uses logback for logging. You can customize logging by:</p>"},{"location":"en/commandline/#setting-log-level","title":"Setting Log Level","text":"<pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-debug.xml\" job.json\n</code></pre>"},{"location":"en/commandline/#custom-log-file","title":"Custom Log File","text":"<pre><code>bin/addax.sh -j \"-Daddax.log.file=/var/log/addax/job.log\" job.json\n</code></pre>"},{"location":"en/commandline/#best-practices","title":"Best Practices","text":""},{"location":"en/commandline/#resource-management","title":"Resource Management","text":"<ul> <li>Use appropriate JVM heap sizes based on your data volume</li> <li>Monitor memory usage during large data transfers</li> <li>Set reasonable channel numbers based on system capacity</li> </ul>"},{"location":"en/commandline/#error-handling","title":"Error Handling","text":"<ul> <li>Always check exit codes in scripts</li> <li>Set appropriate error limits for your use case</li> <li>Review logs for detailed error information</li> </ul>"},{"location":"en/commandline/#security","title":"Security","text":"<ul> <li>Avoid passing passwords via command line (use configuration files)</li> <li>Use encrypted password files when possible</li> <li>Limit file permissions on configuration files</li> </ul>"},{"location":"en/commandline/#performance","title":"Performance","text":"<ul> <li>Test with different channel numbers to find optimal concurrency</li> <li>Use speed limits to prevent overwhelming source/target systems</li> <li>Monitor system resources during execution</li> </ul> <p>For more detailed information about specific plugins and configuration options, please refer to the job configuration guide and individual plugin documentation.</p>"},{"location":"en/debug/","title":"Debug Mode","text":"<p>Addax provides debugging capabilities to help troubleshoot issues during data synchronization jobs.</p>"},{"location":"en/debug/#enabling-debug-mode","title":"Enabling Debug Mode","text":""},{"location":"en/debug/#command-line-debug","title":"Command Line Debug","text":"<p>Enable debug mode by adding JVM parameters:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true\" job.json\n</code></pre>"},{"location":"en/debug/#configuration-debug","title":"Configuration Debug","text":"<p>Add debug configuration in your job file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"debug\": true\n    }\n  }\n}\n</code></pre>"},{"location":"en/debug/#debug-features","title":"Debug Features","text":""},{"location":"en/debug/#detailed-logging","title":"Detailed Logging","text":"<p>Debug mode provides more detailed logging including:</p> <ul> <li>SQL statements being executed</li> <li>Data transformation details</li> <li>Performance metrics</li> <li>Error stack traces</li> </ul>"},{"location":"en/debug/#data-sampling","title":"Data Sampling","text":"<p>When debug mode is enabled, Addax will log sample data:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.sample=10\" job.json\n</code></pre> <p>This will log the first 10 records for inspection.</p>"},{"location":"en/debug/#memory-monitoring","title":"Memory Monitoring","text":"<p>Monitor memory usage during execution:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.memory=true\" job.json\n</code></pre>"},{"location":"en/debug/#common-debug-scenarios","title":"Common Debug Scenarios","text":""},{"location":"en/debug/#connection-issues","title":"Connection Issues","text":"<p>If you're experiencing connection problems:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.connection=true\" job.json\n</code></pre>"},{"location":"en/debug/#performance-issues","title":"Performance Issues","text":"<p>For performance debugging:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.performance=true\" job.json\n</code></pre>"},{"location":"en/debug/#data-type-issues","title":"Data Type Issues","text":"<p>For data type conversion problems:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.datatype=true\" job.json\n</code></pre>"},{"location":"en/debug/#log-levels","title":"Log Levels","text":"<p>Set different log levels for various components:</p> <pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-debug.xml\" job.json\n</code></pre>"},{"location":"en/debug/#debug-output-examples","title":"Debug Output Examples","text":""},{"location":"en/debug/#sql-execution-debug","title":"SQL Execution Debug","text":"<pre><code>DEBUG [Reader-0] - Executing SQL: SELECT id, name, age FROM users WHERE id BETWEEN ? AND ?\nDEBUG [Reader-0] - SQL Parameters: [1, 1000]\nDEBUG [Reader-0] - Fetched 856 records in 1.23 seconds\n</code></pre>"},{"location":"en/debug/#data-sample-debug","title":"Data Sample Debug","text":"<pre><code>DEBUG [Channel-0] - Sample record: {\"id\": 1, \"name\": \"John Doe\", \"age\": 30}\nDEBUG [Channel-0] - Sample record: {\"id\": 2, \"name\": \"Jane Smith\", \"age\": 25}\n</code></pre>"},{"location":"en/debug/#performance-debug","title":"Performance Debug","text":"<pre><code>DEBUG [Job] - Channel statistics:\n  - Channel 0: 1000 records/s, 128KB/s\n  - Channel 1: 950 records/s, 122KB/s\n  - Channel 2: 1050 records/s, 135KB/s\n</code></pre>"},{"location":"en/debug/#troubleshooting-tips","title":"Troubleshooting Tips","text":""},{"location":"en/debug/#high-memory-usage","title":"High Memory Usage","text":"<p>Monitor memory usage and adjust heap size:</p> <pre><code>bin/addax.sh -j \"-Xms2g -Xmx8g -Daddax.debug.memory=true\" job.json\n</code></pre>"},{"location":"en/debug/#slow-performance","title":"Slow Performance","text":"<p>Identify bottlenecks:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug.performance=true -Daddax.debug.channel=true\" job.json\n</code></pre>"},{"location":"en/debug/#data-quality-issues","title":"Data Quality Issues","text":"<p>Check for data conversion errors:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug.datatype=true -Daddax.debug.sample=100\" job.json\n</code></pre>"},{"location":"en/debug/#custom-debug-configuration","title":"Custom Debug Configuration","text":"<p>Create a custom logback configuration for specific debug needs:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n    &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder&gt;\n            &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;logger name=\"com.wgzhao.addax\" level=\"DEBUG\"/&gt;\n    &lt;logger name=\"com.wgzhao.addax.core.job\" level=\"TRACE\"/&gt;\n\n    &lt;root level=\"INFO\"&gt;\n        &lt;appender-ref ref=\"STDOUT\"/&gt;\n    &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Save as <code>conf/logback-custom.xml</code> and use:</p> <pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-custom.xml\" job.json\n</code></pre>"},{"location":"en/debug/#production-debugging","title":"Production Debugging","text":"<p>For production environments, use selective debugging:</p> <pre><code># Only log errors and warnings\nbin/addax.sh -j \"-Daddax.debug.errors=true\" job.json\n\n# Log performance metrics only\nbin/addax.sh -j \"-Daddax.debug.performance=true\" job.json\n</code></pre> <p>This helps identify issues without overwhelming log output.</p>"},{"location":"en/encrypt_password/","title":"Password Encryption","text":"<p>Addax supports password encryption to enhance security when storing database credentials in configuration files.</p>"},{"location":"en/encrypt_password/#overview","title":"Overview","text":"<p>Instead of storing passwords in plain text in job configuration files, you can use encrypted passwords. This feature helps protect sensitive credentials, especially in shared environments or version control systems.</p>"},{"location":"en/encrypt_password/#generating-encrypted-passwords","title":"Generating Encrypted Passwords","text":"<p>Use the provided script to encrypt passwords:</p> <pre><code>bin/encrypt.sh &lt;password&gt;\n</code></pre> <p>Example:</p> <pre><code>bin/encrypt.sh mypassword123\n</code></pre> <p>Output: <pre><code>Encrypted password: addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\n</code></pre></p>"},{"location":"en/encrypt_password/#using-encrypted-passwords","title":"Using Encrypted Passwords","text":""},{"location":"en/encrypt_password/#in-job-configuration","title":"In Job Configuration","text":"<p>Replace plain text passwords with encrypted ones in your job configuration:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"dbuser\",\n            \"password\": \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/testdb\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/encrypt_password/#environment-variables","title":"Environment Variables","text":"<p>You can also store encrypted passwords in environment variables:</p> <pre><code>export DB_PASSWORD=\"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\"\n</code></pre> <p>Then reference it in your configuration:</p> <pre><code>{\n  \"parameter\": {\n    \"password\": \"${DB_PASSWORD}\"\n  }\n}\n</code></pre>"},{"location":"en/encrypt_password/#encryption-algorithm","title":"Encryption Algorithm","text":"<p>Addax uses AES (Advanced Encryption Standard) for password encryption:</p> <ul> <li>Algorithm: AES-128</li> <li>Mode: CBC (Cipher Block Chaining)</li> <li>Padding: PKCS5Padding</li> <li>Key: Generated based on system properties</li> </ul>"},{"location":"en/encrypt_password/#security-considerations","title":"Security Considerations","text":""},{"location":"en/encrypt_password/#key-management","title":"Key Management","text":"<p>The encryption key is derived from system properties. For enhanced security:</p> <ol> <li> <p>Set custom encryption key:    <pre><code>export ADDAX_ENCRYPT_KEY=\"your-custom-key-here\"\n</code></pre></p> </li> <li> <p>Use different keys per environment:    <pre><code># Development\nexport ADDAX_ENCRYPT_KEY=\"dev-key-2024\"\n\n# Production  \nexport ADDAX_ENCRYPT_KEY=\"prod-key-2024\"\n</code></pre></p> </li> </ol>"},{"location":"en/encrypt_password/#best-practices","title":"Best Practices","text":"<ol> <li>Rotate encryption keys regularly</li> <li>Use different keys for different environments</li> <li>Store keys securely (not in source code)</li> <li>Limit access to encryption keys</li> <li>Use encrypted passwords for all sensitive data</li> </ol>"},{"location":"en/encrypt_password/#advanced-usage","title":"Advanced Usage","text":""},{"location":"en/encrypt_password/#custom-encryption-provider","title":"Custom Encryption Provider","text":"<p>You can implement a custom encryption provider by implementing the <code>PasswordEncryptor</code> interface:</p> <pre><code>public class CustomPasswordEncryptor implements PasswordEncryptor {\n    @Override\n    public String encrypt(String plainPassword) {\n        // Your custom encryption logic\n        return \"custom:enc:\" + encryptedPassword;\n    }\n\n    @Override\n    public String decrypt(String encryptedPassword) {\n        // Your custom decryption logic\n        return decryptedPassword;\n    }\n}\n</code></pre>"},{"location":"en/encrypt_password/#batch-encryption","title":"Batch Encryption","text":"<p>For multiple passwords, create a script:</p> <pre><code>#!/bin/bash\npasswords=(\"password1\" \"password2\" \"password3\")\n\nfor pwd in \"${passwords[@]}\"; do\n    echo \"Encrypting: $pwd\"\n    bin/encrypt.sh \"$pwd\"\n    echo \"---\"\ndone\n</code></pre>"},{"location":"en/encrypt_password/#configuration-examples","title":"Configuration Examples","text":""},{"location":"en/encrypt_password/#mysql-with-encrypted-password","title":"MySQL with Encrypted Password","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"readonly_user\",\n            \"password\": \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\",\n            \"column\": [\"*\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://prod-db:3306/analytics\",\n                \"table\": [\"user_events\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"analytics_user\",\n            \"password\": \"addax:enc:AES:9nPsrKlMN8xR2vY5aBcDfG==\",\n            \"column\": [\"*\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://warehouse:5432/analytics\",\n                \"table\": [\"user_events\"]\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/encrypt_password/#multiple-environments","title":"Multiple Environments","text":"<p>Development (dev.json): <pre><code>{\n  \"parameter\": {\n    \"password\": \"addax:enc:AES:devKeyEncryptedPassword==\"\n  }\n}\n</code></pre></p> <p>Production (prod.json): <pre><code>{\n  \"parameter\": {\n    \"password\": \"addax:enc:AES:prodKeyEncryptedPassword==\"\n  }\n}\n</code></pre></p>"},{"location":"en/encrypt_password/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/encrypt_password/#decryption-errors","title":"Decryption Errors","text":"<p>If you encounter decryption errors:</p> <ol> <li>Verify encryption key: Ensure the same key is used for encryption and decryption</li> <li>Check password format: Ensure the encrypted password starts with <code>addax:enc:AES:</code></li> <li>Validate environment: Confirm environment variables are set correctly</li> </ol>"},{"location":"en/encrypt_password/#password-not-recognized","title":"Password Not Recognized","text":"<pre><code># Test decryption\nbin/decrypt.sh \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\"\n</code></pre>"},{"location":"en/encrypt_password/#key-management-issues","title":"Key Management Issues","text":"<pre><code># Check current encryption key\necho $ADDAX_ENCRYPT_KEY\n\n# Set temporary key for testing\nexport ADDAX_ENCRYPT_KEY=\"test-key-123\"\n</code></pre>"},{"location":"en/encrypt_password/#migration-guide","title":"Migration Guide","text":""},{"location":"en/encrypt_password/#from-plain-text-to-encrypted","title":"From Plain Text to Encrypted","text":"<ol> <li>Identify all passwords in configuration files</li> <li>Encrypt each password using the encrypt script</li> <li>Replace plain text with encrypted values</li> <li>Test the configuration to ensure it works</li> <li>Update documentation with new security procedures</li> </ol>"},{"location":"en/encrypt_password/#example-migration-script","title":"Example Migration Script","text":"<pre><code>#!/bin/bash\n\n# Backup original files\ncp config/job.json config/job.json.backup\n\n# Replace passwords (adjust patterns as needed)\nsed -i 's/\"password\": \"plainpassword\"/\"password\": \"addax:enc:AES:encryptedvalue\"/g' config/job.json\n\necho \"Migration complete. Test the configuration before deploying.\"\n</code></pre> <p>This encryption feature significantly improves the security posture of your Addax deployments while maintaining ease of use.</p>"},{"location":"en/plugin_development/","title":"Plugin Development","text":"<p>This guide is primarily for developers who need to develop Addax plugins to meet their specific needs.</p>"},{"location":"en/plugin_development/#addax-flow","title":"Addax Flow","text":"<p>The general process for running a task in Addax is as follows:</p> <p></p> <p>The startup steps are:</p> <ol> <li>Parse configurations, including <code>job.json</code>, <code>core.json</code>, and <code>plugin.json</code>.</li> <li>Set the <code>jobId</code> in the <code>configuration</code>.</li> <li>Start the Engine via <code>Engine.start()</code> to enter the startup procedure.</li> <li>Set the <code>RUNTIME_MODE</code> in the <code>configuration</code>.</li> <li>Start via the <code>JobContainer</code>'s <code>start()</code> method.</li> <li>Execute the job's <code>preHandler()</code>, <code>init()</code>, <code>prepare()</code>, <code>split()</code>, <code>schedule()</code>, <code>post()</code>, and <code>postHandle()</code> methods in sequence.</li> <li>The <code>init()</code> method involves initializing the reader and writer plugins based on the configuration. This includes hot-loading JAR packages and calling the plugin's <code>init()</code> method, as well as setting the reader and writer's configuration.</li> <li>The <code>prepare()</code> method involves initializing the reader and writer plugins by calling their respective <code>prepare()</code> methods. Each plugin has its own <code>jarLoader</code>, which inherits from <code>URLClassLoader</code>.</li> <li>The <code>split()</code> method adjusts the number of channels via <code>adjustChannelNumber()</code> and performs the most granular splitting for the reader and writer. It's important to note that the writer's split result must reference the reader's split result to ensure an equal number of splits, satisfying the 1:1 channel model.</li> <li>The channel count is mainly determined by byte and record rate limiting, which is calculated as the first step in the <code>split()</code> function.</li> <li>In the <code>split()</code> method, the reader plugin will split based on the channel value, but some reader plugins might not use this value. The writer plugin will always split 1:1 based on the reader's splits.</li> <li>The <code>mergeReaderAndWriterTaskConfigs()</code> method inside <code>split()</code> is responsible for merging the configurations of the reader, writer, and transformer to generate task configurations and override the <code>job.content</code> configuration.</li> <li>The <code>schedule()</code> method allocates and generates <code>TaskGroup</code> objects based on the task configurations generated by <code>split()</code>. The number of task groups is determined by dividing the total number of tasks by the number of tasks a single <code>TaskGroup</code> can support.</li> <li><code>schedule()</code> is executed internally by <code>AbstractScheduler</code>'s <code>schedule()</code> method, which then calls <code>startAllTaskGroup()</code> to create all <code>TaskGroupContainer</code>s to organize the related tasks. <code>TaskGroupContainerRunner</code> is responsible for running the <code>TaskGroupContainer</code> to execute the assigned tasks. The concrete implementation class for the scheduler is <code>ProcessInnerScheduler</code>.</li> <li><code>taskGroupContainerExecutorService</code> starts a fixed thread pool to execute <code>TaskGroupContainerRunner</code> objects. The <code>run()</code> method of <code>TaskGroupContainerRunner</code> calls <code>taskGroupContainer.start()</code>, which creates a <code>TaskExecutor</code> for each channel and starts the task via <code>taskExecutor.doStart()</code>.</li> </ol>"},{"location":"en/plugin_development/#plugin-mechanism","title":"Plugin Mechanism","text":"<p>To handle the differences between various data sources while providing consistent synchronization primitives and extensibility, <code>Addax</code> uses a <code>Framework</code> + <code>Plugin</code> model:</p> <ul> <li>Plugins only need to care about reading from or writing to the data source itself.</li> <li>Common synchronization issues, such as type conversion, performance, and statistics, are handled by the framework.</li> </ul> <p>As a plugin developer, you need to focus on two issues:</p> <ol> <li>The correctness of reading and writing data from the specific data source.</li> <li>How to communicate with the framework and use it correctly.</li> </ol>"},{"location":"en/plugin_development/#framework-from-a-plugins-perspective","title":"Framework from a Plugin's Perspective","text":""},{"location":"en/plugin_development/#logical-execution-model","title":"Logical Execution Model","text":"<p>Plugin developers don't need to worry about too much; they mainly need to focus on reading from and writing to specific systems, and how their code is logically executed\u2014which method is called and when. Before that, you need to understand the following concepts:</p> <ul> <li><code>Job</code>: Describes a synchronization job from a source to a destination, representing the smallest business unit of data synchronization. For example, synchronizing a table from MySQL to a table in PostgreSQL.</li> <li><code>Task</code>: The smallest execution unit, obtained by splitting a <code>Job</code> to maximize performance. For example, a <code>Job</code> reading from a sharded MySQL database with 1024 sub-tables can be split into 1024 read <code>Tasks</code>, executed concurrently.</li> <li><code>TaskGroup</code>: A collection of <code>Tasks</code>. A set of <code>Tasks</code> executed within the same <code>TaskGroupContainer</code> is called a <code>TaskGroup</code>.</li> <li><code>JobContainer</code>: The <code>Job</code> executor, a work unit responsible for the global splitting, scheduling, pre- and post-statements of a <code>Job</code>. Similar to the JobTracker in Yarn.</li> <li><code>TaskGroupContainer</code>: The <code>TaskGroup</code> executor, a work unit responsible for executing a group of <code>Tasks</code>. Similar to the TaskTracker in Yarn.</li> </ul> <p>In short, a <code>Job</code> is split into <code>Tasks</code>, which are executed in containers provided by the framework. The plugin only needs to implement the logic for <code>Job</code> and <code>Task</code>.</p>"},{"location":"en/plugin_development/#programming-interface","title":"Programming Interface","text":"<p>So, how should the logic of <code>Job</code> and <code>Task</code> be mapped to specific code?</p> <p>First, the entry class of a plugin must extend the <code>Reader</code> or <code>Writer</code> abstract class and implement their respective <code>Job</code> and <code>Task</code> inner abstract classes. The implementations of <code>Job</code> and <code>Task</code> must be in the form of inner classes.</p> <pre><code>public class SomeReader\n        extends Reader\n{\n    public static class Job\n            extends Reader.Job\n    {\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public List&lt;Configuration&gt; split(int adviceNumber)\n        {\n            return null;\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n\n    public static class Task\n            extends Reader.Task\n    {\n\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public void startRead(RecordSender recordSender)\n        {\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n}\n</code></pre> <p>The <code>Job</code> interface functions are as follows:</p> <ul> <li><code>init</code>: Initialization work for the Job object. At this point, you can get the plugin-related configuration via <code>super.getPluginJobConf()</code>. The read plugin gets the <code>reader</code> section of the configuration, and the write plugin gets the <code>writer</code> section.</li> <li><code>prepare</code>: Global preparation work, such as clearing the target table in MySQL.</li> <li><code>split</code>: Splits the job into <code>Tasks</code>. The <code>adviceNumber</code> parameter is the number of splits suggested by the framework, usually the concurrency level configured at runtime. The return value is a list of <code>Task</code> configurations.</li> <li><code>post</code>: Global post-processing work, such as the <code>rename</code> operation for a MySQL writer after synchronizing to a shadow table.</li> <li><code>destroy</code>: Destruction work for the Job object itself.</li> </ul> <p>The <code>Task</code> interface functions are as follows:</p> <ul> <li><code>init</code>: Initialization of the Task object. At this point, you can get the <code>Task</code>-related configuration via <code>super.getPluginJobConf()</code>. This configuration is one of the configurations returned by the <code>Job#split</code> method.</li> <li><code>prepare</code>: Local preparation work.</li> <li><code>startRead</code>: Reads data from the data source and writes it to the <code>RecordSender</code>. The <code>RecordSender</code> writes the data to the buffer queue connecting the <code>Reader</code> and <code>Writer</code>.</li> <li><code>startWrite</code>: Reads data from the <code>RecordReceiver</code> and writes it to the target data source. The data in the <code>RecordReceiver</code> comes from the buffer queue between the <code>Reader</code> and <code>Writer</code>.</li> <li><code>post</code>: Local post-processing work.</li> <li><code>destroy</code>: Destruction work for the Task object itself.</li> </ul> <p>Please note:</p> <ul> <li>There must be no shared variables between <code>Job</code> and <code>Task</code>, because in a distributed runtime, there is no guarantee that shared variables will be initialized correctly. They can only depend on each other through configuration files.</li> <li><code>prepare</code> and <code>post</code> exist in both <code>Job</code> and <code>Task</code>. The plugin needs to determine where to perform operations based on the actual situation.</li> </ul> <p>The framework executes the <code>Job</code> and <code>Task</code> interfaces in the following order:</p> <pre><code>stateDiagram-v2\ndirection TB\nInit:::job --&gt; Prepare:::job\nPrepare --&gt; Split:::job\nSplit --&gt; Schedule:::fw\nstate Schedule {\n    direction LR\n    init\\nprepare\\nstartRead\\npost\\ndestroy1 --&gt; init\\nprepare\\nstartRead\\npost\\ndestroy : Channel\n}\nSchedule --&gt; Post:::job\n\nclassDef job fill:yellow\nclassDef fw fill:#c6fac4\nclassDef ctask fill:blue</code></pre> <p>In the diagram above, yellow represents the execution phase of the <code>Job</code> part, gray represents the execution phase of the <code>Task</code> part, and green represents the framework's execution phase.</p> <p>The related class relationships are as follows:</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\n    class Pluginable {\n    + init()\n    + destroy()\n    + others()\n    }\n    class AbstractPlugin {\n        + prepare()\n        + post()\n        + others()\n    }\n    class AbstractJobPlugin {\n        + getJobPluginCollector(): JobPluginCollector\n        + setJobPluginCollector(JobPluginCollector)\n    }\n\n    class AbstractTaskPlugin {\n        + getTaskPluginCollector(): TaskPluginCollector\n        + setTaskPluginCollector(TaskPluginCollector)\n    }\n    class Reader_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Writer_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Reader_Task {\n        + startRead(RecordSender)\n    }\n\n    class Writer_Task {\n        + startWrite(RecordReceiver)\n    }\n\n    AbstractJobPlugin &lt;|-- Reader_Job\n    AbstractJobPlugin &lt;|-- Writer_Job\n\n    AbstractTaskPlugin &lt;|-- Reader_Task\n    AbstractTaskPlugin &lt;|-- Writer_Task\n\n    AbstractPlugin &lt;|-- AbstractJobPlugin\n    AbstractPlugin &lt;|-- AbstractTaskPlugin\n\n    Pluginable &lt;|-- AbstractPlugin\n</code></pre>"},{"location":"en/plugin_development/#plugin-definition","title":"Plugin Definition","text":"<p>In each plugin's project, there is a <code>plugin.json</code> file that defines the plugin's information, including its entry class. For example:</p> <pre><code>{\n  \"name\": \"mysqlwriter\",\n  \"class\": \"com.wgzhao.addax.plugin.writer.mysqlwriter.MysqlWriter\",\n  \"description\": \"Use Jdbc connect to database, execute insert sql.\",\n  \"developer\": \"wgzhao\"\n}\n</code></pre> <ul> <li><code>name</code>: The plugin name, case-sensitive. The framework searches for the plugin based on the name specified by the user in the configuration file. Very important.</li> <li><code>class</code>: The fully qualified name of the entry class. The framework creates an instance of the entry class via reflection. Very important.</li> <li><code>description</code>: Descriptive information.</li> <li><code>developer</code>: The developer.</li> </ul>"},{"location":"en/plugin_development/#packaging-and-publishing","title":"Packaging and Publishing","text":"<p><code>Addax</code> uses <code>assembly</code> for packaging. The packaging commands are as follows:</p> <pre><code>mvn clean package\nmvn package assembly:single\n</code></pre> <p><code>Addax</code> plugins need to follow a uniform directory structure:</p> <pre><code>${ADDAX_HOME}\n\u251c\u2500\u2500 bin\n\u2502     \u251c\u2500\u2500 addax.sh\n\u251c\u2500\u2500 conf\n\u2502     \u251c\u2500\u2500 core.json\n\u2502     \u2514\u2500\u2500 logback.xml\n\u251c\u2500\u2500 job\n\u251c\u2500\u2500 lib\n\u2502     \u251c\u2500\u2500 addax-common-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-core-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-rdbms-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-storage-&lt;version&gt;.jar\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 plugin\n\u2502     \u251c\u2500\u2500 reader\n\u2502     \u2502     \u251c\u2500\u2500 cassandrareader\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 cassandrareader-&lt;version&gt;.jar\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 libs\n\u2502     \u2502     \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 plugin.json\n\u2502     \u2502     \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u2502     \u2514\u2500\u2500 writer\n\u2502         \u251c\u2500\u2500 cassandrawriter\n\u2502         \u2502     \u251c\u2500\u2500 cassandrawriter-&lt;version&gt;.jar\n\u2502         \u2502     \u251c\u2500\u2500 libs\n\u2502         \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502         \u2502     \u251c\u2500\u2500 plugin.json\n\u2502         \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u251c\u2500\u2500 shared\n</code></pre> <ul> <li><code>${ADDAX_HOME}/bin</code>: Executable program directory</li> <li><code>${ADDAX_HOME}/conf</code>: Framework configuration directory</li> <li><code>${ADDAX_HOME}/lib</code>: Framework dependency library directory</li> <li><code>${ADDAX_HOME}/shared</code>: Plugin dependency directory</li> <li><code>${ADDAX_HOME}/plugin</code>: Plugin directory</li> </ul> <p>The plugin directory is divided into <code>reader</code> and <code>writer</code> subdirectories, where read and write plugins are stored respectively. The plugin directory structure is as follows:</p> <ul> <li><code>${PLUGIN_HOME}/libs</code>: The plugin's dependency libraries. To reduce package size, these dependencies are symbolic links to the <code>shared</code> directory.</li> <li><code>${PLUGIN_HOME}/plugin-name-version.jar</code>: The plugin's own jar file.</li> <li><code>${PLUGIN_HOME}/plugin.json</code>: The plugin description file.</li> </ul> <p>Although the framework adds all jar files under <code>${PLUGIN_HOME}</code> to the <code>classpath</code> when loading the plugin, it is recommended to keep dependency library jars and the plugin's own jar separate.</p> <p>Special</p> <p>The plugin's directory name must be consistent with the plugin name defined in <code>plugin.json</code>.</p>"},{"location":"en/plugin_development/#configuration-file","title":"Configuration File","text":"<p><code>Addax</code> uses <code>json</code> as the format for configuration files. A typical <code>Addax</code> task configuration is as follows:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>Addax</code> framework has a <code>core.json</code> configuration file that specifies the framework's default behavior. A task's configuration can specify configuration items that already exist in the framework, and these will have a higher priority, overriding the settings in <code>core.json</code>.</p> <p>The <code>value</code> part of <code>job.content.reader.parameter</code> in the configuration is passed to <code>Reader.Job</code>; the <code>value</code> part of <code>job.content.writer.parameter</code> is passed to <code>Writer.Job</code>. <code>Reader.Job</code> and <code>Writer.Job</code> can retrieve them using <code>super.getPluginJobConf()</code>.</p>"},{"location":"en/plugin_development/#how-to-design-configuration-parameters","title":"How to Design Configuration Parameters","text":"<p>Designing the configuration file is the first step in plugin development!</p> <p>The <code>parameter</code> section under <code>reader</code> and <code>writer</code> in the task configuration contains the plugin's configuration parameters. These parameters should follow these principles:</p> <ul> <li>Camel Case: All configuration items should use lower camel case, with the first letter being lowercase.</li> <li>Orthogonality Principle: Configuration items must be orthogonal, with no overlapping functionality or hidden rules.</li> <li>Rich Types: Use JSON types appropriately to reduce unnecessary processing logic and the possibility of errors.<ul> <li>Use the correct data types. For example, for a <code>bool</code> type, use <code>true</code>/<code>false</code> instead of <code>\"yes\"</code>/<code>\"true\"</code>/<code>0</code>.</li> <li>Use collection types reasonably, for example, use an array instead of a delimited string.</li> </ul> </li> <li>Consistency with Similar Plugins: Follow the conventions of similar plugin types. For example, the <code>connection</code> parameter for relational databases usually has the following structure:</li> </ul> <pre><code>{\n  \"connection\": [\n    {\n      \"table\": [\"table_1\", \"table_2\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.1:3306/database_1\",\n        \"jdbc:mysql://127.0.0.2:3306/database_1_slave\"\n      ]\n    },\n    {\n      \"table\": [\"table_3\", \"table_4\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.3:3306/database_2\",\n        \"jdbc:mysql://127.0.0.4:3306/database_2_slave\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"en/plugin_development/#how-to-use-the-configuration-class","title":"How to Use the <code>Configuration</code> Class","text":"<p>To simplify operations on <code>json</code>, <code>Addax</code> provides a simple DSL to be used with the <code>Configuration</code> class.</p> <p><code>Configuration</code> provides common operations for reading and writing configuration items, such as <code>get</code>, <code>get with type</code>, <code>get with default value</code>, and <code>set</code>, as well as methods like <code>clone</code> and <code>toJSON</code>. Read and write operations require a <code>path</code> parameter.</p> <ol> <li>A child map is represented by <code>.key</code>. The first dot in the <code>path</code> is omitted.</li> <li>An array element is represented by <code>[index]</code>.</li> </ol> <p>For example, to operate on the following json:</p> <pre><code>{\n  \"a\": {\n    \"b\": {\n      \"c\": 2\n    },\n    \"f\": [\n      1,\n      2,\n      {\n        \"g\": true,\n        \"h\": false\n      },\n      4\n    ]\n  },\n  \"x\": 4\n}\n</code></pre> <p>When calling the <code>configuration.get(path)</code> method, the results for the following path values are:</p> <ul> <li><code>x</code>: <code>4</code></li> <li><code>a.b.c</code>: <code>2</code></li> <li><code>a.b.c.d</code>: <code>null</code></li> <li><code>a.f[0]</code>: <code>1</code></li> <li><code>a.f[2].g</code>: <code>true</code></li> </ul> <p>Note that because the configuration seen by the plugin is only a part of the whole configuration, you need to be aware of the current root path when using a <code>Configuration</code> object.</p> <p>For more operations on <code>Configuration</code>, please refer to Configuration.java.</p>"},{"location":"en/plugin_development/#plugin-data-transfer","title":"Plugin Data Transfer","text":"<p>Like the general <code>producer-consumer</code> model, <code>Reader</code> and <code>Writer</code> plugins also transfer data through a <code>channel</code>. The <code>channel</code> can be in-memory or persistent.</p> <p>A piece of data in the <code>channel</code> is a <code>Record</code> object. A <code>Record</code> can contain multiple <code>Column</code> objects, which can be simply understood as records and columns in a database.</p> <p><code>Record</code> has the following methods:</p> <pre><code>public interface Record\n{\n    // Add a column to the end\n    void addColumn(Column column);\n\n    // Set a column at a specific index\n    void setColumn(int i, final Column column);\n\n    // Get a column\n    Column getColumn(int i);\n\n    // Convert to a JSON String\n    String toString();\n\n    // Get the total number of columns\n    int getColumnNumber();\n\n    // Calculate the byte size of the entire record in memory\n    int getByteSize();\n}\n</code></pre> <p>Since <code>Record</code> is an interface, the <code>Reader</code> plugin first calls <code>RecordSender.createRecord()</code> to create a <code>Record</code> instance, and then adds <code>Column</code>s one by one to the <code>Record</code>.</p> <p>The <code>Writer</code> plugin calls the <code>RecordReceiver.getFromReader()</code> method to get a <code>Record</code>, then iterates through the <code>Column</code>s and writes them to the target storage. When the <code>Reader</code> has not yet exited and the transfer is still in progress, if <code>getFromReader()</code> returns <code>null</code>, it means there is currently no data in the channel. If the transfer has already ended, it will return <code>null</code>, and the <code>Writer</code> plugin can use this to determine whether to end the <code>startWrite</code> method.</p>"},{"location":"en/plugin_development/#type-conversion","title":"Type Conversion","text":"<p>To standardize type conversion operations between the source and destination and ensure data integrity, Addax supports six internal data types:</p> <ul> <li><code>Long</code>: Fixed-point numbers (Int, Short, Long, BigInteger, etc.).</li> <li><code>Double</code>: Floating-point numbers (Float, Double, BigDecimal (infinite precision), etc.).</li> <li><code>String</code>: String type, with no length limit, using a universal character set (Unicode).</li> <li><code>Date</code>: Date type.</li> <li><code>Timestamp</code>: Timestamp type.</li> <li><code>Bool</code>: Boolean value.</li> <li><code>Bytes</code>: Binary data, which can store unstructured data such as MP3s.</li> </ul> <p>Correspondingly, there are seven <code>Column</code> implementations: <code>DateColumn</code>, <code>LongColumn</code>, <code>DoubleColumn</code>, <code>BytesColumn</code>, <code>StringColumn</code>, <code>BoolColumn</code>, and <code>TimestampColumn</code>.</p> <p>In addition to providing data-related methods, <code>Column</code> also provides a series of type conversion methods starting with <code>as</code>.</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\ndirection TB\nclass Column {\n    &lt;&lt;interface&gt;&gt;\n    - rawData: Object\n    - type: Type\n    + getRawData(): Object\n    + getType(): Type\n    + getByteSize(): init\n    + asLong(): Long\n    + asDouble(): Doule\n    + asString(): String\n    + asDate(): Date\n    + asBytes(): Bytes\n    + asBigDecimal(): BigDecimal\n    + asBoolean(): Boolean\n}\nColumn &lt;|-- Stringcolumn\nColumn &lt;|-- Doublecolumn\nColumn &lt;|-- Longcolumn\nColumn &lt;|-- Datecolumn\nColumn &lt;|-- Boolcolumn\nColumn &lt;|-- Bytescolumn</code></pre> <p>Addax's internal types use different Java types in their implementation:</p> Internal Type Implementation Type Notes Date java.util.Date Timestamp java.sql.Timestamp Can be precise to the nanosecond Long java.math.BigInteger Uses infinite-precision integers to ensure no loss of precision Double java.lang.String Represented as a String to ensure no loss of precision Bytes byte[] String java.lang.String Bool java.lang.Boolean <p>The relationships for converting between types are as follows:</p> from/to Date Long Double Bytes String Bool Date - Use millisecond timestamp Not supported Not supported Convert according to configured format Not supported Long Construct Date from ms - <code>BigDecimal.doubleValue()</code> Not supported <code>BigInteger.toString()</code> 0 is <code>false</code>, others are <code>true</code> Double Not supported <code>BigDecimal.longValue()</code> - Not supported Return internal String directly Not supported Bytes Not supported Not supported Not supported - Convert to <code>byte[]</code> with UTF-8 encoding Not supported String Parse with configured format <code>BigDecimal.longValue</code> <code>BigDecimal.doubleValue</code><sup>1</sup> Convert to <code>byte[]</code> with UTF-8 encoding<sup>2</sup> - \"true\" is <code>true</code>, \"false\" is <code>false</code> Bool Not supported <code>true</code> is <code>1L</code>, else <code>0L</code> <code>true</code> is <code>1.0</code>, else <code>0.0</code> Not supported <code>Boolean.toString()</code> -"},{"location":"en/plugin_development/#dirty-data-handling","title":"Dirty Data Handling","text":""},{"location":"en/plugin_development/#what-is-dirty-data","title":"What is Dirty Data","text":"<p>Currently, there are three main types of dirty data:</p> <ol> <li>The Reader reads an unsupported type or an illegal value.</li> <li>Unsupported type conversion, for example, converting <code>Bytes</code> to <code>Date</code>.</li> <li>Writing to the target fails, for example, an integer value exceeds the length limit when writing to MySQL.</li> </ol>"},{"location":"en/plugin_development/#how-to-handle-dirty-data","title":"How to Handle Dirty Data","text":"<p>In <code>Reader.Task</code> and <code>Writer.Task</code>, you can get a <code>TaskPluginCollector</code> through <code>AbstractTaskPlugin.getPluginCollector()</code>. It provides a series of <code>collectDirtyRecord</code> methods. When dirty data is encountered, you can call this method and pass in the <code>Record</code> that is considered dirty.</p> <p>Users can specify a limit on the number of dirty data records or a percentage limit in the task configuration. When the dirty data exceeds the limit, the framework will terminate the synchronization task. Plugins need to ensure that all dirty data is collected.</p>"},{"location":"en/plugin_development/#loading-principle","title":"Loading Principle","text":"<ol> <li>The framework scans the <code>plugin/reader</code> and <code>plugin/writer</code> directories and loads each plugin's <code>plugin.json</code> file.</li> <li>It indexes all plugin configurations using the <code>name</code> from the <code>plugin.json</code> file as the key. If duplicate plugin names are found, the framework will exit with an error.</li> <li>The user specifies the plugin name in the <code>name</code> field of the <code>reader</code>/<code>writer</code> configuration. The framework then scans all jars in the plugin's path based on the plugin type (<code>reader</code>/<code>writer</code>) and name, and adds them to a custom <code>classloader</code>.</li> <li>Based on the entry class defined in the plugin configuration, the framework instantiates the corresponding <code>Job</code> and <code>Task</code> objects via reflection.</li> </ol> <ol> <li> <p>Handles values like <code>NaN</code>, <code>Infinity</code>, <code>-Infinity</code>.\u00a0\u21a9</p> </li> <li> <p>Unless another encoding format is specified.\u00a0\u21a9</p> </li> </ol>"},{"location":"en/quickstart/","title":"Quick Start","text":""},{"location":"en/quickstart/#download-and-install","title":"Download and Install","text":""},{"location":"en/quickstart/#download","title":"Download","text":"<p>You can download the installation package from the release page, or you can build it yourself from source code.</p>"},{"location":"en/quickstart/#installation","title":"Installation","text":"<p>Unzip the downloaded installation package to the directory where you want to install it:</p> <pre><code>tar -xzf addax-{version}.tar.gz\n</code></pre>"},{"location":"en/quickstart/#environment-requirements","title":"Environment Requirements","text":"<ul> <li>Linux or macOS operating system</li> <li>Java 8 or higher version</li> <li>Python 3.6 or higher version (required for some plugins)</li> </ul>"},{"location":"en/quickstart/#first-synchronization-job","title":"First Synchronization Job","text":"<p>Let's start with a simple example - synchronizing data from a text file to another text file.</p>"},{"location":"en/quickstart/#prepare-test-data","title":"Prepare Test Data","text":"<p>Create a test data file:</p> <pre><code>echo -e \"1,zhangsan,20\\n2,lisi,21\\n3,wangwu,22\" &gt; /tmp/test.csv\n</code></pre>"},{"location":"en/quickstart/#create-job-configuration","title":"Create Job Configuration","text":"<p>Create a job configuration file <code>job.json</code>:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/test.csv\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\n              {\n                \"index\": 0,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 1,\n                \"type\": \"string\"\n              },\n              {\n                \"index\": 2,\n                \"type\": \"long\"\n              }\n            ],\n            \"fieldDelimiter\": \",\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"txtfilewriter\",\n          \"parameter\": {\n            \"path\": \"/tmp/result.csv\",\n            \"fileName\": \"result\",\n            \"writeMode\": \"truncate\",\n            \"encoding\": \"UTF-8\",\n            \"fieldDelimiter\": \",\",\n            \"nullFormat\": \"\\\\N\"\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/quickstart/#execute-job","title":"Execute Job","text":"<p>Run the synchronization job:</p> <pre><code>bin/addax.sh job.json\n</code></pre> <p>If successful, you should see output similar to:</p> <pre><code>2023-12-07 10:30:01.234 [main] INFO  JobContainer - Job ID: 202312071030, Total records: 3, Speed: 3rec/s (30B/s), Error records: 0\n</code></pre> <p>And you should find the result file at <code>/tmp/result.csv</code>.</p>"},{"location":"en/quickstart/#database-synchronization-example","title":"Database Synchronization Example","text":"<p>Here's a more practical example - synchronizing data from MySQL to PostgreSQL.</p>"},{"location":"en/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>MySQL database with test data</li> <li>PostgreSQL database for destination</li> </ul>"},{"location":"en/quickstart/#create-job-configuration_1","title":"Create Job Configuration","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"mysql_user\",\n            \"password\": \"mysql_password\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test\",\n                \"table\": [\"user_table\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"postgres_user\",\n            \"password\": \"postgres_password\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/test\",\n                \"table\": [\"user_table\"]\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/quickstart/#execute-job_1","title":"Execute Job","text":"<pre><code>bin/addax.sh mysql_to_postgresql.json\n</code></pre>"},{"location":"en/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about job configuration</li> <li>Explore available reader plugins</li> <li>Explore available writer plugins</li> <li>Learn about performance tuning</li> </ul>"},{"location":"en/server/","title":"Server Module","text":"<p>The Server module provides HTTP interfaces for submitting and managing data collection tasks. Users can submit JSON job configurations via POST, and the server executes tasks asynchronously, returning a unique task ID. Progress and results can be queried using this ID.</p>"},{"location":"en/server/#features","title":"Features","text":"<ul> <li>RESTful API for task submission and status query</li> <li>Configurable maximum concurrent tasks (default: 30)</li> <li>Integrated with Addax core Engine for direct job execution</li> <li>Supports command-line and environment variable concurrency settings</li> <li>Startup/shutdown script with background mode support</li> </ul>"},{"location":"en/server/#http-api","title":"HTTP API","text":""},{"location":"en/server/#1-submit-task","title":"1. Submit Task","text":"<ul> <li>URL: <code>/api/submit?k1=v1&amp;k2=v2</code></li> <li>Method: POST</li> <li>Request Example: <pre><code>curl 'http://localhost:10601/api/submit?jobName=example-job' \\\n-H 'Content-Type: application/json' \\\n-d @job/job.json\n</code></pre></li> <li>Response Example: <pre><code>{\n    \"taskId\": \"xxxx-xxxx-xxxx\"\n}\n</code></pre></li> <li>If concurrency limit is reached: <pre><code>{\n    \"error\": \"ERROR: Maximum number of concurrent tasks reached.\"\n}\n</code></pre></li> </ul>"},{"location":"en/server/#2-query-task-status","title":"2. Query Task Status","text":"<ul> <li>URL: <code>/api/status?taskId={taskId}</code></li> <li>Method: GET</li> <li>Response Example: <pre><code>{\n    \"taskId\": \"xxxx-xxxx-xxxx\",\n    \"status\": \"SUCCESS\",\n    \"result\": \"Job example-job executed.\",\n    \"error\": null\n}\n</code></pre></li> </ul>"},{"location":"en/server/#start-and-stop","title":"Start and Stop","text":"<p>Use the script <code>core/src/main/bin/addax-server.sh</code> to start and stop the service.</p>"},{"location":"en/server/#start-service","title":"Start Service","text":"<pre><code>./addax-server.sh start\n</code></pre>"},{"location":"en/server/#set-max-concurrency-eg-50-and-run-in-background","title":"Set Max Concurrency (e.g., 50) and Run in Background","text":"<pre><code>./addax-server.sh start -p 50 --daemon\n</code></pre>"},{"location":"en/server/#stop-service","title":"Stop Service","text":"<pre><code>./addax-server.sh stop\n</code></pre>"},{"location":"en/server/#concurrency-configuration","title":"Concurrency Configuration","text":"<ul> <li>Command-line <code>-p</code> or <code>--parallel</code> has the highest priority</li> <li>Environment variable <code>ADDAX_SERVER_PARALLEL</code> is next</li> <li>Default concurrency is 30</li> </ul>"},{"location":"en/server/#dependencies","title":"Dependencies","text":"<ul> <li>Minimal Spring Boot Web components</li> <li>Depends on core module Engine class</li> </ul>"},{"location":"en/server/#notes","title":"Notes","text":"<ul> <li>Ensure the job parameter is a valid Addax job JSON</li> <li>High concurrency may impact system performance</li> </ul> <p>For more help, refer to other documentation or contact the project maintainers.</p>"},{"location":"en/setupJob/","title":"Job Configuration","text":"<p>This document details how to configure Addax synchronization jobs. Addax uses JSON format configuration files to describe synchronization tasks.</p>"},{"location":"en/setupJob/#configuration-structure","title":"Configuration Structure","text":"<p>A complete job configuration file consists of three main parts:</p> <ul> <li>core: Core configuration</li> <li>job: Job configuration </li> <li>setting: Runtime settings</li> </ul> <p>Here's the basic structure:</p> <pre><code>{\n  \"core\": {\n    \"transport\": {\n      \"channel\": {\n        \"speed\": {\n          \"byte\": 1048576\n        }\n      }\n    }\n  },\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {},\n        \"writer\": {}\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/setupJob/#core-configuration","title":"Core Configuration","text":"<p>The <code>core</code> section contains system-level configuration:</p>"},{"location":"en/setupJob/#transport-configuration","title":"Transport Configuration","text":"<pre><code>{\n  \"core\": {\n    \"transport\": {\n      \"channel\": {\n        \"speed\": {\n          \"byte\": 1048576,\n          \"record\": -1\n        },\n        \"flowControlInterval\": 20,\n        \"capacity\": 512,\n        \"byteCapacity\": 67108864\n      }\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>speed.byte</code>: Byte-level speed limit (bytes per second), -1 means no limit</li> <li><code>speed.record</code>: Record-level speed limit (records per second), -1 means no limit  </li> <li><code>flowControlInterval</code>: Flow control check interval (milliseconds)</li> <li><code>capacity</code>: Channel capacity (number of records)</li> <li><code>byteCapacity</code>: Channel byte capacity</li> </ul>"},{"location":"en/setupJob/#job-configuration_1","title":"Job Configuration","text":"<p>The <code>job</code> section contains the main synchronization task configuration:</p>"},{"location":"en/setupJob/#content-array","title":"Content Array","text":"<p>The <code>content</code> array can contain multiple reader-writer pairs for complex synchronization scenarios:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {}\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\", \n          \"parameter\": {}\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/setupJob/#reader-configuration","title":"Reader Configuration","text":"<p>Each reader must specify:</p> <ul> <li><code>name</code>: Reader plugin name</li> <li><code>parameter</code>: Reader-specific parameters</li> </ul> <p>Example MySQL reader:</p> <pre><code>{\n  \"reader\": {\n    \"name\": \"mysqlreader\",\n    \"parameter\": {\n      \"username\": \"root\",\n      \"password\": \"password\",\n      \"column\": [\"id\", \"name\", \"age\"],\n      \"splitPk\": \"id\",\n      \"connection\": [\n        {\n          \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test\",\n          \"table\": [\"user_table\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"en/setupJob/#writer-configuration","title":"Writer Configuration","text":"<p>Each writer must specify:</p> <ul> <li><code>name</code>: Writer plugin name</li> <li><code>parameter</code>: Writer-specific parameters</li> </ul> <p>Example PostgreSQL writer:</p> <pre><code>{\n  \"writer\": {\n    \"name\": \"postgresqlwriter\",\n    \"parameter\": {\n      \"username\": \"postgres\",\n      \"password\": \"password\", \n      \"column\": [\"id\", \"name\", \"age\"],\n      \"connection\": [\n        {\n          \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/test\",\n          \"table\": [\"user_table\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"en/setupJob/#setting-configuration","title":"Setting Configuration","text":"<p>The <code>setting</code> section controls job execution behavior:</p>"},{"location":"en/setupJob/#speed-control","title":"Speed Control","text":"<pre><code>{\n  \"setting\": {\n    \"speed\": {\n      \"channel\": 3,\n      \"byte\": 1048576,\n      \"record\": 10000\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>channel</code>: Number of parallel channels (concurrency level)</li> <li><code>byte</code>: Byte-level speed limit per second</li> <li><code>record</code>: Record-level speed limit per second</li> </ul>"},{"location":"en/setupJob/#error-control","title":"Error Control","text":"<pre><code>{\n  \"setting\": {\n    \"errorLimit\": {\n      \"record\": 0,\n      \"percentage\": 0.02\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>record</code>: Maximum allowed error records</li> <li><code>percentage</code>: Maximum allowed error percentage</li> </ul>"},{"location":"en/setupJob/#data-type-mapping","title":"Data Type Mapping","text":"<p>Addax uses a unified internal type system for data conversion:</p> Addax Type Description Java Type long Long integer java.lang.Long double Double precision float java.lang.Double string String java.lang.String date Date/time java.util.Date bool Boolean java.lang.Boolean bytes Byte array byte[]"},{"location":"en/setupJob/#variable-substitution","title":"Variable Substitution","text":"<p>Addax supports variable substitution in configuration files:</p> <pre><code>{\n  \"reader\": {\n    \"parameter\": {\n      \"jdbcUrl\": \"jdbc:mysql://${host}:${port}/${database}\",\n      \"username\": \"${username}\",\n      \"password\": \"${password}\"\n    }\n  }\n}\n</code></pre> <p>Variables can be passed via command line:</p> <pre><code>bin/addax.sh job.json -p \"-Dhost=localhost -Dport=3306\"\n</code></pre>"},{"location":"en/setupJob/#configuration-examples","title":"Configuration Examples","text":""},{"location":"en/setupJob/#simple-file-transfer","title":"Simple File Transfer","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/input.txt\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\"*\"],\n            \"fieldDelimiter\": \"\\t\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"txtfilewriter\",\n          \"parameter\": {\n            \"path\": \"/tmp/output\",\n            \"fileName\": \"result.txt\",\n            \"encoding\": \"UTF-8\",\n            \"fieldDelimiter\": \"\\t\"\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/setupJob/#database-to-database","title":"Database to Database","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"source_user\",\n            \"password\": \"source_pass\",\n            \"column\": [\"id\", \"name\", \"email\", \"created_time\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://source-host:3306/source_db\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"target_user\",\n            \"password\": \"target_pass\",\n            \"column\": [\"id\", \"name\", \"email\", \"created_time\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://target-host:5432/target_db\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"byte\": 10485760\n      },\n      \"errorLimit\": {\n        \"record\": 10,\n        \"percentage\": 0.1\n      }\n    }\n  }\n}\n</code></pre> <p>For plugin-specific configuration details, please refer to the respective plugin documentation in the reader and writer sections.</p>"},{"location":"en/statsreport/","title":"Statistics Report","text":"<p>Addax provides detailed statistics and monitoring during job execution to help you understand performance and troubleshoot issues.</p>"},{"location":"en/statsreport/#overview","title":"Overview","text":"<p>During execution, Addax reports various metrics including:</p> <ul> <li>Records processed per second</li> <li>Bytes transferred per second  </li> <li>Error counts and percentages</li> <li>Channel performance</li> <li>Memory usage</li> <li>Job progress</li> </ul>"},{"location":"en/statsreport/#statistics-output","title":"Statistics Output","text":""},{"location":"en/statsreport/#console-output","title":"Console Output","text":"<p>During job execution, Addax displays real-time statistics:</p> <pre><code>2023-12-07 10:30:15.123 [Statistics] INFO - Total records: 15000, Speed: 1500 rec/s (1.2 MB/s), Errors: 0 (0.00%)\n2023-12-07 10:30:25.124 [Statistics] INFO - Total records: 30000, Speed: 1500 rec/s (1.2 MB/s), Errors: 2 (0.01%)\n2023-12-07 10:30:35.125 [Statistics] INFO - Total records: 45000, Speed: 1500 rec/s (1.2 MB/s), Errors: 2 (0.00%)\n</code></pre>"},{"location":"en/statsreport/#final-report","title":"Final Report","text":"<p>At job completion, a comprehensive report is displayed:</p> <pre><code>====================Job Statistics====================\nJob ID: 202312071030001\nStart Time: 2023-12-07 10:30:00\nEnd Time: 2023-12-07 10:35:30\nTotal Time: 5m30s\n\nRecords:\n  - Total Read: 100000\n  - Total Written: 99998\n  - Errors: 2\n  - Error Rate: 0.002%\n\nPerformance:\n  - Average Speed: 303 rec/s\n  - Peak Speed: 450 rec/s\n  - Total Bytes: 12.5 MB\n  - Throughput: 38.5 KB/s\n\nChannels:\n  - Channel 0: 25000 records, 310 rec/s\n  - Channel 1: 25000 records, 305 rec/s  \n  - Channel 2: 24999 records, 298 rec/s\n  - Channel 3: 24999 records, 295 rec/s\n========================================================\n</code></pre>"},{"location":"en/statsreport/#enabling-detailed-statistics","title":"Enabling Detailed Statistics","text":""},{"location":"en/statsreport/#command-line-option","title":"Command Line Option","text":"<p>Enable detailed statistics reporting:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.detailed=true\" job.json\n</code></pre>"},{"location":"en/statsreport/#configuration-setting","title":"Configuration Setting","text":"<p>Add to job configuration:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"detailed\": true,\n        \"interval\": 10000\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/statsreport/#statistics-components","title":"Statistics Components","text":""},{"location":"en/statsreport/#performance-metrics","title":"Performance Metrics","text":"Metric Description Unit Records/sec Number of records processed per second rec/s Bytes/sec Amount of data transferred per second B/s Error Rate Percentage of failed records % Channel Utilization Performance of each parallel channel rec/s"},{"location":"en/statsreport/#memory-statistics","title":"Memory Statistics","text":"<pre><code>bin/addax.sh -j \"-Daddax.stats.memory=true\" job.json\n</code></pre> <p>Output includes: - JVM heap usage - Memory allocation rates - Garbage collection statistics</p>"},{"location":"en/statsreport/#io-statistics","title":"I/O Statistics","text":"<p>Track input/output performance:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.io=true\" job.json\n</code></pre> <p>Includes: - Network I/O rates - Disk read/write speeds - Connection pool statistics</p>"},{"location":"en/statsreport/#custom-statistics-interval","title":"Custom Statistics Interval","text":""},{"location":"en/statsreport/#set-update-frequency","title":"Set Update Frequency","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"interval\": 5000\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/statsreport/#disable-statistics","title":"Disable Statistics","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"enabled\": false\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/statsreport/#export-statistics","title":"Export Statistics","text":""},{"location":"en/statsreport/#json-format","title":"JSON Format","text":"<p>Export statistics to JSON file:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.export=/tmp/job_stats.json\" job.json\n</code></pre>"},{"location":"en/statsreport/#csv-format","title":"CSV Format","text":"<p>Export to CSV for analysis:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.export=/tmp/job_stats.csv -Daddax.stats.format=csv\" job.json\n</code></pre>"},{"location":"en/statsreport/#monitoring-integration","title":"Monitoring Integration","text":""},{"location":"en/statsreport/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Enable Prometheus endpoint:</p> <pre><code>bin/addax.sh -j \"-Daddax.metrics.prometheus.enabled=true -Daddax.metrics.prometheus.port=9090\" job.json\n</code></pre>"},{"location":"en/statsreport/#jmx-monitoring","title":"JMX Monitoring","text":"<p>Enable JMX for external monitoring tools:</p> <pre><code>bin/addax.sh -j \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999\" job.json\n</code></pre>"},{"location":"en/statsreport/#performance-analysis","title":"Performance Analysis","text":""},{"location":"en/statsreport/#identifying-bottlenecks","title":"Identifying Bottlenecks","text":"<p>Look for these patterns in statistics:</p> <p>Reader Bottleneck: <pre><code>Channel 0: 1000 rec/s (reader working hard)\nChannel 1: 100 rec/s (waiting for reader)\nChannel 2: 100 rec/s (waiting for reader)\n</code></pre></p> <p>Writer Bottleneck: <pre><code>Channel 0: 200 rec/s (waiting for writer)\nChannel 1: 200 rec/s (waiting for writer)  \nChannel 2: 200 rec/s (waiting for writer)\n</code></pre></p> <p>Network Bottleneck: <pre><code>High record rate but low byte rate indicates small records\nLow record rate but high byte rate indicates large records\n</code></pre></p>"},{"location":"en/statsreport/#optimization-recommendations","title":"Optimization Recommendations","text":"<p>Based on statistics, consider:</p> <ol> <li>Low Channel Utilization: Increase channel count</li> <li>High Error Rate: Check data quality and mappings</li> <li>Memory Issues: Adjust JVM heap size</li> <li>Uneven Channel Performance: Check data distribution</li> </ol>"},{"location":"en/statsreport/#alerting","title":"Alerting","text":""},{"location":"en/statsreport/#error-threshold-alerts","title":"Error Threshold Alerts","text":"<p>Set up alerts for error rates:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"errorLimit\": {\n        \"record\": 100,\n        \"percentage\": 0.1\n      },\n      \"statistics\": {\n        \"alerts\": {\n          \"errorRate\": 0.05\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/statsreport/#performance-alerts","title":"Performance Alerts","text":"<p>Alert on low performance:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"alerts\": {\n          \"minSpeed\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/statsreport/#troubleshooting-with-statistics","title":"Troubleshooting with Statistics","text":""},{"location":"en/statsreport/#common-issues","title":"Common Issues","text":"<p>Job Running Slowly: - Check channel utilization - Monitor memory usage - Verify network connectivity</p> <p>High Error Rate: - Review data type mappings - Check source data quality - Verify target constraints</p> <p>Memory Errors: - Monitor heap usage trends - Check for memory leaks - Adjust JVM settings</p>"},{"location":"en/statsreport/#debug-commands","title":"Debug Commands","text":"<p>Get real-time statistics:</p> <pre><code># Monitor job progress\ntail -f $ADDAX_HOME/log/addax.log | grep Statistics\n\n# Check JVM status\njstat -gc &lt;pid&gt;\n\n# Monitor network connections\nnetstat -an | grep &lt;port&gt;\n</code></pre> <p>This comprehensive statistics system helps ensure your data synchronization jobs run efficiently and provides the visibility needed for troubleshooting and optimization.</p>"},{"location":"en/transformer/","title":"Data Transformation","text":""},{"location":"en/transformer/#transformer-definition","title":"Transformer Definition","text":"<p>During data synchronization and transmission, users may have customized requirements for data processing, such as trimming columns or transforming column values. This can be achieved through the T (Transformer) process in ETL. Addax includes a Transformer module that allows for flexible data transformation by defining a series of UDFs (User-Defined Functions).</p>"},{"location":"en/transformer/#execution-model","title":"Execution Model","text":"<pre><code>graph LR\nsource((\"source\"))\nsubgraph fr[\"Addax Framework\"]\n    direction LR\n    Reader ==&gt; Transformer ==&gt;Writer\nend\ntarget((\"target\"))\nsource ==&gt; fr ==&gt; target</code></pre>"},{"location":"en/transformer/#udf-functions","title":"UDF Functions","text":""},{"location":"en/transformer/#dx_substr","title":"dx_substr","text":"<p><code>dx_substr(idx, pos, length) -&gt; str</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>pos</code>: The starting position within the field's value.</li> <li><code>length</code>: The length of the target substring.</li> </ul> <p>Returns: A substring of the specified length from the specified starting position (inclusive). An exception is thrown if the starting position is invalid. If the field is null, it is returned directly (i.e., this transformer does not process it).</p>"},{"location":"en/transformer/#dx_pad","title":"dx_pad","text":"<p><code>dx_pad(idx, flag, length, chr)</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>flag</code>: \"l\" or \"r\", indicating whether to pad at the beginning (left) or the end (right).</li> <li><code>length</code>: The target length of the field.</li> <li><code>chr</code>: The character to use for padding.</li> </ul> <p>Returns: If the source string's length is less than the target length, it returns the string after padding. If it's longer, it is truncated (always from the right). If the field is null, it is converted to an empty string before padding.</p> <p>Examples:</p> <ul> <li><code>dx_pad(1, \"l\", \"4\", \"A\")</code>: If <code>column 1</code>'s value is <code>xyz</code>, the transformed value is <code>Axyz</code>. If the value is <code>xyzzzzz</code>, it becomes <code>xyzz</code>.</li> <li><code>dx_pad(1, \"r\", \"4\", \"A\")</code>: If <code>column 1</code>'s value is <code>xyz</code>, the transformed value is <code>xyzA</code>. If the value is <code>xyzzzzz</code>, it becomes <code>xyzz</code>.</li> </ul>"},{"location":"en/transformer/#dx_replace","title":"dx_replace","text":"<p><code>dx_replace(idx, pos, length, str) -&gt; str</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>pos</code>: The starting position within the field's value.</li> <li><code>length</code>: The length of the substring to be replaced.</li> <li><code>str</code>: The string to replace with.</li> </ul> <p>Returns: Replaces a substring of a specified length from a specified starting position (inclusive). An exception is thrown if the starting position is invalid. If the field is null, it is returned directly (i.e., this transformer does not process it).</p> <p>Examples:</p> <ul> <li><code>dx_replace(1, \"2\", \"4\", \"****\")</code>: If <code>column 1</code>'s value is <code>addaxTest</code>, it is transformed to <code>da****est</code>.</li> <li><code>dx_replace(1, \"5\", \"10\", \"****\")</code>: If <code>column 1</code>'s value is <code>addaxTest</code>, it is transformed to <code>data****</code>.</li> </ul>"},{"location":"en/transformer/#dx_filter","title":"dx_filter","text":"<p><code>dx_filter(idx, operator, expr) -&gt; str</code></p> <p>Parameters:</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>operator</code>: The operator. Supported operators are <code>like</code>, <code>not like</code>, <code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code>.</li> <li><code>expr</code>: A regular expression (Java-style) or a value.</li> </ul> <p>Returns:</p> <ul> <li>If the condition is met, it returns <code>null</code>, which filters out the entire row. If the condition is not met, the row is kept.</li> <li><code>like</code> and <code>not like</code>: The field is converted to a string and then fully matched against the target regular expression.</li> <li><code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code>: Comparison is performed based on the data type. Numeric types are compared by value; string and boolean types are compared lexicographically.</li> <li>If the target field is <code>null</code>, it will satisfy the <code>= null</code> filter condition and be filtered out. For the <code>!= null</code> condition, <code>null</code> does not satisfy the filter condition and is not filtered. For <code>like</code>, if the field is <code>null</code>, it is not filtered.</li> </ul> <p>Examples:</p> <ul> <li><code>dx_filter(1, \"like\", \"dataTest\")</code></li> <li><code>dx_filter(1, \"&gt;=\", \"10\")</code></li> </ul> <p>Compound filters (i.e., conditions involving multiple fields) are not currently supported as the function parameters would be too complex for users.</p>"},{"location":"en/transformer/#dx_groovy","title":"dx_groovy","text":"<p><code>dx_groovy(code, package) -&gt; record</code></p> <p>Parameters</p> <ul> <li><code>code</code>: Code that conforms to Groovy syntax.</li> <li><code>package</code>: <code>extraPackage</code>, which can be a list or empty.</li> </ul> <p>Returns</p> <p>A <code>Record</code> data type.</p> <p>Notes:</p> <ul> <li><code>dx_groovy</code> can only be called once per transformer configuration. Multiple calls are not allowed.</li> <li>The <code>groovy code</code> supports packages from <code>java.lang</code> and <code>java.util</code>. Objects that can be directly referenced include <code>record</code> and various column types under <code>element</code> (BoolColumn.class, BytesColumn.class, DateColumn.class, DoubleColumn.class, LongColumn.class, StringColumn.class). Other packages are not supported by default. If you need to use other packages, you can set <code>extraPackage</code>. Note that <code>extraPackage</code> does not support third-party JARs.</li> <li>In the <code>groovy code</code>, you must return the updated <code>Record</code> (e.g., <code>record.setColumn(columnIndex, new StringColumn(newValue));</code>) or <code>null</code>. Returning <code>null</code> filters out the current row.</li> <li>You can directly call static utility methods (GroovyTransformerStaticUtil).</li> </ul> <p>Examples:</p> <p>Groovy implementation of <code>subStr</code>:</p> <pre><code>String code=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = oriValue.substring(0, 3);\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\ndx_groovy(code); // Note: The original doc had `dx_groovy(record)` which is incorrect. It should be the code string.\n</code></pre> <p>Groovy implementation of <code>replace</code>:</p> <pre><code>String code2=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = \\\"****\\\" + oriValue.substring(3, oriValue.length());\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>Groovy implementation of <code>pad</code>:</p> <pre><code>String code3=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String padString = \\\"12345\\\";\\n\"+\n        \" String finalPad = \\\"\\\";\\n\"+\n        \" int NeedLength = 8 - oriValue.length();\\n\"+\n        \"        while (NeedLength &gt; 0) {\\n\"+\n        \"\\n\"+\n        \"            if (NeedLength &gt;= padString.length()) {\\n\"+\n        \"                finalPad += padString;\\n\"+\n        \"                NeedLength -= padString.length();\\n\"+\n        \"            } else {\\n\"+\n        \"                finalPad += padString.substring(0, NeedLength);\\n\"+\n        \"                NeedLength = 0;\\n\"+\n        \"            }\\n\"+\n        \"        }\\n\"+\n        \" String newValue= finalPad + oriValue;\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>Starting from version <code>4.1.2</code>, <code>dx_groovy</code> supports loading Groovy code from an external file. The file is read relative to the <code>$ADDAX_HOME</code> directory, which is the installation directory of Addax.</p> <p>For example, to implement <code>subStr</code>, you can create a file <code>job/substr.groovy</code> with the following content:</p> <pre><code>Column column = record.getColumn(1)\nString oriValue = column.asString()\nString newValue = oriValue.substring(0, 3)\nrecord.setColumn(1, new StringColumn(newValue))\nreturn record\n</code></pre> <p>Then, define it in the <code>job</code> file like this:</p> <pre><code>{\n  \"transformer\": [\n    {\n      \"name\": \"dx_groovy\",\n      \"parameter\": {\n        \"codeFile\": \"job/substr.groovy\"\n      }\n    }\n  ]\n}\n</code></pre> <p>You can also specify an absolute path for the file.</p>"},{"location":"en/transformer/#job-definition","title":"Job Definition","text":"<p>In this example, four UDFs are configured.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"My name is xxxx\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"password is Passw0rd\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"random\": \"0,10\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      },\n      \"transformer\": [\n        {\n          \"name\": \"dx_replace\",\n          \"parameter\": {\n            \"columnIndex\": 0,\n            \"paras\": [\n              \"11\",\n              \"6\",\n              \"wgzhao\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_substr\",\n          \"parameter\": {\n            \"columnIndex\": 1,\n            \"paras\": [\n              \"0\",\n              \"12\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_map\",\n          \"parameter\": {\n            \"columnIndex\": 2,\n            \"paras\": [\n              \"^\",\n              \"2\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_filter\",\n          \"parameter\": {\n            \"columnIndex\": 6,\n            \"paras\": [\n              \"&lt;\",\n              \"5\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"en/transformer/#custom-functions","title":"Custom Functions","text":"<p>If the built-in functions do not meet your data transformation requirements, you can write code that conforms to Groovy specifications within the <code>transformer</code>. Here is a complete example:</p> <pre><code>\n</code></pre> <p>The <code>transformer</code> code above modifies the first two fields of each record. It adds the prefix <code>Header_</code> to the first string field and doubles the value of the second integer field. The execution result is as follows:</p> <pre><code>$ bin/addax.sh job/transformer_demo.json \n\n  ___      _     _            \n / _ \\    | |   | |           \n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt; \n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-04 15:45:56.421 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-04 15:45:56.443 [        main] INFO  Engine               - \n\n.....\n\n2021-08-04 15:45:56.458 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-04 15:45:56.459 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-04 15:45:56.460 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-04 15:45:56.470 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-04 15:45:56.498 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-04 15:45:56.505 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-08-04 15:45:56.520 [ taskGroup-0] INFO  TransformerUtil      -  user config transformers [[dx_groovy]], loading...\n2021-08-04 15:45:56.531 [ taskGroup-0] INFO  TransformerUtil      -  1 of transformer init success. name=dx_groovy, isNative=true parameter = \n  {\"code\":\"record.setColumn(0, new StringColumn('Header_' + record.getColumn(0).asString()));record.setColumn(1, new LongColumn(record.getColumn(1).asLong() * 2));return record;\"}\n\nHeader_Addax    2       1989-06-04 00:00:01     true    test\nHeader_Addax    4       1989-06-03 00:00:01     true    test\nHeader_Addax    6       1989-06-02 00:00:01     true    test\nHeader_Addax    8       1989-06-01 00:00:01     true    test\nHeader_Addax    10      1989-05-31 00:00:01     true    test\nHeader_Addax    12      1989-05-30 00:00:01     true    test\nHeader_Addax    14      1989-05-29 00:00:01     true    test\nHeader_Addax    16      1989-05-28 00:00:01     true    test\nHeader_Addax    18      1989-05-27 00:00:01     true    test\nHeader_Addax    20      1989-05-26 00:00:01     true    test\n\n2021-08-04 15:45:59.515 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-04 15:45:59.517 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-04 15:45:59.518 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-08-04 15:45:59.521 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-04 15:45:59.524 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 330 bytes | Speed 110B/s, 3 records/s | Error 0 records, 0 bytes |  \n  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Transformer Success 10 records | Transformer Error 0 records | Transformer Filter 0 records \n  | Transformer usedTime 0.383s | Percentage 100.00%\n2021-08-04 15:45:59.527 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-04 15:45:56\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-04 15:45:59\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              110B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n\n2021-08-04 15:45:59.528 [       job-0] INFO  JobContainer         - \nTransformer\u6210\u529f\u8bb0\u5f55\u603b\u6570         :                  10\nTransformer\u5931\u8d25\u8bb0\u5f55\u603b\u6570         :                   0\nTransformer\u8fc7\u6ee4\u8bb0\u5f55\u603b\u6570         :                   0\n</code></pre>"},{"location":"en/transformer/#metrics-and-dirty-data","title":"Metrics and Dirty Data","text":"<p>The Transform process involves data conversion, which may increase or decrease the amount of data. Therefore, precise metrics are needed, including:</p> <ul> <li>Number of input records and bytes for the Transform.</li> <li>Number of output records and bytes from the Transform.</li> <li>Number of dirty data records and bytes from the Transform.</li> <li>If there are multiple Transforms, and one of them generates dirty data, subsequent transforms will not be executed for that record, and it will be directly counted as dirty data.</li> <li>Currently, only overall metrics for all Transforms are provided (success, failure, filtered counts, and time consumed by the transform).</li> </ul> <p>The metrics displayed during the process are defined as follows:</p> <pre><code>Total 1000000 records, 22000000 bytes | Transform 100000 records(in), 10000 records(out) | Speed 2.10MB/s, 100000 records/s | Error 0 records, 0 bytes | Percentage 100.00%\n</code></pre> <p>Note: This mainly records the input and output of the transformation, which requires monitoring changes in the number of data records.</p> <p>The final job metrics are displayed as follows:</p> <pre><code>Job start  at             : 2025-07-23 09:08:26\nJob end    at             : 2025-07-23 09:08:29\nJob took secs             :                  3s\nAverage   bps             :              110B/s\nAverage   rps             :              3rec/s\nNumber of rec             :                  10\nFailed record             :                   0\nTransformer success records:                  10\nTransformer failed  records:                   0\nTransformer filter  records:                   0\n</code></pre> <p>Note: This mainly records the input and output of the transformation, which requires monitoring changes in the number of data records.</p>"},{"location":"en/reader/accessreader/","title":"Access Reader","text":"<p>AccessReader implements the ability to read data from Access databases, based on Addax RDBMS Reader.</p>"},{"location":"en/reader/accessreader/#example","title":"Example","text":"<p>We download the test file AccessThemeDemo.zip, extract it to get the <code>AccessThemeDemo.mdb</code> file, which contains a <code>tbl_Users</code> table. We will synchronize the data from this table to the terminal.</p> <p>The following configuration reads the table to the terminal:</p> job/access2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"accessreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"tbl_Users\"\n            ],\n            \"jdbcUrl\": \"jdbc:ucanaccess:///Users/wgzhao/Downloads/AccessThemeDemo.mdb\"\n          },\n          \"where\": \"\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"encoding\": \"utf-8\",\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/access2stream.json</code></p>"},{"location":"en/reader/accessreader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/access2stream.json\n</code></pre>"},{"location":"en/reader/accessreader/#parameters","title":"Parameters","text":"<p>AccessReader is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/cassandrareader/","title":"Cassandra Reader","text":"<p><code>CassandraReader</code> plugin implements the ability to read data from Cassandra.</p>"},{"location":"en/reader/cassandrareader/#configuration","title":"Configuration","text":"<p>Below is an example configuration for reading data from Cassandra to terminal</p> job/cassandra2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"cassandrareader\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"test\",\n          \"table\": \"addax_src\",\n          \"column\": [\n            \"textCol\",\n            \"blobCol\",\n            \"writetime(blobCol)\",\n            \"boolCol\",\n            \"smallintCol\",\n            \"tinyintCol\",\n            \"intCol\",\n            \"bigintCol\",\n            \"varintCol\",\n            \"floatCol\",\n            \"doubleCol\",\n            \"decimalCol\",\n            \"dateCol\",\n            \"timeCol\",\n            \"timeStampCol\",\n            \"uuidCol\",\n            \"inetCol\",\n            \"durationCol\",\n            \"listCol\",\n            \"mapCol\",\n            \"setCol\",\n            \"tupleCol\",\n            \"udtCol\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/cassandrareader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description host Yes list None Connection domain or IP, multiple nodes separated by commas port Yes int 9042 Port username No string None Username password No string None Password useSSL No boolean false Whether to use SSL connection keyspace Yes string None Keyspace where the table to be synchronized is located table Yes string None Selected table to be synchronized column Yes list None Collection of columns to be synchronized in the configured table, elements can specify column name or <code>writetime(column_name)</code>, the latter form reads timestamp of <code>column_name</code> column instead of data where No string None Data filtering condition <code>cql</code> expression allowFiltering No boolean None Whether to filter data on server side, refer to official documentation for detailed description consistencyLevel No string LOCAL_QUORUM Data consistency level, options: <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code>"},{"location":"en/reader/clickhousereader/","title":"ClickHouse Reader","text":"<p><code>ClickHouseReader</code> plugin supports reading data from ClickHouse database.</p>"},{"location":"en/reader/clickhousereader/#example","title":"Example","text":""},{"location":"en/reader/clickhousereader/#table-structure-and-data-information","title":"Table Structure and Data Information","text":"<p>Assume the table structure and data to be read are as follows:</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre>"},{"location":"en/reader/clickhousereader/#configure-json-file","title":"Configure JSON File","text":"<p>The following configuration file reads specified table data from ClickHouse database and prints to terminal</p> job/clickhouse2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/clickhouse2stream.json</code></p>"},{"location":"en/reader/clickhousereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/clickhouse2stream.json\n</code></pre> <p>The output information is as follows (non-critical information removed):</p> <pre><code>021-01-06 14:39:35.742 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n\n2021-01-06 14:39:35.767 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        \"*\"\n                    ],\n                    \"connection\":[\n                        {\n                            \"jdbcUrl\":[\n                                \"jdbc:clickhouse://127.0.0.1:8123/\"\n                            ],\n                            \"table\":[\n                                \"ck_addax\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"default\"\n                },\n                \"name\":\"clickhousereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n127 -32768  2147483647  -9223372036854775808    255 65535   4294967295  18446744073709551615    1   1   1234567891234567891234567891.1234567891Hello String 2c:16:db:a3:3a:4f   \n5f042a36-5b0c-4f71-adfd-4df4fca1b863    2021-01-01  2021-01-01 00:00:00 2021-01-01 00:00:00 hello\n\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-06 14:39:35\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-06 14:39:39\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               77B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              0rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   1\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/clickhousereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"en/reader/clickhousereader/#supported-data-types","title":"Supported Data Types","text":"Addax Internal Type ClickHouse Data Type Long Uint8, Uint16, Uint32, Uint64, Int8, Int16, Int32, Int64, Enum8, Enum16 Double Float32, Float64, Decimal String String, FixedString(N) Date Date, DateTime, DateTime64 Boolean UInt8 Bytes String"},{"location":"en/reader/clickhousereader/#limitations","title":"Limitations","text":"<p>Except for the above listed field types, other types are not supported, such as Array, Nested, etc.</p>"},{"location":"en/reader/databendreader/","title":"Databend Reader","text":"<p>DatabendReader plugin implements reading data from Databend.</p> <p>Note that Databend has MySQL client protocol compatible implementation, so you can directly use MySQL Reader to read Databend data.</p>"},{"location":"en/reader/databendreader/#example","title":"Example","text":"<p>We can start Databend database in the following way</p> <pre><code>docker run  -tid  --rm  -p 8000:8000 \\\n   -e QUERY_DEFAULT_USER=databend \\\n   -e QUERY_DEFAULT_PASSWORD=databend \\\n   datafuselabs/databend\n</code></pre> <p>Then create a table to read</p> <pre><code>(\n    id int,\n    name varchar(255),\n    salary float,\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>And populate necessary data</p> <p>The following configuration reads this table to terminal:</p> job/databend2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"databendreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://127.0.0.1:8000/default\",\n            \"table\": [\n              \"addax_reader\"\n            ]\n          },\n          \"username\": \"databend\",\n          \"password\": \"databend\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/databend2stream.json</code></p>"},{"location":"en/reader/databendreader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/databend2stream.json\n</code></pre>"},{"location":"en/reader/databendreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"en/reader/databendreader/#limitations","title":"Limitations","text":"<p>None at the moment</p>"},{"location":"en/reader/datareader/","title":"Data Reader","text":"<p><code>DataReader</code> plugin is specifically designed to generate data that meets certain rule requirements in development and testing environments.</p> <p>In actual development and testing, we need to generate test data according to certain business rules, not just random content, such as ID card numbers, bank account numbers, stock codes, etc.</p>"},{"location":"en/reader/datareader/#why-reinvent-the-wheel","title":"Why Reinvent the Wheel","text":"<p>Indeed, there are quite a few specialized data generation tools on the Internet, many of which are powerful and performant. However, most of these tools consider the data generation part but ignore the problem of writing data to the target end, or some consider it but only consider one or a limited number of databases.</p> <p>It happens that the Addax tool can provide enough target-end writing capabilities, plus the existing Stream Reader is already a simple version of data generation tool. Therefore, adding some specific rules to this functionality and leveraging the diversity of the writing end naturally makes it a good data generation tool.</p>"},{"location":"en/reader/datareader/#configuration-example","title":"Configuration Example","text":"<p>Here I list all the rules currently supported by the plugin in the example below</p> datareader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"datareader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"1,100,\",\n              \"rule\": \"random\",\n              \"type\": \"double\"\n            },\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"1\",\n              \"rule\": \"incr\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989/06/04 00:00:01,-1\",\n              \"rule\": \"incr\",\n              \"type\": \"date\",\n              \"dateFormat\": \"yyyy/MM/dd hh:mm:ss\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"rule\": \"address\"\n            },\n            {\n              \"rule\": \"bank\"\n            },\n            {\n              \"rule\": \"company\"\n            },\n            {\n              \"rule\": \"creditCard\"\n            },\n            {\n              \"rule\": \"debitCard\"\n            },\n            {\n              \"rule\": \"idCard\"\n            },\n            {\n              \"rule\": \"lat\"\n            },\n            {\n              \"rule\": \"lng\"\n            },\n            {\n              \"rule\": \"name\"\n            },\n            {\n              \"rule\": \"job\"\n            },\n            {\n              \"rule\": \"phone\"\n            },\n            {\n              \"rule\": \"stockCode\"\n            },\n            {\n              \"rule\": \"stockAccount\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content to <code>job/datareader2stream.json</code></p> <p>Then execute this task, the output result is similar to the following:</p> <pre><code>$ bin/addax.sh job/datareader2stream.json\n\n  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-13 17:02:00.888 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-13 17:02:00.910 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"rule\":\"random\",\n                            \"type\":\"double\",\n                            \"scale\": \"2\",\n                            \"value\":\"1,100,\"\n                        },\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"rule\":\"incr\",\n                            \"type\":\"long\",\n                            \"value\":\"1\"\n                        },\n                        {\n                            \"dateFormat\":\"yyyy/MM/dd hh:mm:ss\",\n                            \"rule\":\"incr\",\n                            \"type\":\"date\",\n                            \"value\":\"1989/06/04 00:00:01,-1\"\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        },\n                        {\n                            \"rule\":\"address\"\n                        },\n                        {\n                            \"rule\":\"bank\"\n                        },\n                        {\n                            \"rule\":\"company\"\n                        },\n                        {\n                            \"rule\":\"creditCard\"\n                        },\n                        {\n                            \"rule\":\"debitCard\"\n                        },\n                        {\n                            \"rule\":\"idCard\"\n                        },\n                        {\n                            \"rule\":\"lat\"\n                        },\n                        {\n                            \"rule\":\"lng\"\n                        },\n                        {\n                            \"rule\":\"name\"\n                        },\n                        {\n                            \"rule\":\"job\"\n                        },\n                        {\n                            \"rule\":\"phone\"\n                        },\n                        {\n                            \"rule\":\"stockCode\"\n                        },\n                        {\n                            \"rule\":\"stockAccount\"\n                        }\n                    ],\n                    \"sliceRecordCount\":10\n                },\n                \"name\":\"datareader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-08-13 17:02:00.937 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-13 17:02:00.938 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-13 17:02:00.940 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-13 17:02:00.976 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do prepare work .\n2021-08-13 17:02:00.977 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-13 17:02:00.978 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-13 17:02:00.979 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] splits to [1] tasks.\n2021-08-13 17:02:00.980 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-13 17:02:01.002 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-13 17:02:01.009 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n7.65    DataX   1   1989-06-04 00:00:01 test    \u5929\u6d25\u5e02\u5357\u4eac\u53bf\u957f\u5bff\u533a\u5149\u660e\u8def263\u53f7    \u4ea4\u901a\u94f6\u884c    \u6613\u52a8\u529b\u4fe1\u606f\u6709\u9650\u516c\u53f8   6227894836568607    6235712610856305437 450304194808316766  31.3732613  -125.3507716    \u9f9a\u519b  \u673a\u7535\u5de5\u7a0b\u5e08   13438631667 726929  8741848665\n18.58   DataX   2   1989-06-03 00:00:01 test    \u6c5f\u82cf\u7701\u592a\u539f\u5e02\u6d54\u9633\u533a\u4e1c\u5c71\u8def33\u53f7 \u4e2d\u56fd\u94f6\u884c    \u65f6\u7a7a\u76d2\u6570\u5b57\u4fe1\u606f\u6709\u9650\u516c\u53f8 4096666711928233    6217419359154239015 220301200008188547  48.6648764  104.8567048 \u5321\u98de  \u5316\u5986\u5e08 18093137306 006845  1815787371\n16.16   DataX   3   1989-06-02 00:00:01 test    \u53f0\u6e7e\u7701\u90af\u90f8\u5e02\u6e05\u6cb3\u533a\u4e07\u987a\u8def10\u53f7 \u5927\u540c\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u79d1\u6280\u6709\u9650\u516c\u53f8 4096713966912225    6212977716107080594 150223196408276322  29.0134395  142.6426842 \u652f\u6ce2  \u5ba1\u6838\u5458 13013458079 020695  3545552026\n63.89   DataX   4   1989-06-01 00:00:01 test    \u4e0a\u6d77\u5e02\u8f9b\u96c6\u53bf\u516d\u679d\u7279\u533a\u7518\u56ed\u8def119\u53f7   \u4e2d\u56fd\u519c\u4e1a\u94f6\u884c  \u6cf0\u9e92\u9e9f\u4f20\u5a92\u6709\u9650\u516c\u53f8   6227893481508780    6215686558778997167 220822196208286838  -71.6484635 111.8181273 \u656c\u5764  \u623f\u5730\u4ea7\u5ba2\u670d   13384928291 174445  0799668655\n79.18   DataX   5   1989-05-31 00:00:01 test    \u9655\u897f\u7701\u5357\u4eac\u5e02\u671d\u9633\u533a\u5927\u80dc\u8def170\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u6656\u6765\u8ba1\u7b97\u673a\u4fe1\u606f\u6709\u9650\u516c\u53f8 6227535683896707    6217255315590053833 350600198508222018  -24.9783587 78.017024   \u848b\u6768  \u56fa\u5b9a\u8d44\u4ea7\u4f1a\u8ba1  18766298716 402188  9633759917\n14.97   DataX   6   1989-05-30 00:00:01 test    \u6d77\u5357\u7701\u957f\u6625\u53bf\u74a7\u5c71\u533a\u78a7\u6d77\u8857147\u53f7    \u534e\u590f\u94f6\u884c    \u6d59\u5927\u4e07\u670b\u79d1\u6280\u6709\u9650\u516c\u53f8  6224797475369912    6215680436662199846 220122199608190275  -3.5088667  -40.2634359 \u8fb9\u6768  \u7763\u5bfc/\u5de1\u5e97   13278765923 092780  2408887582\n45.49   DataX   7   1989-05-29 00:00:01 test    \u53f0\u6e7e\u7701\u6f5c\u6c5f\u53bf\u6881\u5e73\u533a\u4e03\u661f\u8857201\u53f7    \u664b\u57ce\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u4fe1\u606f\u6709\u9650\u516c\u53f8 5257468530819766    6213336008535546044 141082197908244004  -72.9200596 120.6018163 \u6851\u660e  \u7cfb\u7edf\u5de5\u7a0b\u5e08   13853379719 175864  8303448618\n8.45    DataX   8   1989-05-28 00:00:01 test    \u6d77\u5357\u7701\u676d\u5dde\u53bf\u57ce\u5317\u533a\u5929\u5174\u8def11\u53f7 \u5927\u540c\u5546\u884c    \u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8  6227639043120062    6270259717880740332 430405198908214042  -16.5115338 -39.336119  \u8983\u5065  \u4eba\u4e8b\u603b\u76d1    13950216061 687461  0216734574\n15.01   DataX   9   1989-05-27 00:00:01 test    \u4e91\u5357\u7701\u60e0\u5dde\u5e02\u548c\u5e73\u533a\u6d77\u9e25\u8857201\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u9ec4\u77f3\u91d1\u627f\u4fe1\u606f\u6709\u9650\u516c\u53f8  6200358843233005    6235730928871528500 130300195008312067  -61.646097  163.0882369 \u536b\u5efa\u534e \u7535\u8bdd\u91c7\u7f16    15292600492 001658  1045093445\n55.14   DataX   10  1989-05-26 00:00:01 test    \u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7    \u5eca\u574a\u94f6\u884c    \u521b\u6c47\u79d1\u6280\u6709\u9650\u516c\u53f8    6227605280751588    6270262330691012025 341822200908168063  77.2165746  139.5431377 \u6c60\u6d69  \u591a\u5a92\u4f53\u8bbe\u8ba1   18693948216 201678  0692522928\n\n2021-08-13 17:02:04.020 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-13 17:02:04.021 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-13 17:02:04.022 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do post work.\n2021-08-13 17:02:04.025 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-13 17:02:04.028 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 1817 bytes | Speed 605B/s, 3 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-08-13 17:02:04.030 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-13 17:02:00\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-13 17:02:04\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              605B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/datareader/#configuration-description","title":"Configuration Description","text":"<p>The configuration of <code>column</code> is slightly different from other plugins. A field consists of the following configuration items:</p> Configuration Required Default Value Example Description value No None <code>Addax</code> Data value, required in some cases rule No <code>constant</code> <code>idCard</code> Data generation rule, detailed description below type No <code>string</code> <code>double</code> Data value type dateFormat No <code>yyyy-MM-dd HH:mm:ss</code> <code>yyyy/MM/dd HH:mm:ss</code> Date format, only valid when <code>type</code> is <code>date</code>"},{"location":"en/reader/datareader/#rule-description","title":"rule Description","text":"<p>The core of this plugin's field configuration is the <code>rule</code> field, which is used to indicate what kind of data should be generated, and according to different rules, combined with other configuration options to generate data that meets expectations. Currently, the configurations of <code>rule</code> are all built-in supported rules, custom rules are not supported yet. Detailed description follows:</p>"},{"location":"en/reader/datareader/#constant","title":"constant","text":"<p><code>constant</code> is the default configuration of <code>rule</code>. This rule means that the data value to be generated is determined by the <code>value</code> configuration item, with no changes. For example:</p> <pre><code>{\n  \"value\": \"Addax\",\n  \"type\": \"string\",\n  \"rule\": \"constant\"\n}\n</code></pre> <p>This means the data value generated by this field is all <code>Addax</code></p>"},{"location":"en/reader/datareader/#incr","title":"incr","text":"<p>The meaning of <code>incr</code> configuration item is consistent with the <code>incr</code> meaning in the <code>streamreader</code> plugin, indicating this is an incremental data generation rule. For example:</p> <pre><code>{\n  \"value\": \"1,2\",\n  \"rule\": \"incr\",\n  \"type\": \"long\"\n}\n</code></pre> <p>This means the field data is a long integer, starting from 1, incrementing by 2 each time, forming an incremental sequence starting from 1 with step size 2.</p> <p>For more detailed configuration rules and precautions for this field, refer to the <code>incr</code> description in streamreader.</p>"},{"location":"en/reader/datareader/#random","title":"random","text":"<p>The meaning of <code>random</code> configuration item is consistent with the <code>random</code> meaning in the <code>streamreader</code> plugin, indicating this is an incremental data generation rule. For example:</p> <pre><code>{\n  \"value\": \"1,10\",\n  \"rule\": \"random\",\n  \"type\": \"string\"\n}\n</code></pre> <p>This means the field data is a random string with length 1 to 10 (both 1 and 10 included).</p> <p>For more detailed configuration rules and precautions for this field, refer to the <code>random</code> description in streamreader.</p> Rule Name Meaning Example Data Type Description <code>address</code> Randomly generate address information that basically meets domestic actual conditions <code>\u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7</code> string <code>bank</code> Randomly generate a domestic bank name <code>\u534e\u590f\u94f6\u884c</code> string <code>company</code> Randomly generate a company name <code>\u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8</code> string <code>creditCard</code> Randomly generate a credit card number <code>430405198908214042</code> string 16 digits <code>debitCard</code> Randomly generate a debit card number <code>6227894836568607</code> string 19 digits <code>email</code> Randomly generate an email address <code>ok2a@gmail.com</code> string <code>idCard</code> Randomly generate a domestic ID card number <code>350600198508222018</code> string 18 digits, follows validation rules, first 6 digits meet administrative division requirements <code>lat</code> Randomly generate latitude data <code>48.6648764</code> double Fixed 7 decimal places, can also use <code>latitude</code> <code>lng</code> Randomly generate longitude data <code>120.6018163</code> double Fixed 7 decimal places, can also use <code>longitude</code> <code>name</code> Randomly generate a domestic name <code>\u6c60\u6d69</code> string Currently doesn't consider surname distribution in China <code>job</code> Randomly generate a domestic job title <code>\u7cfb\u7edf\u5de5\u7a0b\u5e08</code> string Data source from recruitment websites <code>phone</code> Randomly generate a domestic mobile phone number <code>15292600492</code> string Currently doesn't consider virtual phone numbers <code>stockCode</code> Randomly generate a 6-digit stock code <code>687461</code> string First two digits meet domestic stock code standards <code>stockAccount</code> Randomly generate a 10-digit stock trading account <code>0692522928</code> string Completely random, doesn't meet account standards <code>uuid</code> Randomly generate a UUID string <code>bc1cf125-929b-43b7-b324-d7c4cc5a75d2</code> string Completely random <code>zipCode</code> Randomly generate a domestic postal code <code>411105</code> long Doesn't fully meet domestic postal code standards <p>Note: The data types returned by the rules in the above table are fixed and cannot be modified, so <code>type</code> doesn't need to be configured, and the configured type will be ignored. Since data generation comes from internal rules, <code>value</code> also doesn't need to be configured, and the configured content will be ignored.</p>"},{"location":"en/reader/dbfreader/","title":"Dbf Reader","text":"<p><code>DbfReader</code> plugin supports reading DBF format files.</p>"},{"location":"en/reader/dbfreader/#configuration","title":"Configuration","text":"<p>The following is a configuration example for reading DBF files and printing to terminal</p> jobs/dbf2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"dbfreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"dbf\",\n              \"type\": \"string\"\n            }\n          ],\n          \"path\": [\n            \"/tmp/out\"\n          ],\n          \"encoding\": \"GBK\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/dbfreader/#parameters","title":"Parameters","text":"<p><code>parameter</code> configuration supports the following configurations:</p> Configuration Required Default Value Description path Yes None DBF file path, supports writing multiple paths, detailed description below column Yes None Collection of columns to be synchronized in the configured table, is a collection of <code>{type: value}</code> or <code>{type: index}</code>, detailed configuration below encoding No GBK DBF file encoding, such as <code>GBK</code>, <code>UTF-8</code> nullFormat No <code>\\N</code> Define which string can represent null"},{"location":"en/reader/dbfreader/#path","title":"path","text":"<p>Description: Path information of local file system, note that multiple paths can be supported here.</p> <ul> <li>When specifying a single local file, DbfFileReader can currently only use single-threaded data extraction. Phase 2 considers multi-threaded concurrent reading for single files in uncompressed file situations.</li> <li>When specifying multiple local files, DbfFileReader supports using multi-threaded data extraction. Thread concurrency is specified by the number of channels.</li> <li>When specifying wildcards, DbfFileReader attempts to traverse multiple file information. For example: specifying <code>/*</code> represents reading all files under the / directory, specifying <code>/foo/*</code> represents reading all files under the <code>foo</code> directory. dbfFileReader currently only supports <code>*</code> as a file wildcard.</li> </ul> <p>It is particularly important to note that Addax treats all dbf files synchronized under one job as the same data table. Users must ensure that all files can adapt to the same set of schema information. Users must ensure that the read files are in dbf-like format and provide Addax with read permissions.</p> <p>It is particularly important to note that if there are no matching files for extraction under the path specified by Path, Addax will report an error.</p>"},{"location":"en/reader/dbfreader/#column","title":"column","text":"<p>List of fields to read, <code>type</code> specifies the type of source data, <code>name</code> is the field name with maximum length of 8, <code>value</code> specifies that the current type is constant, not reading data from source file, but automatically generating corresponding columns based on <code>value</code> value.</p> <p>By default, users can read all data as <code>String</code> type, configured as follows:</p> <pre><code>{\n  \"column\": [\"*\"]\n}\n</code></pre> <p>Users can specify Column field information, configured as follows:</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"addax\"\n  }\n]\n</code></pre> <ul> <li><code>\"index\": 0</code> means getting int field from the first column of local DBF file</li> <li><code>\"value\": \"addax\"</code> means generating string field <code>addax</code> internally from dbfFileReader as the current field. For user-specified <code>column</code> information, <code>type</code> must be filled, and <code>index</code> and <code>value</code> must choose one.</li> </ul>"},{"location":"en/reader/dbfreader/#supported-data-types","title":"Supported Data Types","text":"<p>Local files provide data types themselves, this type is defined by Addax dbfFileReader:</p> Addax Internal Type Local File Data Type Long Long Double Double String String Boolean Boolean Date Date <p>Where:</p> <ul> <li>Long refers to string representation of integer form in local file text, such as <code>19901219</code>.</li> <li>Double refers to string representation of Double form in local file text, such as <code>3.1415</code>.</li> <li>Boolean refers to string representation of Boolean form in local file text, such as <code>true</code>, <code>false</code>. Case insensitive.</li> <li>Date refers to string representation of Date form in local file text, such as <code>2014-12-31</code>, can configure <code>dateFormat</code> to specify format.</li> </ul>"},{"location":"en/reader/elasticsearchreader/","title":"ElasticSearchReader","text":"<p>ElasticSearchReader plugin implements the functionality of reading indexes from Elasticsearch. It uses the Rest API provided by Elasticsearch (default port 9200) to execute specified query statements and batch retrieve data.</p>"},{"location":"en/reader/elasticsearchreader/#example","title":"Example","text":"<p>Assume the index content to be retrieved is as follows</p> <pre><code>{\n  \"took\": 14,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 1,\n    \"hits\": [\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"38\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      },\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"103\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>Configure a task to read data from Elasticsearch and print to terminal</p> job/es2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"elasticsearchreader\",\n        \"parameter\": {\n          \"endpoint\": \"http://127.0.0.1:9200\",\n          \"accessId\": \"\",\n          \"accesskey\": \"\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"searchType\": \"dfs_query_then_fetch\",\n          \"headers\": {},\n          \"scroll\": \"3m\",\n          \"search\": [\n            {\n              \"query\": {\n                \"match\": {\n                  \"col_ip\": \"1.1.1.1\"\n                }\n              },\n              \"aggregations\": {\n                \"top_10_states\": {\n                  \"terms\": {\n                    \"field\": \"col_date\",\n                    \"size\": 10\n                  }\n                }\n              }\n            }\n          ],\n          \"column\": [\n            \"col_ip\",\n            \"col_double\",\n            \"col_long\",\n            \"col_integer\",\n            \"col_keyword\",\n            \"col_text\",\n            \"col_geo_point\",\n            \"col_date\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/es2stream.json</code></p> <p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/es2stream.json\n</code></pre> <p>The output result is similar to the following (output records are reduced):</p> <pre><code>2021-02-19 13:38:15.860 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-02-19 13:38:15.895 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"accessId\":\"\",\n                    \"headers\":{},\n                    \"endpoint\":\"http://127.0.0.1:9200\",\n                    \"search\":[\n                      {\n                        \"query\": {\n                          \"match\": {\n                            \"col_ip\": \"1.1.1.1\"\n                          }\n                        },\n                        \"aggregations\": {\n                          \"top_10_states\": {\n                            \"terms\": {\n                              \"field\": \"col_date\",\n                              \"size\": 10\n                            }\n                          }\n                        }\n                      }\n                    ],\n                    \"accesskey\":\"*****\",\n                    \"searchType\":\"dfs_query_then_fetch\",\n                    \"scroll\":\"3m\",\n                    \"column\":[\n                        \"col_ip\",\n                        \"col_double\",\n                        \"col_long\",\n                        \"col_integer\",\n                        \"col_keyword\",\n                        \"col_text\",\n                        \"col_geo_point\",\n                        \"col_date\"\n                    ],\n                    \"index\":\"test-1\",\n                    \"type\":\"default\"\n                },\n                \"name\":\"elasticsearchreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-02-19 13:38:15.934 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-19 13:38:15.934 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-19 13:38:15.937 [main] INFO  JobContainer - Set jobId = 0\n\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n\n2021-02-19 13:38:19.845 [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.\n2021-02-19 13:38:19.848 [job-0] INFO  JobContainer - Addax Writer.Job [streamwriter] do post work.\n2021-02-19 13:38:19.849 [job-0] INFO  JobContainer - Addax Reader.Job [elasticsearchreader] do post work.\n2021-02-19 13:38:19.855 [job-0] INFO  JobContainer - PerfTrace not enable!\n2021-02-19 13:38:19.858 [job-0] INFO  StandAloneJobContainerCommunicator - Total 95 records, 8740 bytes | Speed 2.84KB/s, 31 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.103s | Percentage 100.00%\n2021-02-19 13:38:19.861 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-19 13:38:15\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-19 13:38:19\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.84KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             31rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   2\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/elasticsearchreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description endpoint Yes string None ElasticSearch connection address accessId No string <code>\"\"</code> User in http auth accessKey No string <code>\"\"</code> Password in http auth index Yes string None Index name in elasticsearch type No string index name Type name of index in elasticsearch search Yes list <code>[]</code> JSON format API search data body column Yes list None Fields to be read timeout No int 60 Client timeout (unit: seconds) discovery No boolean false Enable node discovery (polling) and periodically update server list in client compression No boolean true HTTP request, enable compression multiThread No boolean true HTTP request, whether multi-threaded searchType No string <code>dfs_query_then_fetch</code> Search type headers No map <code>{}</code> HTTP request headers scroll No string <code>\"\"</code> Scroll pagination configuration"},{"location":"en/reader/elasticsearchreader/#search","title":"search","text":"<p>The search configuration item allows configuration of content that meets Elasticsearch API query requirements, like this:</p> <pre><code>{\n  \"query\": {\n    \"match\": {\n      \"message\": \"myProduct\"\n    }\n  },\n  \"aggregations\": {\n    \"top_10_states\": {\n      \"terms\": {\n        \"field\": \"state\",\n        \"size\": 10\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/elasticsearchreader/#searchtype","title":"searchType","text":"<p>searchType currently supports the following types:</p> <ul> <li>dfs_query_then_fetch</li> <li>query_then_fetch</li> <li>count</li> <li>scan</li> </ul>"},{"location":"en/reader/excelreader/","title":"Excel Reader","text":"<p><code>Excel Reader</code> plugin implements the ability to read data from Microsoft Excel files.</p>"},{"location":"en/reader/excelreader/#configuration","title":"Configuration","text":""},{"location":"en/reader/excelreader/#get-sample-files","title":"Get Sample Files","text":"<p>Download the Excel compressed file for demonstration from here and extract it to the <code>/tmp/in</code> directory. The three folders have the same content, where:</p> <ul> <li><code>demo.xlsx</code> is the new Excel format</li> <li><code>demo.xls</code> is the old Excel format</li> <li><code>demo_gbk.xlsx</code> is created under Windows and stored with GBK encoding</li> </ul> <p>File content is shown in the following table:</p> No. Integer Type Float Type String Type Date Type Formula Calculation Cell Formatting 1 11 1102.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/10 5544.17 \u00a51,102.23 2 12 1103.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/11 5552.17 \u00a51,103.23 3 13 1104.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/12 5560.17 \u00a51,104.23 4 14 1105.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/13 5568.17 \u00a51,105.23 5 15 1106.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/14 5576.17 \u00a51,106.23 6 16 1107.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/15 5584.17 \u00a51,107.23 7 17 1108.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/16 5592.17 \u00a51,108.23 8 18 1109.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/17 5600.17 \u00a51,109.23 9 19 1110.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/18 5608.17 \u00a51,110.23 10 20 1111.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/19 5616.17 \u00a51,111.23 11 21 1112.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/20 5624.17 \u00a51,112.23 12 22 1113.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/21 5632.17 \u00a51,113.23 13 23 1114.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/22 5640.17 \u00a51,114.23 14 24 1115.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/23 5648.17 \u00a51,115.23 15 25 1116.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/24 5656.17 \u00a51,116.23 16 26 1117.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/25 5664.17 \u00a51,117.23 17 27 1118.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/26 5672.17 \u00a51,118.23 18 28 1119.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/27 5680.17 \u00a51,119.23 19 29 1120.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/28 5688.17 \u00a51,120.23 20 30 1121.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/29 5696.17 \u00a51,121.23 <p>The table headers roughly describe the characteristics of cell data.</p>"},{"location":"en/reader/excelreader/#create-job-file","title":"Create Job File","text":"<p>Create the following JSON file:</p> excel2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"excelreader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/in\"\n          ],\n          \"header\": true,\n          \"skipRows\": 0\n        }\n      },\n      \"writer\": {\n        \"parameter\": {\n          \"print\": true\n        },\n        \"name\": \"streamwriter\"\n      }\n    }\n  }\n}\n</code></pre> <p>Save the output content to the <code>job/excel2stream.json</code> file and execute the collection command:</p> <pre><code>$ bin/addax.sh job/excel2stream.json\n</code></pre> <p>If there are no errors, you should get the following output:</p> Click to expand <pre><code> ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3)\n\n2021-09-09 14:43:42.579 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-09 14:43:42.621 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"path\":[\n                        \"/tmp/in\"\n                    ],\n                    \"column\":[\n                        {\n                            \"name\":\"no\",\n                            \"type\":\"long\"\n                        },\n                        {\n                            \"name\":\"birth\",\n                            \"format\":\"yyyy-MM-dd HH:mm:ss\",\n                            \"type\":\"date\"\n                        },\n                        {\n                            \"name\":\"kk\",\n                            \"type\":\"string\"\n                        }\n                    ],\n                    \"header\":true,\n                    \"skipHeader\":true,\n                    \"encoding\":\"UTF-8\",\n                    \"fieldDelimiter\":\",\"\n                },\n                \"name\":\"excelreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":2\n        }\n    }\n}\n\n2021-09-09 14:43:42.653 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-09 14:43:42.653 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-09 14:43:42.655 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_old.xls] as a candidate to be read.\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_gbk.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - The number of files to read is: [3]\n2021-09-09 14:43:42.677 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do prepare work .\n2021-09-09 14:43:42.678 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-09-09 14:43:42.679 [       job-0] INFO  JobContainer         - Job set Channel-Number to 2 channels.\n2021-09-09 14:43:42.681 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] splits to [3] tasks.\n2021-09-09 14:43:42.682 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [3] tasks.\n2021-09-09 14:43:42.727 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-09 14:43:42.736 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [2] channels for [3] tasks.\n2021-09-09 14:43:42.741 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-09 14:43:42.742 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo.xlsx\n2021-09-09 14:43:42.757 [0-0-0-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.758 [0-0-0-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_gbk.xlsx\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n1   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n1   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n1   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n1   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n1   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n1   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n1   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n1   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n1   20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n1   21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n1   22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n1   23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n1   24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n1   25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n1   26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n1   27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n1   28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n1   29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n1   30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_old.xls\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:45.753 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-09 14:43:45.754 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-09-09 14:43:45.756 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do post work.\n2021-09-09 14:43:45.761 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-09 14:43:45.762 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 60 records, 3360 bytes | Speed 1.09KB/s, 20 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.993s | Percentage 100.00%\n2021-09-09 14:43:45.764 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-09 14:43:42\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-09 14:43:45\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            1.09KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             20rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  60\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/excelreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description path Yes string/list None Specify the folder to read, multiple can be specified header No boolean false Whether the file contains headers skipRows No int 0 How many rows to skip at the beginning"},{"location":"en/reader/excelreader/#header","title":"header","text":"<p>Whether the Excel file contains headers, if so, skip them.</p>"},{"location":"en/reader/excelreader/#skiprows","title":"skipRows","text":"<p>Specify the number of rows to skip, default is 0, meaning no skipping. Note that if <code>header</code> is set to true and <code>skipRows</code> is set to 2, it means the first three rows are all skipped. If <code>header</code> is false, it means skipping the first two rows.</p>"},{"location":"en/reader/excelreader/#supported-data-types","title":"Supported Data Types","text":"<p>The Excel reading functionality implementation depends on the Apache POI project, which has a very broad definition of cell data types. It only defines three types: Boolean, Double (numeric), and String. Among them, numeric type includes all integers, decimals, and dates. Currently, simple distinction is made for numeric types:</p> <ol> <li>Use library utility classes to detect if it's a date type, if so, determine it as date type</li> <li>Convert the numeric value to long integer and compare with the original value, if equal, determine as Long type</li> <li>Otherwise determine as Double type</li> </ol>"},{"location":"en/reader/excelreader/#limitations","title":"Limitations","text":"<ol> <li>Currently only reads the first Sheet of the file and ignores other Sheets</li> <li>Does not support specifying column reading</li> <li>Does not support skipping trailing rows (for example, summary tail rows may not meet requirements)</li> <li>Does not check if the number of columns in each row is equal, Excel must ensure this</li> <li>Only reads files with <code>xlsx</code> or <code>xls</code> file extensions in the specified directory, other extension files will be ignored with warning messages</li> </ol>"},{"location":"en/reader/ftpreader/","title":"FTP Reader","text":"<p>FTP Reader provides the ability to read data storage from remote FTP/SFTP file systems.</p>"},{"location":"en/reader/ftpreader/#functionality","title":"Functionality","text":""},{"location":"en/reader/ftpreader/#configuration-example","title":"Configuration Example","text":"job/ftp2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"ftpreader\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"127.0.0.1\",\n          \"port\": 22,\n          \"username\": \"xx\",\n          \"password\": \"xxx\",\n          \"path\": [\n            \"/var/ftp/test.txt\",\n            \"/var/tmp/*.txt\",\n            \"/public/ftp\",\n            \"/public/a??.txt\"\n          ],\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"ftpWriter\",\n        \"parameter\": {\n          \"path\": \"/var/ftp/FtpWriter/result\",\n          \"fileName\": \"shihf\",\n          \"writeMode\": \"truncate\",\n          \"format\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/ftpreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description protocol Yes string None Server protocol, currently supports <code>ftp</code> and <code>sftp</code> transport protocols host Yes string None Server address port No int 22/21 If transport protocol is <code>sftp</code>, default is 22; if standard ftp protocol, default is 21 timeout No int 60000 Connection timeout for ftp server, in milliseconds (ms) connectPattern No string PASV Connection mode, only supports <code>PORT</code>, <code>PASV</code> modes. This parameter is only used for ftp protocol username Yes string None FTP server access username password No string None FTP server access password useKey No boolean false Whether to use private key login, only valid for sftp login keyPath No string <code>~/.ssh/id_rsa</code> Private key address keyPass No string None Private key password, if no private key password is set, no need to configure this path Yes list None Remote FTP file system path information, note that multiple paths can be supported, detailed description below column Yes <code>list&lt;map&gt;</code> None List of fields to read, type specifies the type of source data, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, default empty means no compression. Supports <code>zip</code>, <code>gz</code>, <code>bzip2</code> encoding No string <code>utf-8</code> File encoding configuration for reading skipHeader No boolean false CSV format files may have header titles that need to be skipped. Default is not to skip nullFormat No char <code>\\N</code> Define which strings can represent null maxTraversalLevel No int 100 Maximum number of folder levels allowed for traversal csvReaderConfig No map None CSV file reading parameter configuration, Map type. Default values used if not configured, see below"},{"location":"en/reader/hanareader/","title":"HANA Reader","text":"<p>HANA Reader plugin implements the ability to read data from SAP HANA.</p>"},{"location":"en/reader/hanareader/#example","title":"Example","text":"<p>The following configuration reads this table to terminal:</p> job/hanareader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hanareader\",\n          \"parameter\": {\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": {\n              \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n              \"table\": [\n                \"addax_tbl\"\n              ]\n            },\n            \"username\": \"system\",\n            \"password\": \"HXEHana1\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hana2stream.json</code></p>"},{"location":"en/reader/hanareader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"en/reader/hanareader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"en/reader/hbase11xreader/","title":"HBase11X Reader","text":"<p>HBase11X Reader plugin supports reading data from HBase 1.x version. Its implementation method is to connect to remote HBase service through HBase's Java client and read data within the <code>rowkey</code> range you specify through Scan method.</p>"},{"location":"en/reader/hbase11xreader/#configuration","title":"Configuration","text":""},{"location":"en/reader/hbase11xreader/#table-creation-and-data-population","title":"Table Creation and Data Population","text":"<p>The following demonstration is based on the table and data created below:</p> <pre><code>create 'users', 'address','info'\nput 'users', 'lisi', 'address:country', 'china'\nput 'users', 'lisi', 'address:province',    'beijing'\nput 'users', 'lisi', 'info:age',        27\nput 'users', 'lisi', 'info:birthday',   '1987-06-17'\nput 'users', 'lisi', 'info:company',    'baidu'\nput 'users', 'xiaoming', 'address:city',    'hangzhou'\nput 'users', 'xiaoming', 'address:country', 'china'\nput 'users', 'xiaoming', 'address:province',    'zhejiang'\nput 'users', 'xiaoming', 'info:age',        29\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17'\nput 'users', 'xiaoming', 'info:company',    'alibaba'\n</code></pre>"},{"location":"en/reader/hbase11xreader/#normal-mode","title":"normal Mode","text":"<p>Read HBase table as a normal two-dimensional table (horizontal table), reading the latest version data. For example:</p> <pre><code>hbase(main):017:0&gt; scan 'users'\nROW           COLUMN+CELL\n lisi         column=address:city, timestamp=1457101972764, value=beijing\n lisi         column=address:country, timestamp=1457102773908, value=china\n lisi         column=address:province, timestamp=1457101972736, value=beijing\n lisi         column=info:age, timestamp=1457101972548, value=27\n lisi         column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi         column=info:company, timestamp=1457101972653, value=baidu\n xiaoming     column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming     column=address:country, timestamp=1457082195729, value=china\n</code></pre> <p>For detailed configuration and parameters, please refer to the original HBase11X Reader documentation.</p>"},{"location":"en/reader/hbase11xsqlreader/","title":"HBase11x SQL Reader","text":"<p>HBase11x SQL Reader plugin implements reading data from Phoenix(HBase SQL), supporting HBase version 1.x.</p>"},{"location":"en/reader/hbase11xsqlreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Phoenix to local:</p> <pre><code>{\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"byte\":-1,\n              \"channel\": 1\n            }\n        },\n        \"content\": [ {\n                \"reader\": {\n                    \"name\": \"hbase11xsqlreader\",\n                    \"parameter\": {\n                        \"queryServerAddress\": \"http://127.0.0.1:8765\",\n                        \"serialization\": \"PROTOBUF\",\n                        \"table\": \"TEST\",\n                        \"column\": [\"ID\", \"NAME\"]\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"en/reader/hbase11xsqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/hbase20xreader/","title":"HBase20 Reader","text":"<p>HBase20 Reader plugin supports reading data from HBase 2.x version. Its implementation method is to connect to remote HBase service through HBase's Java client and read data within the <code>rowkey</code> range you specify through Scan method.</p>"},{"location":"en/reader/hbase20xreader/#configuration","title":"Configuration","text":"<p>The following demonstration is based on the table and data created below:</p> <pre><code>create 'users', {NAME=&gt;'address', VERSIONS=&gt;100},{NAME=&gt;'info',VERSIONS=&gt;1000}\nput 'users', 'lisi', 'address:country', 'china1', 20200101\nput 'users', 'lisi', 'address:province',    'beijing1', 20200101\nput 'users', 'lisi', 'info:age',        27, 20200101\nput 'users', 'lisi', 'info:birthday',   '1987-06-17', 20200101\nput 'users', 'lisi', 'info:company',    'baidu1', 20200101\nput 'users', 'xiaoming', 'address:city',    'hangzhou1', 20200101\nput 'users', 'xiaoming', 'address:country', 'china1', 20200101\nput 'users', 'xiaoming', 'address:province',    'zhejiang1',20200101\nput 'users', 'xiaoming', 'info:age',        29, 20200101\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17',20200101\n</code></pre> <p>For detailed configuration and parameters, please refer to the original HBase20 Reader documentation.</p>"},{"location":"en/reader/hbase20xsqlreader/","title":"HBase20 SQL Reader","text":"<p>HBase20 SQL Reader plugin implements reading data from Phoenix(HBase SQL), corresponding to HBase2.X and Phoenix5.X versions.</p>"},{"location":"en/reader/hbase20xsqlreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Phoenix to local:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hbase20xsqlreader\",\n          \"parameter\": {\n            \"queryServerAddress\": \"http://127.0.0.1:8765\",\n            \"serialization\": \"PROTOBUF\",\n            \"table\": \"TEST\",\n            \"column\": [\"ID\", \"NAME\"]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/reader/hbase20xsqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/hdfsreader/","title":"HDFS Reader","text":"<p>HDFS Reader provides the ability to read data storage from distributed file system Hadoop HDFS.</p> <p>Currently HdfsReader supports the following file formats:</p> <ul> <li>textfile\uff08text\uff09</li> <li>orcfile\uff08orc\uff09</li> <li>rcfile\uff08rc\uff09</li> <li>sequence file\uff08seq\uff09</li> <li>Csv(csv)</li> <li>parquet</li> </ul>"},{"location":"en/reader/hdfsreader/#features-and-limitations","title":"Features and Limitations","text":"<ol> <li>Supports textfile, orcfile, parquet, rcfile, sequence file and csv format files, and requires that the file content stores a logically two-dimensional table.</li> <li>Supports reading multiple types of data (represented using String), supports column pruning, supports column constants</li> <li>Supports recursive reading, supports regular expressions (<code>*</code> and <code>?</code>).</li> <li>Supports common compression algorithms, including GZIP, SNAPPY, ZLIB, etc.</li> <li>Multiple Files can support concurrent reading.</li> <li>Supports sequence file data compression, currently supports lzo compression method.</li> <li>csv type supports compression formats: gzip, bz2, zip, lzo, lzo_deflate, snappy.</li> <li>Currently the Hive version in the plugin is <code>3.1.1</code>, Hadoop version is <code>3.1.1</code>, writes normally in Hadoop <code>2.7.x</code>, Hadoop <code>3.1.x</code> and Hive <code>2.x</code>, hive <code>3.1.x</code> test environments; other versions are theoretically supported, but please test further before using in production environments;</li> <li>Supports <code>kerberos</code> authentication</li> </ol>"},{"location":"en/reader/hdfsreader/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hdfsreader\",\n        \"parameter\": {\n          \"path\": \"/user/hive/warehouse/mytable01/*\",\n          \"defaultFS\": \"hdfs://xxx:port\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"hello\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            }\n          ],\n          \"fileType\": \"orc\",\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/hdfsreader/#configuration-parameters","title":"Configuration Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File path to read defaultFS Yes string None HDFS <code>NAMENODE</code> node address, if HA mode is configured, it is the value of <code>defaultFS</code> fileType Yes string None File type column Yes <code>list&lt;map&gt;</code> None List of fields to read fieldDelimiter No char <code>,</code> Specify text file field delimiter, binary files do not need to specify this encoding No string <code>utf-8</code> File encoding configuration, currently only supports <code>utf-8</code> nullFormat No string None Characters that can represent null, if user configures: <code>\"\\\\N\"</code>, then if source data is <code>\"\\N\"</code>, it's treated as <code>null</code> field haveKerberos No boolean None Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Kerberos authentication credential file path, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Kerberos authentication credential principal, e.g. <code>addax/node1@WGZHAO.COM</code> compress No string None Specify compression format of files to read hadoopConfig No map None Can configure some advanced parameters related to Hadoop, such as HA configuration hdfsSitePath No string None Path to <code>hdfs-site.xml</code>, detailed explanation below"},{"location":"en/reader/hivereader/","title":"Hive Reader","text":"<p>Hive Reader plugin implements the ability to read data from Apache Hive database.</p> <p>The main purpose of adding this plugin is to solve the problem of Kerberos authentication when using RDBMS Reader plugin to read Hive database. If your Hive database does not have Kerberos authentication enabled, you can directly use RDBMS Reader. If Kerberos authentication is enabled, you can use this plugin.</p>"},{"location":"en/reader/hivereader/#example","title":"Example","text":"<p>We create the following table in Hive's test database and insert a record:</p> <pre><code>create table default.hive_reader\n(\n    col1 int,\n    col2 string,\n    col3 timestamp\n)\nstored as orc;\n\n\ninsert into hive_reader values(1, 'hello', current_timestamp()), (2, 'world', current_timestamp());\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/hive2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hivereader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:hive2://localhost:10000/default;principal=hive/_HOST@EXAMPLE.COM\",\n            \"table\": [\n              \"hive_reader\"\n            ]\n          },\n          \"where\": \"logdate='20211013'\",\n          \"haveKerberos\": true,\n          \"kerberosKeytabFilePath\": \"/etc/security/keytabs/hive.headless.keytab\",\n          \"kerberosPrincipal\": \"hive@EXAMPLE.COM\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hive2stream.json</code></p>"},{"location":"en/reader/hivereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hive2stream.json\n</code></pre>"},{"location":"en/reader/hivereader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description jdbcUrl Yes list None JDBC connection information of target database driver No string None Custom driver class name to solve compatibility issues, see description below username Yes string None Username of data source password No string None Password for specified username of data source, can be omitted if no password"},{"location":"en/reader/httpreader/","title":"HTTP Reader","text":"<p>HTTP Reader plugin implements the ability to read Restful API data.</p>"},{"location":"en/reader/httpreader/#example","title":"Example","text":""},{"location":"en/reader/httpreader/#sample-interface-and-data","title":"Sample Interface and Data","text":"<p>The following configuration demonstrates how to get data from a specified API, assuming the accessed interface is:</p> <p>http://127.0.0.1:9090/mock/17/LDJSC/ASSET</p> <p>The interface accepts GET requests with the following parameters:</p> Parameter Name Example Value CURR_DATE 2021-01-17 DEPT 9400 USERNAME andi <p>The following is a sample of accessed data (actual returned data may vary slightly):</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": \"9700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1581.03,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 36.75,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.009448781026677719,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015153586011995693,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": 0.0652347643813081,\n      \"TMMARKET_VALUE_SHARECOM\": 0.024853621341525287,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.005242133578517903,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1645.1193961136973,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.16690149257388515,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.017886267801303465,\n      \"POTENTIAL_LOST_ASSETS\": 56.76,\n      \"TOTAL_LIABILITIES\": 57.81,\n      \"TOTAL_ASSETS\": 1306.33,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 4.79,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.006797058194980485,\n      \"NEW_ASSETS_DAY\": 14.92,\n      \"NEW_ASSETS_MON\": 90.29,\n      \"NEW_ASSETS_YEAR\": 297.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.04015576541561927,\n      \"NEW_FUNDS_DAY\": 18.16,\n      \"INFLOW_FUNDS_DAY\": 2.12,\n      \"OUTFLOW_FUNDS_DAY\": 9.73,\n      \"OVERALL_POSITION\": 0.810298404938773,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.03521615634095476,\n      \"NEW_CUST_FUNDS_MON\": 69.44,\n      \"INFLOW_FUNDS_MONTH\": 62.26,\n      \"OUTFLOW_FUNDS_MONTH\": 32.59\n    },\n    {\n      \"CURR_DATE\": \"2019-08-30\",\n      \"DEPT\": \"8700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1596.74,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 41.86,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": 0.03470208565515685,\n      \"TMMARKET_VALUE_GROWTH_MON\": 0.07818120801111743,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.05440250244736409,\n      \"TMMARKET_VALUE_SHARECOM\": 0.09997733019626448,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.019726478499825697,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1007.9314679742108,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.15123738798885086,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.04694052069678048,\n      \"POTENTIAL_LOST_ASSETS\": 52.48,\n      \"TOTAL_LIABILITIES\": 55.28,\n      \"TOTAL_ASSETS\": 1366.72,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 10.12,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.009708491982487952,\n      \"NEW_ASSETS_DAY\": 12.42,\n      \"NEW_ASSETS_MON\": 41.14,\n      \"NEW_ASSETS_YEAR\": 279.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.025878627161898062,\n      \"NEW_FUNDS_DAY\": 3.65,\n      \"INFLOW_FUNDS_DAY\": 14.15,\n      \"OUTFLOW_FUNDS_DAY\": 17.08,\n      \"OVERALL_POSITION\": 0.9098432997243932,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02111922282868306,\n      \"NEW_CUST_FUNDS_MON\": 57.21,\n      \"INFLOW_FUNDS_MONTH\": 61.16,\n      \"OUTFLOW_FUNDS_MONTH\": 15.83\n    },\n    {\n      \"CURR_DATE\": \"2019-06-30\",\n      \"DEPT\": \"6501\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1506.72,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": -13.23,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.0024973354204176554,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015530793150701896,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.08556724628979398,\n      \"TMMARKET_VALUE_SHARECOM\": 0.15000077963967678,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.049629446804825755,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1250.1040863177336,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19098445630488178,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": -0.007881179708853471,\n      \"POTENTIAL_LOST_ASSETS\": 50.53,\n      \"TOTAL_LIABILITIES\": 56.62,\n      \"TOTAL_ASSETS\": 1499.53,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 29.56,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.02599813232345556,\n      \"NEW_ASSETS_DAY\": 28.81,\n      \"NEW_ASSETS_MON\": 123.24,\n      \"NEW_ASSETS_YEAR\": 263.63,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.0073986669331394875,\n      \"NEW_FUNDS_DAY\": 18.52,\n      \"INFLOW_FUNDS_DAY\": 3.26,\n      \"OUTFLOW_FUNDS_DAY\": 6.92,\n      \"OVERALL_POSITION\": 0.8713692113306709,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02977644553289545,\n      \"NEW_CUST_FUNDS_MON\": 85.14,\n      \"INFLOW_FUNDS_MONTH\": 23.35,\n      \"OUTFLOW_FUNDS_MONTH\": 92.95\n    },\n    {\n      \"CURR_DATE\": \"2019-12-07\",\n      \"DEPT\": \"8705\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1575.85,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 8.94,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.04384846980627058,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.022962456288549656,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.005047009316021089,\n      \"TMMARKET_VALUE_SHARECOM\": 0.07819484815809447,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.008534369960890256,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1340.0339240689955,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19019952857677042,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.01272353909992914,\n      \"POTENTIAL_LOST_ASSETS\": 54.63,\n      \"TOTAL_LIABILITIES\": 53.17,\n      \"TOTAL_ASSETS\": 1315.08,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 49.31,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.0016538407028265922,\n      \"NEW_ASSETS_DAY\": 29.17,\n      \"NEW_ASSETS_MON\": 44.75,\n      \"NEW_ASSETS_YEAR\": 172.87,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.045388692595736746,\n      \"NEW_FUNDS_DAY\": 18.46,\n      \"INFLOW_FUNDS_DAY\": 12.93,\n      \"OUTFLOW_FUNDS_DAY\": 10.38,\n      \"OVERALL_POSITION\": 0.8083127036694828,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.02847453515632541,\n      \"NEW_CUST_FUNDS_MON\": 49.74,\n      \"INFLOW_FUNDS_MONTH\": 81.93,\n      \"OUTFLOW_FUNDS_MONTH\": 18.17\n    }\n  ]\n}\n</code></pre> <p>We need to get partial key value data from the <code>result</code> results.</p>"},{"location":"en/reader/httpreader/#configuration","title":"Configuration","text":"<p>The following configuration implements getting data from the interface and printing to terminal</p> job/httpreader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"httpreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"url\": \"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\",\n              \"proxy\": {\n                \"host\": \"http://127.0.0.1:3128\",\n                \"auth\": \"user:pass\"\n              }\n            }\n          ],\n          \"reqParams\": {\n            \"CURR_DATE\": \"2021-01-18\",\n            \"DEPT\": \"9700\"\n          },\n          \"resultKey\": \"result\",\n          \"method\": \"GET\",\n          \"column\": [\n            \"CURR_DATE\",\n            \"DEPT\",\n            \"TOTAL_MANAGED_MARKET_VALUE\",\n            \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n          ],\n          \"username\": \"user\",\n          \"password\": \"passw0rd\",\n          \"headers\": {\n            \"X-Powered-by\": \"Addax\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/httpreader2stream.json</code> file.</p>"},{"location":"en/reader/httpreader/#execution","title":"Execution","text":"<p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/httpreader2stream.json\n</code></pre> <p>The output of the above command is roughly as follows:</p> <pre><code>2021-01-20 09:07:41.864 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-01-20 09:07:41.877 [main] INFO  Engine - the machine info  =&gt;\n\n    osInfo:     Mac OS X x86_64 10.15.1\n    jvmInfo:    AdoptOpenJDK 14 14.0.2+12\n    cpu num:    8\n\n    totalPhysicalMemory:    -0.00G\n    freePhysicalMemory: -0.00G\n    maxFileDescriptorCount: -1\n    currentOpenFileDescriptorCount: -1\n\n    GC Names    [G1 Young Generation, G1 Old Generation]\n\n    MEMORY_NAME                    | allocation_size                | init_size\n    CodeHeap 'profiled nmethods'   | 117.21MB                       | 2.44MB\n    G1 Old Gen                     | 2,048.00MB                     | 39.00MB\n    G1 Survivor Space              | -0.00MB                        | 0.00MB\n    CodeHeap 'non-profiled nmethods' | 117.21MB                       | 2.44MB\n    Compressed Class Space         | 1,024.00MB                     | 0.00MB\n    Metaspace                      | -0.00MB                        | 0.00MB\n    G1 Eden Space                  | -0.00MB                        | 25.00MB\n    CodeHeap 'non-nmethods'        | 5.57MB                         | 2.44MB\n\n\n2021-01-20 09:07:41.903 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"reqParams\":{\n                        \"CURR_DATE\":\"2021-01-18\",\n                        \"DEPT\":\"9700\"\n                    },\n                    \"method\":\"GET\",\n                    \"column\":[\n                        \"CURR_DATE\",\n                        \"DEPT\",\n                        \"TOTAL_MANAGED_MARKET_VALUE\",\n                        \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n                    ],\n                    \"resultKey\":\"result\",\n                    \"connection\":[\n                        {\n                            \"url\":\"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\"\n                        }\n                    ]\n                },\n                \"name\":\"httpreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":\"true\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-01-20 09:07:41.926 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-01-20 09:07:41.927 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-01-20 09:07:41.928 [main] INFO  JobContainer - Set jobId = 0\n2021-01-20 09:07:42.002 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started\n\n2019-08-30  9700    1539.85 -14.78\n2019-10-01  9700    1531.71 47.66\n2020-12-03  9700    1574.38 7.34\n2020-11-31  9700    1528.13 41.62\n2019-03-01  9700    1554.28 -9.29\n\n2021-01-20 09:07:45.006 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-20 09:07:41\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-20 09:07:44\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               42B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              1rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   5\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/httpreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description url Yes string None HTTP address to access reqParams No map None Interface request parameters resultKey No string None Key value to get results, if getting entire return value, no need to fill method No string get Request mode, only supports GET and POST, case insensitive column Yes list None Keys to get, configure as <code>\"*\"</code> to get all key values username No string None Authentication account required for interface request (if any) password No string None Password required for interface request (if any) proxy No map None Proxy address, see description below headers No map None Custom request header information isPage No boolean None Whether interface supports pagination pageParams No map None Pagination parameters"},{"location":"en/reader/httpreader/#reqparams","title":"reqParams","text":"<p>reqParams are request parameters. If the request is <code>GET</code> method, it will be appended to the <code>url</code> in <code>k=v</code> format. If the request is <code>POST</code> mode, <code>reqParams</code> will be sent as JSON content in the request body. In particular, in <code>POST</code> mode, if your request body is not a <code>k-v</code> structure, you can set the <code>key</code> to empty string, like:</p> <pre><code>{\n  \"reqParams\": {\n    \"\": [123,3456]\n  }\n}\n</code></pre> <p>The program will handle this case specially.</p>"},{"location":"en/reader/httpreader/#proxy","title":"proxy","text":"<p>If the accessed interface needs to go through a proxy, you can configure the <code>proxy</code> configuration item, which is a json dictionary containing a required <code>host</code> field and an optional <code>auth</code> field.</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"http://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p>For <code>socks</code> proxy (V4, V5), you can write:</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"socks://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p><code>host</code> is the proxy address, including proxy type. Currently only supports <code>http</code> proxy and <code>socks</code> (both V4 and V5) proxy. If the proxy requires authentication, you can configure <code>auth</code>, which consists of username and password separated by colon (<code>:</code>).</p>"},{"location":"en/reader/httpreader/#column","title":"column","text":"<p>Besides directly specifying keys, <code>column</code> also allows using JSON Xpath style to specify key values to get. Suppose you want to read the following JSON file:</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": {\n        \"ID\": \"9700\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 1\n        },\n        {\n          \"COL2\": 2\n        }\n      ]\n    },\n    {\n      \"CURR_DATE\": \"2021-11-09\",\n      \"DEPT\": {\n        \"ID\": \"6500\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 3\n        },\n        {\n          \"COL2\": 4\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>If we want to read <code>CURR_DATE</code>, <code>ID</code>, <code>COL1</code>, <code>COL2</code> as four fields, your <code>column</code> can be configured like this:</p> <pre><code>{\n  \"column\": [\n    \"CURR_DATE\",\n    \"DEPT.ID\",\n    \"KK[0].COL1\",\n    \"KK[1].COL2\"\n  ]\n}\n</code></pre> <p>The execution result is as follows:</p> <pre><code>...\n2021-10-30 14:01:50.273 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n2019-12-09  9700    1   2\n2021-11-09  6500    3   4\n\n2021-10-30 14:01:53.283 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Reader.Job [httpreader] do post work.\n2021-10-30 14:01:53.286 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-10-30 14:01:53.289 [       job-0] INFO  JobContainer         -\nTask start time                    : 2021-10-30 14:01:50\nTask end time                      : 2021-10-30 14:01:53\nTask total duration                :                  3s\nTask average throughput            :               10B/s\nRecord write speed                 :              0rec/s\nTotal records read                 :                   2\nTotal read/write failures          :                   0\n</code></pre> <p>Note: If you specify a non-existent key, it returns NULL value directly.</p>"},{"location":"en/reader/httpreader/#ispage","title":"isPage","text":"<p>The <code>isPage</code> parameter is used to specify whether the interface supports pagination. It is a boolean value. If <code>true</code>, it means the interface supports pagination, otherwise it doesn't.</p> <p>When the interface supports pagination, it will automatically paginate reading until the number of records returned by the interface's last return is less than the number of records per page.</p>"},{"location":"en/reader/httpreader/#pageparams","title":"pageParams","text":"<p>The <code>pageParams</code> parameter only takes effect when the <code>isPage</code> parameter is <code>true</code>. It is a JSON dictionary containing two optional fields <code>pageIndex</code> and <code>pageSize</code>.</p> <p><code>pageIndex</code> is used to indicate the current page for pagination. It is a JSON field containing two optional fields <code>key</code> and <code>value</code>, where <code>key</code> specifies the parameter name for page number, and <code>value</code> specifies the current page number value.</p> <p><code>pageSize</code> is used to indicate the page size for pagination. It is a JSON field containing two optional fields <code>key</code> and <code>value</code>, where <code>key</code> specifies the parameter name for page size, and <code>value</code> specifies the page size value.</p> <p>The default values for these two parameters are:</p> <pre><code>{\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"pageIndex\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"pageSize\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>If your interface pagination parameters are not <code>pageIndex</code> and <code>pageSize</code>, you can specify them through the <code>pageParams</code> parameter. For example:</p> <pre><code>{\n  \"isPage\": true,\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"page\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"size\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>This means the pagination parameters passed to the interface are <code>page=1&amp;size=100</code>.</p>"},{"location":"en/reader/httpreader/#limitations","title":"Limitations","text":"<ol> <li>The returned result must be JSON type</li> <li>Currently all key values are treated as string type</li> <li>Interface Token authentication mode not yet supported</li> <li>Pagination retrieval not yet supported</li> <li>Proxy only supports <code>http</code> mode</li> </ol>"},{"location":"en/reader/influxdb2reader/","title":"InfluxDB2 Reader","text":"<p>InfluxDB2 Reader plugin implements reading data from InfluxDB version 2.0 and above.</p> <p>Note: If your InfluxDB is version 1.8 and below, you should use the InfluxDBReader plugin.</p>"},{"location":"en/reader/influxdb2reader/#example","title":"Example","text":"<p>The following example demonstrates how this plugin reads data from specified tables (i.e., metrics) and outputs to terminal.</p>"},{"location":"en/reader/influxdb2reader/#create-job-file","title":"Create Job File","text":"<p>Create <code>job/influx2stream.json</code> file with the following content:</p> job/influx2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdb2reader\",\n        \"parameter\": {\n          \"column\": [\n            \"location\",\n            \"height\",\n            \"wet\"\n          ],\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"bucket\": \"test\",\n            \"table\": [\n              \"temperature\"\n            ],\n            \"org\": \"com.wgzhao\"\n          },\n          \"token\": \"YOUR_SECURE_TOKEN\",\n          \"range\": [\n            \"-1h\",\n            \"-5m\"\n          ],\n          \"limit\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/influxdb2reader/#run","title":"Run","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/influx2stream.json\n</code></pre>"},{"location":"en/reader/influxdb2reader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None InfluxDB connection string token Yes string None Token for accessing database table No list None Selected table names (i.e., metrics) to be synchronized org Yes string None Specify InfluxDB org name bucket Yes string None Specify InfluxDB bucket name column No list None Collection of column names to be synchronized in configured table, detailed description see rdbmreader range Yes list None Time range for reading data limit No int None Limit number of records to get"},{"location":"en/reader/influxdb2reader/#column","title":"column","text":"<p>If <code>column</code> is not specified, or <code>column</code> is specified as <code>[\"*\"]</code>, all valid <code>_field</code> fields and <code>_time</code> field will be read. Otherwise, read according to specified fields.</p>"},{"location":"en/reader/influxdb2reader/#range","title":"range","text":"<p><code>range</code> is used to specify the time range for reading metrics, with the following format:</p> <pre><code>{\n  \"range\": [\"start_time\", \"end_time\"]\n}\n</code></pre> <p><code>range</code> consists of a list of two strings, the first string represents start time, the second represents end time. The time expression format must comply with Flux format requirements, like this:</p> <pre><code>{\n  \"range\": [\"-15h\", \"-2h\"]\n}\n</code></pre> <p>If you don't want to specify the second end time, you can omit it, like this:</p> <pre><code>{\n  \"range\": [\"-15h\"]\n}\n</code></pre>"},{"location":"en/reader/influxdb2reader/#type-conversion","title":"Type Conversion","text":"<p>Current implementation treats all fields as strings.</p>"},{"location":"en/reader/influxdb2reader/#limitations","title":"Limitations","text":"<ol> <li>Current plugin only supports version 2.0 and above</li> </ol>"},{"location":"en/reader/influxdbreader/","title":"InfluxDB Reader","text":"<p>InfluxDBReader plugin implements reading data from InfluxDB. The underlying implementation calls InfluxQL language to query tables and get returned data.</p>"},{"location":"en/reader/influxdbreader/#example","title":"Example","text":"<p>The following example demonstrates how this plugin reads data from specified tables (i.e., metrics) and outputs to terminal</p>"},{"location":"en/reader/influxdbreader/#create-required-database-tables-and-data","title":"Create Required Database Tables and Data","text":"<p>Use the following commands to create tables and data to be read:</p> <pre><code># create database\ninflux --execute \"CREATE DATABASE NOAA_water_database\"\n# download sample data\ncurl https://s3.amazonaws.com/noaa.water-database/NOAA_data.txt -o NOAA_data.txt\n# import data via influx-cli\ninflux -import -path=NOAA_data.txt -precision=s -database=NOAA_water_database\n</code></pre>"},{"location":"en/reader/influxdbreader/#create-job-file","title":"Create Job File","text":"<p>Create <code>job/influxdb2stream.json</code> file with the following content:</p> job/influxdb2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdbreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"where\": \"1=1\",\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"NOAA_water_database\",\n            \"table\": \"h2o_feet\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/influxdbreader/#run","title":"Run","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/influxdb2stream.json\n</code></pre>"},{"location":"en/reader/influxdbreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None InfluxDB connection string username Yes string None Username of data source password No string None Password for specified username of data source database Yes string None Database specified by data source table Yes string None Selected table name to be synchronized column Yes list None Collection of column names to be synchronized in configured table, detailed description see rdbmreader connTimeout No int 15 Set connection timeout value, in seconds readTimeout No int 20 Set read timeout value, in seconds writeTimeout No int 20 Set write timeout value, in seconds where No string None Filtering conditions for the table querySql No string None Use SQL query to get data, if configured, <code>table</code> and <code>column</code> configuration items are invalid"},{"location":"en/reader/influxdbreader/#type-conversion","title":"Type Conversion","text":"<p>Current implementation treats all fields as strings.</p>"},{"location":"en/reader/influxdbreader/#limitations","title":"Limitations","text":"<ol> <li>Current plugin only supports version 1.x, version 2.0 and above are not supported</li> </ol>"},{"location":"en/reader/jsonfilereader/","title":"JSON File Reader","text":"<p>JSON File Reader provides the ability to read data from local file system storage.</p>"},{"location":"en/reader/jsonfilereader/#configuration-example","title":"Configuration Example","text":"job/json2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      },\n      \"reader\": {\n        \"name\": \"jsonfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/test*.json\"\n          ],\n          \"column\": [\n            {\n              \"index\": \"$.id\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.name\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": \"$.age\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.score.math\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.score.english\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.pubdate\",\n              \"type\": \"date\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"constant string\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Where <code>/tmp/test*.json</code> are multiple copies of the same JSON file, with content as follows:</p> <pre><code>{\"name\": \"zhangshan\",\"id\": 19890604,\"age\": 12,\"score\": {\"math\": 92.5,\"english\": 97.5,\"chinese\": 95},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"lisi\",\"id\": 19890605,\"age\": 12,\"score\": {\"math\": 90.5,\"english\": 77.5,\"chinese\": 90},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"wangwu\",\"id\": 19890606,\"age\": 12,\"score\": {\"math\": 89,\"english\": 100,\"chinese\": 92},\"pubdate\": \"2020-09-05\"}\n</code></pre>"},{"location":"en/reader/jsonfilereader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes list None Local file system path information, note that multiple paths can be supported column Yes list None List of fields to read, type specifies the type of source data fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, default empty means no compression. Supports zip, gzip, bzip2 encoding No string utf-8 Encoding configuration for reading files singleLine No boolean true Whether each data record is on one line"},{"location":"en/reader/jsonfilereader/#path","title":"path","text":"<p>Local file system path information, note that multiple paths can be supported, for example:</p> <pre><code>{\n  \"path\": [\n    \"/var/ftp/test.json\", // Read test.json file in /var/ftp directory\n    \"/var/tmp/*.json\", // Read all json files in /var/tmp directory\n    \"/public/ftp\", // Read all files in /public/ftp directory, if ftp is a file, read directly\n    \"/public/a??.json\" // Read all files in /public directory starting with 'a', followed by two characters, ending with json\n  ]\n}\n</code></pre> <p>It is particularly important to note that if there are no matching files for extraction under the path specified by Path, Addax will report an error.</p>"},{"location":"en/reader/jsonfilereader/#column","title":"column","text":"<p>List of fields to read, type specifies the type of source data, index specifies the current column from json specification using Jayway JsonPath syntax, value specifies that the current type is constant, not reading data from source file, but automatically generating corresponding columns based on value. Users must specify Column field information.</p> <p>For user-specified Column information, type must be filled, and index/value must choose one.</p>"},{"location":"en/reader/jsonfilereader/#singleline","title":"singleLine","text":"<p>There are two ways to store data in JSON format in the industry: one is one JSON object per line, which is <code>Single Line JSON (aka. JSONL or JSON Lines)</code>; the other is that the entire file is a JSON array, and each element is a JSON object, which is <code>Multiline JSON</code>.</p> <p>Addax supports one JSON object per line by default, i.e., <code>singleLine = true</code>. In this case, note that:</p> <ol> <li>There should be no comma at the end of each line's JSON object, otherwise parsing will fail.</li> <li>A JSON object cannot span multiple lines, otherwise parsing will fail.</li> </ol> <p>If the data is an entire file as a JSON array with each element being a JSON object, you need to set <code>singleLine</code> to <code>false</code>. Suppose the data in the above example is represented in the following format:</p> <pre><code>{\n  \"result\": [\n    {\n      \"name\": \"zhangshan\",\n      \"id\": 19890604,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 92.5,\n        \"english\": 97.5,\n        \"chinese\": 95\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"lisi\",\n      \"id\": 19890605,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 90.5,\n        \"english\": 77.5,\n        \"chinese\": 90\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"wangwu\",\n      \"id\": 19890606,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 89,\n        \"english\": 100,\n        \"chinese\": 92\n      },\n      \"pubdate\": \"2020-09-05\"\n    }\n  ]\n}\n</code></pre> <p>Because this format is valid JSON format, each JSON object can span multiple lines. Correspondingly, when reading such data, its <code>path</code> configuration should be filled as follows:</p> <pre><code>{\n  \"singleLine\": false,\n  \"column\": [\n    {\n      \"index\": \"$.result[*].id\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].name\",\n      \"type\": \"string\"\n    },\n    {\n      \"index\": \"$.result[*].age\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].score.math\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$.result[*].score.english\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$..result[*].pubdate\",\n      \"type\": \"date\"\n    },\n    {\n      \"type\": \"string\",\n      \"value\": \"constant string\"\n    }\n  ]\n}\n</code></pre> <p>For more detailed usage instructions, please refer to Jayway JsonPath syntax.</p> <p>Note: When this type of data is in a JSON array, the program can only read the entire file into memory and then parse it, so it is not suitable for reading large files. For reading large files, it is recommended to use the format of one JSON object per line, which is the <code>Single Line JSON</code> format. This format can be read line by line without taking up too much memory.</p>"},{"location":"en/reader/jsonfilereader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Local File Data Type Long Long Double Double String String Boolean Boolean Date Date"},{"location":"en/reader/kafkareader/","title":"Kafka Reader","text":"<p>Kafka Reader plugin implements the functionality of reading JSON format messages from Kafka queues. This plugin was introduced in version <code>4.0.10</code>.</p>"},{"location":"en/reader/kafkareader/#example","title":"Example","text":"<p>The following configuration demonstrates how to read from specified topics in Kafka and output to terminal.</p>"},{"location":"en/reader/kafkareader/#create-task-file","title":"Create Task File","text":"<p>First create a task file <code>kafka2stream.json</code>, with the following content:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        },\n        \"reader\": {\n          \"name\": \"kafkareader\",\n          \"parameter\": {\n            \"brokerList\": \"wgzhao-laptop:9092\",\n            \"topic\": \"test-1\",\n            \"column\": [\n              \"col1\",\n              \"col3\",\n              \"col0\",\n              \"col9\"\n            ],\n            \"missingKeyValue\": \"\\\\N\",\n            \"properties\": {\n              \"auto.offset.reset\": \"earliest\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/reader/kafkareader/#run","title":"Run","text":"<p>Execute the <code>bin/addax.sh kafka2stream.json</code> command.</p>"},{"location":"en/reader/kafkareader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description brokerList Yes string None Broker configuration for connecting to kafka service, like <code>localhost:9092</code>, multiple brokers separated by commas (<code>,</code>) topic Yes string None Topic to write to column Yes list None Collection of column names to be synchronized in the configured table, detailed below missingKeyValue No string None What value to fill when field does not exist, detailed below properties No map None Other kafka connection parameters to be set"},{"location":"en/reader/kafkareader/#column","title":"column","text":"<p><code>column</code> is used to specify the keys to read from JSON messages. If set to <code>*</code>, it means reading all keys in the message. Note that in this case, the output will not be sorted, meaning the output order of keys for each record is not guaranteed to be consistent.</p> <p>You can also specify keys to read, for example:</p> <pre><code>{\n  \"column\": [\"col1\", \"col2\", \"col3\"]\n}\n</code></pre> <p>This way, the plugin will try to read the corresponding keys in the given order. If a key to be read does not exist in a message, the plugin will report an error and exit. If you want it not to exit, you can set <code>missingKeyValue</code>, which represents the value to fill when the key to be read does not exist.</p> <p>Additionally, the plugin will automatically guess the type of the key value being read. If the type cannot be guessed, it will be treated as String type.</p>"},{"location":"en/reader/kafkareader/#limitations","title":"Limitations","text":"<ol> <li>Only supports Kafka <code>1.0</code> and above versions, versions below this cannot be guaranteed to work</li> <li>Currently does not support kafka services with <code>kerberos</code> authentication enabled</li> </ol>"},{"location":"en/reader/kudureader/","title":"Kudu Reader","text":"<p>Kudu Reader plugin uses Kudu's Java client KuduClient to perform Kudu read operations.</p>"},{"location":"en/reader/kudureader/#configuration-example","title":"Configuration Example","text":"<p>We connect to kudu service through Trino's <code>kudu connector</code>, then perform table creation and data insertion.</p>"},{"location":"en/reader/kudureader/#table-creation-and-data-insertion-statements","title":"Table Creation and Data Insertion Statements","text":"<pre><code>CREATE TABLE kudu.default.users (\n  user_id int WITH (primary_key = true),\n  user_name varchar with (nullable=true),\n  age int with (nullable=true),\n  salary double with (nullable=true),\n  longtitue decimal(18,6) with (nullable=true),\n  latitude decimal(18,6) with (nullable=true),\n  p decimal(21,20) with (nullable=true),\n  mtime timestamp with (nullable=true)\n) WITH (\n  partition_by_hash_columns = ARRAY['user_id'],\n  partition_by_hash_buckets = 2\n);\n\ninsert into kudu.default.users \nvalues \n(1, cast('wgzhao' as varchar), 18, cast(18888.88 as double), \n cast(123.282424 as decimal(18,6)), cast(23.123456 as decimal(18,6)),\n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 14:40:41'),\n(2, cast('anglina' as varchar), 16, cast(23456.12 as double), \n cast(33.192123 as decimal(18,6)), cast(56.654321 as decimal(18,6)), \n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 03:40:41');\n-- ONLY insert primary key value\n insert into kudu.default.users(user_id) values  (3);\n</code></pre>"},{"location":"en/reader/kudureader/#configuration","title":"Configuration","text":"<p>The following is the configuration for reading kudu table and outputting to terminal:</p> job/kudu2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"kudureader\",\n        \"parameter\": {\n          \"masterAddress\": \"localhost:7051,localhost:7151,localhost:7251\",\n          \"table\": \"users\",\n          \"splitPk\": \"user_id\",\n          \"lowerBound\": 1,\n          \"upperBound\": 100,\n          \"readTimeout\": 5,\n          \"scanTimeout\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/kudu2stream.json</code></p>"},{"location":"en/reader/kudureader/#execution","title":"Execution","text":"<p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/kudu2stream.json\n</code></pre>"},{"location":"en/reader/kudureader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description masterAddress Yes string None Kudu Master cluster RPC address, multiple addresses separated by comma (,) table Yes string None Kudu table name splitPk No string None Parallel reading data shard field lowerBound No string None Lower bound of parallel reading data shard range upperBound No string None Upper bound of parallel reading data shard range readTimeout No int 10 Read data timeout (seconds) scanTimeout No int 20 Data scan request timeout (seconds) column No list None Specify fields to get where No list None Specify other filter conditions, see description below haveKerberos No boolean false Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Credential file path for Kerberos authentication, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Credential principal for Kerberos authentication, e.g. <code>addax/node1@WGZHAO.COM</code>"},{"location":"en/reader/kudureader/#where","title":"where","text":"<p><code>where</code> is used to define more filter conditions. It is an array type, where each element of the array is a filter condition, for example:</p> <pre><code>{\n  \"where\": [\"age &gt; 1\", \"user_name = 'wgzhao'\"] \n}\n</code></pre> <p>The above defines two filter conditions. Each filter condition consists of three parts in the format <code>column operator value</code>:</p> <ul> <li><code>column</code>: Field to filter</li> <li><code>operator</code>: Comparison symbol, currently only supports <code>=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>!=</code>. Other operators are not currently supported</li> <li><code>value</code>: Comparison value</li> </ul> <p>There are other limitations here that need special attention when using:</p> <ol> <li>Multiple filter conditions have logical AND relationship between them, logical OR relationship is not supported yet</li> </ol>"},{"location":"en/reader/kudureader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Kudu Data Type Long byte, short, int, long Double float, double, decimal String string Date timestamp Boolean boolean Bytes binary"},{"location":"en/reader/mongodbreader/","title":"MongoDB Reader","text":"<p>MongoDBReader plugin uses MongoDB's Java client MongoClient to perform MongoDB read operations.</p>"},{"location":"en/reader/mongodbreader/#configuration-example","title":"Configuration Example","text":"<p>This example reads a table from MongoDB and prints to terminal</p> job/mongo2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mongodbreader\",\n        \"parameter\": {\n          \"username\": \"\",\n          \"password\": \"\",\n          \"column\": [\n            \"unique_id\",\n            \"sid\",\n            \"user_id\",\n            \"auction_id\",\n            \"content_type\",\n            \"pool_type\",\n            \"frontcat_id\",\n            \"catagoryid\",\n            \"gmt_create\",\n            \"taglist\",\n            \"property\",\n            \"scorea\",\n            \"scoreb\",\n            \"scorec\"\n          ],\n          \"connection\": {\n            \"address\": [\n              \"127.0.0.1:27017\"\n            ],\n            \"database\": \"tag_per_data\",\n            \"collection\": \"tag_data\",\n            \"authDb\": \"admin\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/mongodbreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description address Yes list None MongoDB data address information, multiple can be written username No string None MongoDB username password No string None MongoDB password database Yes string None MongoDB database collection Yes string None MongoDB collection name column Yes list None MongoDB document column names, does not support <code>[\"*\"]</code> to get all columns query No string None Custom query conditions fetchSize No int 2048 Batch size for retrieving records"},{"location":"en/reader/mongodbreader/#collection","title":"collection","text":"<p>The <code>collection</code> here currently only supports a single collection, so the type is set to string rather than the array type common in other plugins. This is particularly noteworthy.</p>"},{"location":"en/reader/mongodbreader/#column","title":"column","text":"<p><code>column</code> is used to specify the field names to be read. Here we make two assumptions about field name composition:</p> <ul> <li>Cannot start with single quote (<code>'</code>)</li> <li>Cannot consist entirely of numbers and dots (<code>.</code>)</li> </ul> <p>Based on the above assumptions, we can simplify the <code>column</code> configuration while also specifying some constants as supplementary fields. For example, when collecting a table, we generally need to add collection time, collection source and other constants, which can be configured like this:</p> <pre><code>{\n  \"column\": [\n    \"col1\",\n    \"col2\",\n    \"col3\",\n    \"'source_mongodb'\",\n    \"20211026\",\n    \"123.12\"\n  ]\n}\n</code></pre> <p>The last three fields in the above configuration are constants, treated as string type, integer type, and floating point type respectively.</p>"},{"location":"en/reader/mongodbreader/#query","title":"query","text":"<p><code>query</code> is a BSON string that conforms to MongoDB query format, for example:</p> <pre><code>{\n  \"query\": \"{amount: {$gt: 140900}, oc_date: {$gt: 20190110}}\"\n}\n</code></pre> <p>The above query is similar to <code>where amount &gt; 140900 and oc_date &gt; 20190110</code> in SQL.</p>"},{"location":"en/reader/mongodbreader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type MongoDB Data Type Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes"},{"location":"en/reader/mysqlreader/","title":"MySQL Reader","text":"<p>The MySQLReader plugin enables reading data from MySQL databases.</p>"},{"location":"en/reader/mysqlreader/#example","title":"Example","text":"<p>Let's create a table in MySQL's test database and insert a record:</p> <pre><code>CREATE TABLE IF NOT EXISTS test_table (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    age INT,\n    salary DECIMAL(10,2),\n    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO test_table (name, age, salary) VALUES \n('John Doe', 30, 50000.00),\n('Jane Smith', 25, 45000.00);\n</code></pre> <p>Here's a configuration to read from this table to the console:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"password\",\n            \"column\": [\"id\", \"name\", \"age\", \"salary\", \"created_time\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?useSSL=false&amp;serverTimezone=UTC\",\n                \"table\": [\"test_table\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre> <p>Save this configuration as <code>job/mysql2stream.json</code>.</p>"},{"location":"en/reader/mysqlreader/#execute-the-job","title":"Execute the Job","text":"<p>Run the following command to execute the data collection:</p> <pre><code>bin/addax.sh job/mysql2stream.json\n</code></pre>"},{"location":"en/reader/mysqlreader/#parameter-description","title":"Parameter Description","text":"<p>This plugin is based on the RDBMS Reader implementation, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/mysqlreader/#driver","title":"driver","text":"<p>The current Addax uses MySQL JDBC driver version 8.0 or higher, with the driver class name <code>com.mysql.cj.jdbc.Driver</code>, not <code>com.mysql.jdbc.Driver</code>. If you need to collect from a MySQL server lower than version <code>5.6</code> and need to use the <code>Connector/J 5.1</code> driver, you can follow these steps:</p> <p>Replace the built-in driver</p> <pre><code>rm -f plugin/reader/mysqlreader/libs/mysql-connector-java-*.jar\n</code></pre> <p>Copy the old driver to the plugin directory</p> <pre><code>cp mysql-connector-java-5.1.48.jar plugin/reader/mysqlreader/libs/\n</code></pre> <p>Specify the driver class name</p> <p>In your JSON file, configure <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p>"},{"location":"en/reader/mysqlreader/#required-parameters","title":"Required Parameters","text":"Parameter Description Required Default jdbcUrl JDBC connection URL Yes None username Database username Yes None password Database password Yes None table List of tables to read from Yes None column List of columns to read Yes None"},{"location":"en/reader/mysqlreader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Required Default splitPk Primary key for data splitting No None where WHERE clause for filtering No None querySql Custom SQL query No None fetchSize JDBC fetch size No 1024 driver JDBC driver class name No com.mysql.cj.jdbc.Driver"},{"location":"en/reader/mysqlreader/#data-type-mapping","title":"Data Type Mapping","text":"MySQL Type Addax Type Notes TINYINT, SMALLINT, MEDIUMINT, INT long BIGINT long FLOAT, DOUBLE, DECIMAL double VARCHAR, CHAR, TEXT string DATE, TIME, DATETIME, TIMESTAMP date BIT bool BINARY, VARBINARY, BLOB bytes"},{"location":"en/reader/mysqlreader/#performance-tuning","title":"Performance Tuning","text":""},{"location":"en/reader/mysqlreader/#split-key-configuration","title":"Split Key Configuration","text":"<p>For large tables, configure <code>splitPk</code> to enable parallel reading:</p> <pre><code>{\n  \"parameter\": {\n    \"splitPk\": \"id\",\n    \"channel\": 4\n  }\n}\n</code></pre>"},{"location":"en/reader/mysqlreader/#fetch-size-optimization","title":"Fetch Size Optimization","text":"<p>Adjust <code>fetchSize</code> based on your memory and network conditions:</p> <pre><code>{\n  \"parameter\": {\n    \"fetchSize\": 2048\n  }\n}\n</code></pre>"},{"location":"en/reader/mysqlreader/#query-optimization","title":"Query Optimization","text":"<p>Use <code>where</code> clause to filter data at the source:</p> <pre><code>{\n  \"parameter\": {\n    \"where\": \"created_time &gt;= '2023-01-01' AND status = 'active'\"\n  }\n}\n</code></pre>"},{"location":"en/reader/mysqlreader/#error-handling","title":"Error Handling","text":"<p>Common issues and solutions:</p>"},{"location":"en/reader/mysqlreader/#connection-timeout","title":"Connection Timeout","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?connectTimeout=60000&amp;socketTimeout=60000\"\n  }\n}\n</code></pre>"},{"location":"en/reader/mysqlreader/#ssl-connection-issues","title":"SSL Connection Issues","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?useSSL=false\"\n  }\n}\n</code></pre>"},{"location":"en/reader/mysqlreader/#timezone-issues","title":"Timezone Issues","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?serverTimezone=UTC\"\n  }\n}\n</code></pre> <p>For more detailed configuration options, please refer to the RDBMS Reader documentation.</p>"},{"location":"en/reader/oraclereader/","title":"Oracle Reader","text":"<p>Oracle Reader plugin is used to read data from Oracle.</p>"},{"location":"en/reader/oraclereader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Oracle database to local:</p> job/oracle2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"oraclereader\",\n        \"parameter\": {\n          \"username\": \"oracle\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:oracle:thin:@127.0.0.1:5432/orcl\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/oraclereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/oraclereader/#support-for-geometry-type","title":"Support for GEOMETRY Type","text":"<p>Starting from Addax <code>4.0.13</code>, experimental support for Oracle GEOMETRY type is provided. This plugin converts this type of data to JSON array strings.</p> <p>Suppose you have such a table and data:</p> <pre><code>\n</code></pre> <p>The final output result of reading this table data is similar to the following:</p> <pre><code>\n</code></pre> <p>Note: This data type is currently in experimental support stage. The author's understanding of this data type is not deep, and it has not been comprehensively tested. Please do not use it directly in production environments.</p>"},{"location":"en/reader/postgresqlreader/","title":"PostgreSQL Reader","text":"<p>The PostgreSQLReader plugin enables reading data from PostgreSQL databases.</p>"},{"location":"en/reader/postgresqlreader/#example","title":"Example","text":"<p>Create a sample table in PostgreSQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    email VARCHAR(100),\n    age INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO users (username, email, age) VALUES \n('alice', 'alice@example.com', 28),\n('bob', 'bob@example.com', 32),\n('charlie', 'charlie@example.com', 25);\n</code></pre> <p>Configuration to read from PostgreSQL:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"postgresqlreader\",\n          \"parameter\": {\n            \"username\": \"postgres\",\n            \"password\": \"password\",\n            \"column\": [\"id\", \"username\", \"email\", \"age\", \"created_at\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/testdb\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/postgresqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on the RDBMS Reader implementation.</p>"},{"location":"en/reader/postgresqlreader/#required-parameters","title":"Required Parameters","text":"Parameter Description Required Default jdbcUrl PostgreSQL JDBC connection URL Yes None username Database username Yes None password Database password Yes None table List of tables to read from Yes None column List of columns to read Yes None"},{"location":"en/reader/postgresqlreader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Required Default splitPk Primary key for data splitting No None where WHERE clause for filtering No None fetchSize JDBC fetch size No 1024"},{"location":"en/reader/postgresqlreader/#data-type-mapping","title":"Data Type Mapping","text":"PostgreSQL Type Addax Type Notes SMALLINT, INTEGER, BIGINT long REAL, DOUBLE PRECISION, NUMERIC double VARCHAR, CHAR, TEXT string DATE, TIME, TIMESTAMP date BOOLEAN bool BYTEA bytes"},{"location":"en/reader/postgresqlreader/#performance-tips","title":"Performance Tips","text":""},{"location":"en/reader/postgresqlreader/#use-split-key-for-large-tables","title":"Use Split Key for Large Tables","text":"<pre><code>{\n  \"parameter\": {\n    \"splitPk\": \"id\",\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 4\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/postgresqlreader/#optimize-with-where-clause","title":"Optimize with WHERE Clause","text":"<pre><code>{\n  \"parameter\": {\n    \"where\": \"created_at &gt;= '2023-01-01' AND status = 'active'\"\n  }\n}\n</code></pre>"},{"location":"en/reader/postgresqlreader/#connection-examples","title":"Connection Examples","text":""},{"location":"en/reader/postgresqlreader/#standard-connection","title":"Standard Connection","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb\"\n}\n</code></pre>"},{"location":"en/reader/postgresqlreader/#ssl-connection","title":"SSL Connection","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb?ssl=true&amp;sslmode=require\"\n}\n</code></pre>"},{"location":"en/reader/postgresqlreader/#connection-pool-settings","title":"Connection Pool Settings","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb?prepareThreshold=0&amp;preparedStatementCacheQueries=0\"\n}\n</code></pre>"},{"location":"en/reader/rdbmsreader/","title":"RDBMS Reader","text":"<p>RDBMS Reader plugin supports reading data from traditional RDBMS. This is a generic relational database reading plugin that can support more relational database reading by registering database drivers.</p> <p>At the same time, RDBMS Reader is also the base class for other relational database reading plugins. The following reading plugins all depend on this plugin:</p> <ul> <li>Oracle Reader</li> <li>MySQL Reader</li> <li>PostgreSQL Reader</li> <li>ClickHouse Reader</li> <li>SQLServer Reader</li> <li>Access Reader</li> <li>Databend Reader</li> </ul> <p>Note: If a dedicated database reading plugin is already provided, it is recommended to use the dedicated plugin. If the database you need to read does not have a dedicated plugin, consider using this generic plugin. Before use, you need to perform the following operations to run normally, otherwise exceptions will occur.</p>"},{"location":"en/reader/rdbmsreader/#configure-driver","title":"Configure Driver","text":"<p>Suppose you need to read data from IBM DB2. Since no dedicated reading plugin is provided, we can use this plugin to implement it. Before use, you need to download the corresponding JDBC driver and copy it to the <code>plugin/reader/rdbmsreader/libs</code> directory. If your driver class name is special, you need to find the <code>driver</code> item in the task configuration file and fill in the correct JDBC driver name, such as DB2's driver name <code>com.ibm.db2.jcc.DB2Driver</code>. If not filled, the plugin will automatically guess the driver name.</p> <p>The following lists common databases and their corresponding driver names:</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"en/reader/rdbmsreader/#configuration","title":"Configuration","text":"<p>The following configuration shows how to read data from Presto database to terminal</p> job/rdbms2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"rdbmsreader\",\n        \"parameter\": {\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"driver\": \"io.prestosql.jdbc.PrestoDriver\",\n          \"connection\": {\n            \"table\": [\n              \"default.table\"\n            ],\n            \"jdbcUrl\": \"jdbc:presto://127.0.0.1:8080/hive\"\n          },\n          \"fetchSize\": 1024,\n          \"where\": \"1 = 1\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/rdbmsreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description jdbcUrl Yes list None JDBC connection information of the target database, jdbcUrl follows RDBMS official specifications and can include connection attachment control information driver No string None Custom driver class name to solve compatibility issues, see description below username Yes string None Username of the data source password No string None Password for the specified username of the data source table Yes list None Selected table names to be synchronized, using JSON data format. When configured for multiple tables, users need to ensure that multiple tables have the same table structure column Yes list None Collection of column names to be synchronized in the configured table, detailed description below splitPk No string None Use the field represented by splitPk for data sharding, which can greatly improve data synchronization efficiency, see notes below autoPk No boolean false Whether to automatically guess the sharding primary key, introduced in version <code>3.2.6</code>, see description below where No string None Filtering conditions for the table session No list None For local connections, modify session configuration, see below querySql No string None Use custom SQL instead of specified table to get data. When this item is configured, <code>table</code> and <code>column</code> configuration items are ignored fetchSize No int 1024 Defines the number of batch data fetched between plugin and database server each time. Increasing this value may cause Addax OOM excludeColumn No list None Column name fields to be excluded, only valid when <code>column</code> is configured as <code>*</code>"},{"location":"en/reader/rdbmsreader/#jdbcurl","title":"jdbcUrl","text":"<p>In addition to configuring necessary information, <code>jdbcUrl</code> configuration can also add specific configuration properties for each specific driver. Here we particularly mention that we can use configuration properties to support proxies to access databases through proxies. For example, for PrestoSQL database JDBC driver, it supports the <code>socksProxy</code> parameter, so the above configured <code>jdbcUrl</code> can be modified to:</p> <p><code>jdbc:presto://127.0.0.1:8080/hive?socksProxy=192.168.1.101:1081</code></p> <p>Most relational database JDBC drivers support <code>socksProxyHost,socksProxyPort</code> parameters for proxy access. There are also some special cases.</p> <p>The following are the proxy types and configuration methods supported by various database JDBC drivers:</p> Database Proxy Type Proxy Configuration Example MySQL socks socksProxyHost,socksProxyPort <code>socksProxyHost=192.168.1.101&amp;socksProxyPort=1081</code> Presto socks socksProxy <code>socksProxy=192.168.1.101:1081</code> Presto http httpProxy <code>httpProxy=192.168.1.101:3128</code>"},{"location":"en/reader/rdbmsreader/#driver","title":"driver","text":"<p>In most cases, the JDBC driver for a database is fixed, but some have different recommended driver class names due to different versions, such as MySQL. The new MySQL JDBC driver type recommends using <code>com.mysql.cj.jdbc.Driver</code> instead of the previous <code>com.mysql.jdbc.Driver</code>. If you want to use the old driver name, you can configure the <code>driver</code> configuration item. Otherwise, the plugin will automatically guess the driver name based on the string in <code>jdbcUrl</code>.</p>"},{"location":"en/reader/rdbmsreader/#column","title":"column","text":"<p>Collection of column names to be synchronized in the configured table, using JSON array to describe field information. Users use <code>*</code> to represent default use of all column configurations, such as <code>[\"*\"]</code>.</p> <p>Supports column pruning, i.e., columns can be selected for partial export.</p> <p>Supports column reordering, i.e., columns can be exported not according to table schema information.</p> <p>Supports constant configuration, users need to follow JSON format:</p> <p><code>[\"id\", \"`table`\", \"1\", \"'bazhen.csy'\", \"null\", \"to_char(a + 1)\", \"2.3\" , \"true\"]</code></p> <ul> <li><code>id</code> is ordinary column name</li> <li><code>`table`</code> is column name containing reserved words</li> <li><code>1</code> is integer constant</li> <li><code>'bazhen.csy'</code> is string constant</li> <li><code>null</code> is null pointer. Note that <code>null</code> here must appear as a string, i.e., quoted with double quotes</li> <li><code>to_char(a + 1)</code> is expression</li> <li><code>2.3</code> is floating point number</li> <li><code>true</code> is boolean value. Similarly, boolean values here must also be quoted with double quotes</li> </ul> <p>Column must be explicitly filled and cannot be empty!</p>"},{"location":"en/reader/rdbmsreader/#excludecolumn","title":"excludeColumn","text":"<p>There is a situation where we need to read most fields of a table. If the table has many fields, configuring <code>column</code> is obviously time-consuming. In particular, when we collect business data to big data platforms, we generally add some additional fields including partition fields and collection information. When we need to write back to business data tables, we need to exclude these fields. Under this consideration, we introduced the <code>excludeColumn</code> configuration item. When <code>column</code> is configured as <code>*</code>, the <code>excludeColumn</code> configuration item takes effect to exclude some fields.</p> <p>For example:</p> <pre><code>{\n  \"column\": [\"*\"],\n  \"excludeColumn\": [\"partition_col\", \"etl_time\"]\n}\n</code></pre>"},{"location":"en/reader/redisreader/","title":"Redis Reader","text":"<p>Redis Reader plugin is used to read Redis RDB data.</p>"},{"location":"en/reader/redisreader/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": [\"tcp://127.0.0.1:6379\", \"file:///data/dump.rdb\", \"http://localhost/dump.rdb\"],\n            \"auth\": \"password\"\n          },\n          \"include\": [\n            \"^user\"\n          ],\n          \"exclude\": [\n            \"^password\"\n          ],\n          \"db\": [\n            0,\n            1\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"timeout\": 60000\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/redisreader/#parameters","title":"Parameters","text":"Configuration Required Default Value Description uri Yes No Redis connection, supports multiple local rdb files/network rdb files, if cluster, fill in all master node addresses db No None Database index to read, if not filled, read all databases include No None Keys to include, supports regular expressions exclude No None Keys to exclude, supports regular expressions"},{"location":"en/reader/redisreader/#constraints","title":"Constraints","text":"<ol> <li>Does not support directly reading any redis server that does not support the <code>sync</code> command. If needed, please read from backed up rdb files.</li> <li>If it's a native redis cluster, please fill in all master node TCP addresses. The <code>redisreader</code> plugin will automatically dump rdb files from all nodes.</li> <li>Only parses <code>String</code> data type, other composite types (<code>Sets</code>, <code>List</code>, etc. will be ignored)</li> </ol>"},{"location":"en/reader/s3reader/","title":"S3 Reader","text":"<p>S3 Reader plugin is used to read data on Amazon AWS S3 storage. In implementation, this plugin is written based on S3's official SDK 2.0.</p> <p>This plugin also supports reading storage services compatible with S3 protocol, such as MinIO.</p>"},{"location":"en/reader/s3reader/#configuration-example","title":"Configuration Example","text":"<p>The following sample configuration is used to read two files from S3 storage and print them out</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"s3reader\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": [\n            \"1.csv\",\n            \"aa.csv\",\n            \"upload_*.csv\",\n            \"bb_??.csv\"\n          ],\n          \"column\": [\n            \"*\"\n          ],\n          \"region\": \"ap-northeast-1\",\n          \"fileFormat\": \"csv\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/s3reader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None S3 Server EndPoint address, e.g. <code>s3.xx.amazonaws.com</code> region Yes string None S3 Server Region address, e.g. <code>ap-southeast-1</code> accessId Yes string None Access ID accessKey Yes string None Access Key bucket Yes string None Bucket to read object Yes list None Objects to read, can specify multiple and wildcard patterns, see description below column Yes list None Column information of objects to read, refer to <code>column</code> description in RDBMS Reader fieldDelimiter No string <code>,</code> Field delimiter for reading, only supports single character compress No string None File compression format, default is no compression encoding No string <code>utf8</code> File encoding format writeMode No string <code>nonConflict</code> pathStyleAccessEnabled No boolean false Whether to enable path-style access mode"},{"location":"en/reader/s3reader/#object","title":"object","text":"<p>When specifying a single object, the plugin can currently only use single-threaded data extraction.</p>"},{"location":"en/reader/sqlitereader/","title":"SQLite Reader","text":"<p>SQLite Reader plugin is used to read sqlite files in a specified directory. It inherits from RDBMS Reader.</p>"},{"location":"en/reader/sqlitereader/#example","title":"Example","text":"<p>We create an example file:</p> <pre><code>$ sqlite3  /tmp/test.sqlite3\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter \".help\" for instructions\nEnter SQL statements terminated with a \";\"\nsqlite&gt; create table test(id int, name varchar(10), salary double);\nsqlite&gt; insert into test values(1,'foo', 12.13),(2,'bar',202.22);\nsqlite&gt; .q\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/sqlite2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlitereader\",\n        \"parameter\": {\n          \"username\": \"fakeuser\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite:/tmp/test.sqlite3\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/sqlite2stream.json</code></p>"},{"location":"en/reader/sqlitereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/sqlite2stream.json\n</code></pre>"},{"location":"en/reader/sqlitereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/sqlserverreader/","title":"SQLServer Reader","text":"<p>SqlServerReader plugin is used to read data from SQLServer.</p>"},{"location":"en/reader/sqlserverreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from SQLServer database to local:</p> job/sqlserver2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlserverreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://localhost:3433;DatabaseName=dbname\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/sqlserverreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/streamreader/","title":"Stream Reader","text":"<p>Stream Reader is a plugin that reads data from memory, mainly used to quickly generate expected data and test write plugins.</p> <p>A complete StreamReader configuration file is as follows:</p> <pre><code>{\n  \"reader\": {\n    \"name\": \"streamreader\",\n    \"parameter\": {\n      \"column\": [\n        {\n          \"value\": \"unique_id\",\n          \"type\": \"string\"\n        },\n        {\n          \"value\": \"1989-06-04 08:12:13\",\n          \"type\": \"date\",\n          \"dateFormat\": \"yyyy-MM-dd HH:mm:ss\"\n        },\n        {\n          \"value\": 1984,\n          \"type\": \"long\"\n        },\n        {\n          \"value\": 1989.64,\n          \"type\": \"double\"\n        },\n        {\n          \"value\": true,\n          \"type\": \"bool\"\n        },\n        {\n          \"value\": \"a long text\",\n          \"type\": \"bytes\"\n        }\n      ],\n      \"sliceRecordCount\": 10\n    }\n  }\n}\n</code></pre> <p>The above configuration file will generate 10 records (assuming channel is 1), with each record containing:</p> <p><code>unique_id,'1989-06-04 08:12:13',1984,1989.64,true,'a long text'</code></p> <p>Currently StreamReader supports all output data types listed above:</p> <ul> <li><code>string</code> String type</li> <li><code>date</code> Date type  </li> <li><code>long</code> All integer types</li> <li><code>double</code> All floating point numbers</li> <li><code>bool</code> Boolean type</li> <li><code>bytes</code> Byte type</li> </ul> <p>The <code>date</code> type also supports <code>dateFormat</code> configuration to specify the format of input dates, default is <code>yyyy-MM-dd HH:mm:ss</code>. For example, your input can be like this:</p> <pre><code>{\n  \"value\": \"1989/06/04 12:13:14\",\n  \"type\": \"date\",\n  \"dateFormat\": \"yyyy/MM/dd HH:mm:ss\"\n}\n</code></pre> <p>Note that regardless of the input format for date type, it is internally converted to <code>yyyy-MM-dd HH:mm:ss</code> format.</p> <p>StreamReader also supports random input functionality. For example, to randomly get any integer between 0-10, we can configure the column like this:</p> <pre><code>{\n  \"random\": \"0,10\",\n  \"type\": \"long\"\n}\n</code></pre> <p>To get a random floating point number between 0 and 100, configure like this:</p> <pre><code>{\n  \"random\": \"0,100\",\n  \"type\": \"double\"\n}\n</code></pre> <p>To specify decimal places for floating point numbers, e.g., 2 decimal places, configure like this:</p> <pre><code>{\n  \"random\": \"0,100,2\",\n  \"type\": \"double\"\n}\n</code></pre> <p>Note: It cannot guarantee that the generated decimal always has exactly 2 places. If the decimal part is 0, the decimal places will be fewer than specified.</p> <p>Here we use the <code>random</code> keyword to indicate its value is random, with the range being a closed interval.</p> <p>Other random type configurations are as follows:</p> <ul> <li><code>long</code>: random 0, 10 - random number between 0 and 10</li> <li><code>string</code>: random 0, 10 - random string of length between 0 and 10</li> <li><code>bool</code>: random 0, 10 - ratio of false and true occurrences</li> <li><code>double</code>: random 0, 10 - random floating point between 0 and 10</li> <li><code>double</code>: random 0, 10, 2 - random floating point between 0 and 10 with 2 decimal places</li> <li><code>date</code>: random '2014-07-07 00:00:00', '2016-07-07 00:00:00' - random time between start time and end time, default date format (commas not supported) yyyy-MM-dd HH:mm:ss</li> <li><code>BYTES</code>: random 0, 10 - random string of length between 0 and 10, get its UTF-8 encoded binary string</li> </ul> <p>StreamReader also supports increment functions. For example, to get an arithmetic sequence starting from 1 with increment of 5, configure like this:</p> <pre><code>{\n  \"incr\": \"1,5\",\n  \"type\": \"long\"\n}\n</code></pre> <p>To get a decreasing sequence, change the step size (5 in the above example) to negative. Default step size is 1.</p> <p>Increment also supports date type (introduced in version <code>4.0.1</code>), for example:</p> <pre><code>{\n  \"incr\": \"1989-06-04 09:01:02,2,d\",\n  \"type\": \"date\"\n}\n</code></pre> <p><code>incr</code> consists of three parts: start date, step size, and step unit, separated by English commas (,).</p> <ul> <li>Start date: Correct date string, default format is <code>yyyy-MM-dd hh:mm:ss</code>. If time format is different, need to configure <code>dateFormat</code> to specify date format. This is mandatory.</li> <li>Step size: Length to increase each time, default is 1. For decreasing, fill in negative number. This is optional.</li> <li>Step unit: What time unit to increment/decrement by, default is by day. This is optional. Available units:</li> <li>d/day</li> <li>M/month</li> <li>y/year</li> <li>h/hour</li> <li>m/minute</li> <li>s/second</li> <li>w/week</li> </ul> <p>Configuration item <code>sliceRecordCount</code> specifies the number of data records to generate. If <code>channel</code> is specified, actual generated records = <code>sliceRecordCount * channel</code></p>"},{"location":"en/reader/sybasereader/","title":"Sybase Reader","text":"<p>SybaseReader plugin implements reading data from Sybase.</p>"},{"location":"en/reader/sybasereader/#example","title":"Example","text":"<p>We can use Docker container to start a Sybase database</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/sybasereader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sybasereader\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.ijdbc_function_escapes\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/sybase2stream.json</code></p>"},{"location":"en/reader/sybasereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/sybase2stream.json\n</code></pre>"},{"location":"en/reader/sybasereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"en/reader/tdenginereader/","title":"TDengine Reader","text":"<p>TDengine Reader plugin is used to read data from TDengine by TaosData TDengine.</p>"},{"location":"en/reader/tdenginereader/#prerequisites","title":"Prerequisites","text":"<p>Considering performance issues, this plugin uses TDengine's JDBC-JNI driver, which directly calls the client API (<code>libtaos.so</code> or <code>taos.dll</code>) to send write and query requests to taosd instances. Therefore, dynamic library link files need to be configured before use.</p> <p>First copy <code>plugin/reader/tdenginereader/libs/libtaos.so.2.0.16.0</code> to <code>/usr/lib64</code> directory, then execute the following commands to create soft links:</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"en/reader/tdenginereader/#example","title":"Example","text":"<p>TDengine comes with a demo database taosdemo. We read some data from the demo database and print to terminal.</p> <p>The following is the configuration file:</p> job/tdengine2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"tdenginereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"taosdata\",\n          \"beginDateTime\": \"2017-07-14 10:40:00\",\n          \"endDateTime\": \"2017-08-14 10:40:00\",\n          \"splitInterval\": \"1d\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:TAOS://127.0.0.1:6030/test\",\n            \"querySql\": [\n              \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 10\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/tdengine2stream.json</code></p>"},{"location":"en/reader/tdenginereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/tdengine2stream.json\n</code></pre> <p>Command output is similar to the following:</p> <pre><code>2021-02-20 15:32:23.161 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-02-20 15:32:23.229 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"password\":\"*****\",\n                    \"connection\":[\n                        {\n                            \"querySql\":[\n                                \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n                            ],\n                            \"jdbcUrl\":[\n                                \"jdbc:TAOS://127.0.0.1:6030/test\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"root\"\n                },\n                \"name\":\"tdenginereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n2021-02-20 15:32:23.277 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-20 15:32:23.278 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-20 15:32:23.281 [main] INFO  JobContainer - Set jobId = 0\njava.library.path:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n....\n2021-02-20 15:32:23.687 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Begin to read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n2021-02-20 15:32:23.692 [0-0-0-reader] WARN  DBUtil - current database does not supoort TYPE_FORWARD_ONLY/CONCUR_READ_ONLY\n2021-02-20 15:32:23.740 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Finished read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n\n1500000001000   5   5   0   1   beijing\n1500000001000   0   6   2   1   beijing\n1500000001000   7   0   0   1   beijing\n1500000001000   8   9   6   1   beijing\n1500000001000   9   9   1   1   beijing\n1500000001000   8   2   0   1   beijing\n1500000001000   4   5   5   3   beijing\n1500000001000   3   3   3   3   beijing\n1500000001000   5   4   8   3   beijing\n1500000001000   9   4   6   3   beijing\n\n2021-02-20 15:32:26.689 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-20 15:32:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-20 15:32:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              800B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/reader/tdenginereader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description jdbcUrl Yes list None JDBC connection information of target database, note that <code>TAOS</code> here must be uppercase username Yes string None Username of data source password No string None Password for specified username of data source table Yes list None Selected table names to be synchronized, using JSON data format. When configured for multiple tables, users need to ensure multiple tables have the same structure column Yes list None Collection of column names to be synchronized in configured table, detailed description rdbmreader where No string None Filtering conditions for the table querySql No list None Use custom SQL instead of specified table to get data. When this item is configured, Addax system will ignore <code>table</code>, <code>column</code> configuration items beginDateTime Yes string None Data start time, Job migrates data from <code>beginDateTime</code> to <code>endDateTime</code>, format is <code>yyyy-MM-dd HH:mm:ss</code> endDateTime Yes string None Data end time, Job migrates data from <code>beginDateTime</code> to <code>endDateTime</code>, format is <code>yyyy-MM-dd HH:mm:ss</code> splitInterval Yes string None Divide <code>task</code> according to <code>splitInterval</code>, create one <code>task</code> per <code>splitInterval</code>"},{"location":"en/reader/tdenginereader/#splitinterval","title":"splitInterval","text":"<p>Used to divide <code>task</code>. For example, <code>20d</code> represents dividing data into 1 <code>task</code> every 20 days. Configurable time units:</p> <ul> <li><code>d</code> (day)</li> <li><code>h</code> (hour)</li> <li><code>m</code> (minute)</li> <li><code>s</code> (second)</li> </ul>"},{"location":"en/reader/tdenginereader/#using-jdbc-restful-interface","title":"Using JDBC-RESTful Interface","text":"<p>If you don't want to depend on local libraries or don't have permissions, you can use the <code>JDBC-RESTful</code> interface to write to tables. Compared to JDBC-JNI, the configuration differences are:</p> <ul> <li>driverClass specified as <code>com.taosdata.jdbc.rs.RestfulDriver</code></li> <li>jdbcUrl starts with <code>jdbc:TAOS-RS://</code></li> <li>Use <code>6041</code> as connection port</li> </ul> <p>So the <code>connection</code> in the above configuration should be modified as follows:</p> <pre><code>{\n  \"connection\": [\n    {\n      \"querySql\": [\n        \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n      ],\n      \"jdbcUrl\": [\n        \"jdbc:TAOS-RS://127.0.0.1:6041/test\"\n      ],\n      \"driver\": \"com.taosdata.jdbc.rs.RestfulDriver\"\n    }\n  ]\n}\n</code></pre>"},{"location":"en/reader/tdenginereader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type TDengine Data Type Long SMALLINT, TINYINT, INT, BIGINT, TIMESTAMP Double FLOAT, DOUBLE String BINARY, NCHAR Boolean BOOL"},{"location":"en/reader/tdenginereader/#currently-supported-versions","title":"Currently Supported Versions","text":"<p>TDengine 2.0.16</p>"},{"location":"en/reader/tdenginereader/#notes","title":"Notes","text":"<ul> <li>TDengine JDBC-JNI driver and dynamic library versions must match one-to-one. Therefore, if your data version is not <code>2.0.16</code>, you need to replace both the dynamic library and JDBC driver in the plugin directory.</li> </ul>"},{"location":"en/reader/txtfilereader/","title":"Text File Reader","text":"<p>The TxtFileReader plugin reads data from text files with configurable delimiters and encodings.</p>"},{"location":"en/reader/txtfilereader/#example","title":"Example","text":"<p>Sample CSV file (<code>/tmp/users.csv</code>):</p> <pre><code>id,name,age,email\n1,John Doe,30,john@example.com\n2,Jane Smith,25,jane@example.com\n3,Bob Johnson,35,bob@example.com\n</code></pre> <p>Configuration to read the CSV file:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/users.csv\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\n              {\n                \"index\": 0,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 1,\n                \"type\": \"string\"\n              },\n              {\n                \"index\": 2,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 3,\n                \"type\": \"string\"\n              }\n            ],\n            \"fieldDelimiter\": \",\",\n            \"skipHeader\": true\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#parameters","title":"Parameters","text":""},{"location":"en/reader/txtfilereader/#required-parameters","title":"Required Parameters","text":"Parameter Description Type Default path File path or directory path string None column Column configuration array None"},{"location":"en/reader/txtfilereader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Type Default encoding File encoding string UTF-8 fieldDelimiter Field delimiter string , compress Compression format string None skipHeader Skip first line boolean false nullFormat Null value representation string \\N"},{"location":"en/reader/txtfilereader/#column-configuration","title":"Column Configuration","text":"<p>Each column in the <code>column</code> array can be configured with:</p> Property Description Type Required index Column index (0-based) integer Yes type Data type string Yes value Constant value string No"},{"location":"en/reader/txtfilereader/#supported-data-types","title":"Supported Data Types","text":"<ul> <li><code>long</code>: Integer numbers</li> <li><code>double</code>: Floating point numbers  </li> <li><code>string</code>: Text data</li> <li><code>date</code>: Date and time</li> <li><code>bool</code>: Boolean values</li> </ul>"},{"location":"en/reader/txtfilereader/#path-configuration","title":"Path Configuration","text":""},{"location":"en/reader/txtfilereader/#single-file","title":"Single File","text":"<pre><code>{\n  \"path\": \"/data/users.csv\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#multiple-files-with-wildcards","title":"Multiple Files with Wildcards","text":"<pre><code>{\n  \"path\": \"/data/users_*.csv\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#directory","title":"Directory","text":"<pre><code>{\n  \"path\": \"/data/csv_files/\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#delimiter-examples","title":"Delimiter Examples","text":""},{"location":"en/reader/txtfilereader/#tab-separated-values","title":"Tab-separated Values","text":"<pre><code>{\n  \"fieldDelimiter\": \"\\t\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#pipe-separated-values","title":"Pipe-separated Values","text":"<pre><code>{\n  \"fieldDelimiter\": \"|\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#fixed-width-using-spaces","title":"Fixed-width (using spaces)","text":"<pre><code>{\n  \"fieldDelimiter\": \" \"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#compression-support","title":"Compression Support","text":""},{"location":"en/reader/txtfilereader/#gzip-files","title":"GZIP Files","text":"<pre><code>{\n  \"path\": \"/data/users.csv.gz\",\n  \"compress\": \"gzip\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#bzip2-files","title":"BZIP2 Files","text":"<pre><code>{\n  \"path\": \"/data/users.csv.bz2\",  \n  \"compress\": \"bzip2\"\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#advanced-examples","title":"Advanced Examples","text":""},{"location":"en/reader/txtfilereader/#reading-json-lines-format","title":"Reading JSON Lines Format","text":"<pre><code>{\n  \"reader\": {\n    \"name\": \"txtfilereader\",\n    \"parameter\": {\n      \"path\": \"/data/users.jsonl\",\n      \"encoding\": \"UTF-8\",\n      \"column\": [\n        {\n          \"index\": 0,\n          \"type\": \"string\"\n        }\n      ],\n      \"fieldDelimiter\": \"\\n\"\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#reading-log-files","title":"Reading Log Files","text":"<pre><code>{\n  \"reader\": {\n    \"name\": \"txtfilereader\", \n    \"parameter\": {\n      \"path\": \"/var/log/app.log\",\n      \"encoding\": \"UTF-8\",\n      \"column\": [\n        {\n          \"index\": 0,\n          \"type\": \"string\"\n        }\n      ],\n      \"fieldDelimiter\": \"\\n\",\n      \"nullFormat\": \"\"\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#constant-values","title":"Constant Values","text":"<pre><code>{\n  \"column\": [\n    {\n      \"index\": 0,\n      \"type\": \"long\"\n    },\n    {\n      \"index\": 1, \n      \"type\": \"string\"\n    },\n    {\n      \"type\": \"string\",\n      \"value\": \"batch_001\"\n    }\n  ]\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#error-handling","title":"Error Handling","text":""},{"location":"en/reader/txtfilereader/#invalid-data-types","title":"Invalid Data Types","text":"<p>When a field cannot be converted to the specified type, it will be treated as dirty data according to your error handling configuration.</p>"},{"location":"en/reader/txtfilereader/#missing-files","title":"Missing Files","text":"<p>If specified files don't exist, the job will fail. Use wildcards carefully to ensure at least one file matches.</p>"},{"location":"en/reader/txtfilereader/#encoding-issues","title":"Encoding Issues","text":"<p>If file encoding doesn't match the specified encoding, character corruption may occur. Always verify the actual file encoding.</p>"},{"location":"en/reader/txtfilereader/#performance-tips","title":"Performance Tips","text":""},{"location":"en/reader/txtfilereader/#large-files","title":"Large Files","text":"<p>For very large files, consider splitting them or using multiple channels:</p> <pre><code>{\n  \"setting\": {\n    \"speed\": {\n      \"channel\": 4\n    }\n  }\n}\n</code></pre>"},{"location":"en/reader/txtfilereader/#memory-usage","title":"Memory Usage","text":"<p>The reader loads data in chunks, so memory usage is generally stable regardless of file size.</p>"},{"location":"en/reader/txtfilereader/#network-files","title":"Network Files","text":"<p>When reading from network-mounted drives, network latency may affect performance. Consider copying files locally first for better performance.</p>"},{"location":"en/writer/accesswriter/","title":"Access Writer","text":"<p>Access Writer plugin implements the functionality of writing data to Access destination tables.</p>"},{"location":"en/writer/accesswriter/#example","title":"Example","text":"<p>Assume the Access table to be written has the following DDL statement:</p> <pre><code>create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\n</code></pre> <p>Here we use data generated from memory to import into Access.</p> job/stream2access.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"accesswriter\",\n        \"parameter\": {\n          \"username\": \"wgzhao\",\n          \"password\": \"\",\n          \"column\": [\n            \"name\",\n            \"file_size\",\n            \"file_date\",\n            \"file_open\",\n            \"memo\"\n          ],\n          \"ddl\": \"create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:ucanaccess:////Users/wgzhao/Downloads/AccessThemeDemo.mdb\",\n            \"table\": [\n              \"tbl_test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2access.json</code></p>"},{"location":"en/writer/accesswriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2access.json\n</code></pre>"},{"location":"en/writer/accesswriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/accesswriter/#change-log","title":"Change Log","text":"<ol> <li>From version <code>5.0.1</code>, when the Access database file to be written does not exist, it will be automatically created and the database format will be set to <code>Access 2016</code></li> </ol>"},{"location":"en/writer/cassandrawriter/","title":"Cassandra Writer","text":"<p>Cassandra Writer plugin is used to write data to Cassandra.</p>"},{"location":"en/writer/cassandrawriter/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to import data from memory to Cassandra:</p> jobs/stream2cassandra.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"name\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"false\",\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": \"addr\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": 1.234,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 12345678,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 2.345,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 3456789,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"value\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10000000\n        }\n      },\n      \"writer\": {\n        \"name\": \"cassandrawriter\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"stresscql\",\n          \"table\": \"dst\",\n          \"batchSize\": 10,\n          \"column\": [\n            \"name\",\n            \"choice\",\n            \"date\",\n            \"address\",\n            \"dbl\",\n            \"lval\",\n            \"fval\",\n            \"ival\",\n            \"uid\",\n            \"value\",\n            \"listval\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/cassandrawriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description host Yes string None Domain or IP of connection points, multiple nodes separated by commas port Yes int 9042 Cassandra port username No string None Username of data source password No string None Password for specified username of data source useSSL No boolean false Whether to use SSL connection connectionsPerHost No int 8 Client connection pool configuration: how many connections to establish with each server node maxPendingPerConnection No int 128 Client connection pool configuration: maximum requests per connection keyspace Yes string None Keyspace where the table to be synchronized is located table Yes string None Selected table to be synchronized column Yes list None Collection of columns to be synchronized in the configured table consistancyLevel No string <code>LOCAL_QUORUM</code> Data consistency level batchSize No int 1 Number of records in one batch submission (UNLOGGED BATCH)"},{"location":"en/writer/cassandrawriter/#column","title":"column","text":"<p>Content can be column names or <code>writetime()</code>. If a column name is configured as <code>writetime()</code>, the content of this column will be used as timestamp.</p>"},{"location":"en/writer/cassandrawriter/#consistancylevel","title":"consistancyLevel","text":"<p>Options: <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code></p>"},{"location":"en/writer/cassandrawriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Cassandra Data Type Long int, tinyint, smallint,varint,bigint,time Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob <p>Please note:</p> <p>Currently does not support <code>counter</code> type and <code>custom</code> type.</p>"},{"location":"en/writer/cassandrawriter/#constraints","title":"Constraints","text":""},{"location":"en/writer/cassandrawriter/#batchsize","title":"batchSize","text":"<ol> <li>Cannot exceed 65535</li> <li>The content size in batch is limited by server-side <code>batch_size_fail_threshold_in_kb</code>.</li> <li>If batch content exceeds <code>batch_size_warn_threshold_in_kb</code> limit, warn logs will be printed, but it doesn't affect writing and can be ignored.</li> <li>If batch submission fails, all content in this batch will be rewritten record by record.</li> </ol>"},{"location":"en/writer/clickhousewriter/","title":"ClickHouse Writer","text":"<p>ClickHouse Writer plugin is used to write data to ClickHouse.</p>"},{"location":"en/writer/clickhousewriter/#example","title":"Example","text":"<p>The following example demonstrates reading content from one table in ClickHouse and writing it to another table with the same table structure, to test the data structures supported by the plugin.</p>"},{"location":"en/writer/clickhousewriter/#table-structure-and-data","title":"Table Structure and Data","text":"<p>Assume the table structure and data to be read are as follows:</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre> <p>The table to be written uses the same structure as the read table, with the following DDL statement:</p> <pre><code>create table ck_addax_writer as ck_addax;\n</code></pre>"},{"location":"en/writer/clickhousewriter/#configuration","title":"Configuration","text":"<p>The following is the configuration file</p> job/clickhouse2clickhouse.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"clickhousewriter\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          },\n          \"preSql\": [\n            \"alter table @table delete where 1=1\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/\",\n            \"table\": [\n              \"ck_addax\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/clickhouse2clickhouse.json</code></p>"},{"location":"en/writer/clickhousewriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/clickhouse2clickhouse.json\n</code></pre>"},{"location":"en/writer/clickhousewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/databendwriter/","title":"DatabendWriter","text":"<p>Databend plugin is used to write data to Databend database via JDBC.</p> <p>Databend is a database backend compatible with MySQL protocol, so Databend writing can use MySQLWriter for access.</p>"},{"location":"en/writer/databendwriter/#example","title":"Example","text":"<p>Assume the table to be written has the following DDL statement:</p> <pre><code>CREATE DATABASE example_db;\nCREATE TABLE `example_db`.`table1`\n(\n    `siteid`   INT DEFAULT CAST(10 AS INT),\n    `citycode` INT,\n    `username` VARCHAR,\n    `pv`       BIGINT\n);\n</code></pre> <p>The following configures a configuration file to read data from memory and write to databend table:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"databendwriter\",\n        \"parameter\": {\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"postSql\": [],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://localhost:8000/addax\",\n            \"table\": [\n              \"table1\"\n            ]\n          },\n          \"username\": \"u1\",\n          \"password\": \"123\",\n          \"column\": [\n            \"*\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2databend.json</code></p> <p>Execute the following command:</p> <pre><code>bin/addax.sh job/stream2Databend.json\n</code></pre>"},{"location":"en/writer/databendwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds the following configuration items:</p> Configuration Required Type Default Value Description writeMode No string <code>insert</code> Write mode, supports <code>insert</code> and <code>replace</code> modes onConflictColumn No string None Conflict column, when writeMode is <code>replace</code>, must specify conflict column, otherwise write will fail"},{"location":"en/writer/databendwriter/#writemode","title":"writeMode","text":"<p>Used to support Databend's <code>replace into</code> syntax. When this parameter is set to <code>replace</code>, the <code>onConflictColumn</code> parameter must also be specified to determine whether data is inserted or updated.</p> <p>Example of both parameters:</p> <pre><code>{\n  \"writeMode\": \"replace\",\n  \"onConflictColumn\": [\n    \"id\"\n  ]\n}\n</code></pre>"},{"location":"en/writer/dbfwriter/","title":"DBF Writer","text":"<p>DBF Writer provides writing DBF-like format to one or more table files in local file system.</p>"},{"location":"en/writer/dbfwriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"\u4e2d\u6587\u6d4b\u8bd5\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"dbfwriter\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"name\": \"col1\",\n              \"type\": \"char\",\n              \"length\": 100\n            },\n            {\n              \"name\": \"col2\",\n              \"type\": \"numeric\",\n              \"length\": 18,\n              \"scale\": 0\n            },\n            {\n              \"name\": \"col3\",\n              \"type\": \"date\"\n            },\n            {\n              \"name\": \"col4\",\n              \"type\": \"logical\"\n            },\n            {\n              \"name\": \"col5\",\n              \"type\": \"char\",\n              \"length\": 100\n            }\n          ],\n          \"fileName\": \"test.dbf\",\n          \"path\": \"/tmp/out\",\n          \"writeMode\": \"truncate\",\n          \"encoding\": \"GBK\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/dbfwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File directory, note this is a folder, not a file column Yes <code>list&lt;map&gt;</code> None Collection of columns to be synchronized in configured table, see example configuration fileName Yes string None Name of file to write writeMode Yes string None Data cleanup processing mode before writing, see description below encoding No string UTF-8 File encoding, such as <code>GBK</code>, <code>UTF-8</code> nullFormat No string <code>\\N</code> Define which string can represent null dateFormat No string None Format when date type data is serialized to file, e.g. <code>\"yyyy-MM-dd\"</code>"},{"location":"en/writer/dbfwriter/#writemode","title":"writeMode","text":"<p>Data cleanup processing mode before writing:</p> <ul> <li>truncate: Clean all files with <code>fileName</code> prefix under directory before writing</li> <li>append: No processing before writing, write directly using <code>filename</code> and ensure no filename conflicts</li> <li>nonConflict: If there are files with <code>fileName</code> prefix under directory, report error directly</li> </ul>"},{"location":"en/writer/dbfwriter/#type-conversion","title":"Type Conversion","text":"<p>Currently this plugin supports the following write types and corresponding relationships:</p> XBase Type XBase Symbol Java Type used in JavaDBF Character C java.lang.String Numeric N java.math.BigDecimal Floating Point F java.math.BigDecimal Logical L java.lang.Boolean"},{"location":"en/writer/doriswriter/","title":"Doris Writer","text":"<p>Doris Writer plugin implements writing data to Apache Doris.</p>"},{"location":"en/writer/doriswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Doris database. For detailed configuration and parameters, please refer to the original Doris Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"doriswriter\",\n        \"parameter\": {\n          \"loadUrl\": [\n            \"127.0.0.1:8030\"\n          ],\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"batchSize\": 1024,\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"connection\": {\n            \"table\": \"table1\",\n            \"database\": \"example_db\",\n            \"jdbcUrl\": \"jdbc:mysql://localhost:9030/example_db\"\n          },\n          \"loadProps\": {\n            \"format\": \"json\",\n            \"strip_outer_array\": true\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/doriswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Doris with Stream Load capabilities and configurable connection options.</p>"},{"location":"en/writer/elasticsearchwriter/","title":"ElasticSearch Writer","text":"<p>ElasticSearch Writer plugin is used to write data to ElasticSearch. It is implemented through elasticsearch's rest api interface, writing data to elasticsearch in batches.</p>"},{"location":"en/writer/elasticsearchwriter/#configuration-example","title":"Configuration Example","text":"job/stream2es.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1.1.1.1\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"hello world\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"long text\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"41.12,-71.34\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"2017-05-25 11:22:33\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"elasticsearchwriter\",\n        \"parameter\": {\n          \"endpoint\": \"http://localhost:9200\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"cleanup\": true,\n          \"settings\": {\n            \"index\": {\n              \"number_of_shards\": 1,\n              \"number_of_replicas\": 0\n            }\n          },\n          \"discovery\": false,\n          \"batchSize\": 1000,\n          \"splitter\": \",\",\n          \"column\": [\n            {\n              \"name\": \"pk\",\n              \"type\": \"id\"\n            },\n            {\n              \"name\": \"col_ip\",\n              \"type\": \"ip\"\n            },\n            {\n              \"name\": \"col_double\",\n              \"type\": \"double\"\n            },\n            {\n              \"name\": \"col_long\",\n              \"type\": \"long\"\n            },\n            {\n              \"name\": \"col_integer\",\n              \"type\": \"integer\"\n            },\n            {\n              \"name\": \"col_keyword\",\n              \"type\": \"keyword\"\n            },\n            {\n              \"name\": \"col_text\",\n              \"type\": \"text\",\n              \"analyzer\": \"ik_max_word\"\n            },\n            {\n              \"name\": \"col_geo_point\",\n              \"type\": \"geo_point\"\n            },\n            {\n              \"name\": \"col_date\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd HH:mm:ss\"\n            },\n            {\n              \"name\": \"col_nested1\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_nested2\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_object1\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_object2\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_integer_array\",\n              \"type\": \"integer\",\n              \"array\": true\n            },\n            {\n              \"name\": \"col_geo_shape\",\n              \"type\": \"geo_shape\",\n              \"tree\": \"quadtree\",\n              \"precision\": \"10m\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/elasticsearchwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None ElasticSearch connection address, if cluster, multiple addresses separated by comma (,) accessId No string Empty User in http auth, default is empty accessKey No string Empty Password in http auth index Yes string None Index name type No string <code>default</code> Index type cleanup No boolean false Whether to delete original table batchSize No int 1000 Number of records in each batch trySize No int 30 Number of retries after failure timeout No int 600000 Client timeout in milliseconds (ms) discovery No boolean false Enable node discovery (polling) and periodically update server list in client compression No boolean true Whether to enable http request compression multiThread No boolean true Whether to enable multi-threaded http requests ignoreWriteError No boolean false Whether to retry on write error, if <code>true</code> means always retry, otherwise ignore the record ignoreParseError No boolean true Whether to continue writing when data format parsing error occurs alias No string None Alias to write after data import is completed aliasMode No string append Mode for adding alias after data import completion, append (add mode), exclusive (keep only this one) settings No map None Settings when creating index, same as elasticsearch official splitter No string <code>,</code> If inserted data is array, use specified delimiter column Yes <code>list&lt;map&gt;</code> None Field types, the example in the document includes all supported field types dynamic No boolean false Don't use addax mappings, use es's own automatic mappings"},{"location":"en/writer/elasticsearchwriter/#constraints","title":"Constraints","text":"<ul> <li>If importing id, data import failures will also retry, re-import will only overwrite, ensuring data consistency</li> <li>If not importing id, it's append_only mode, elasticsearch automatically generates id, speed will improve about 20%, but data cannot be repaired, suitable for log-type data (low precision requirements)</li> </ul>"},{"location":"en/writer/excelwriter/","title":"Excel Writer","text":"<p>Excel Writer implements the functionality of writing data to Excel files.</p>"},{"location":"en/writer/excelwriter/#configuration-example","title":"Configuration Example","text":"<p>We assume reading data from memory and writing to Excel file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"excelwriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/out\",\n          \"fileName\": \"test\",\n          \"header\": [\n            \"str\",\n            \"\u957f\u5ea6\",\n            \"\u65e5\u671f\",\n            \"\u662f\u5426\u4e3a\u771f\",\n            \"\u5b57\u8282\u7c7b\u578b\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/stream2excel.json</code></p> <p>Execute the following command:</p> <pre><code>bin/addax.sh job/stream2excel.sh\n</code></pre> <p>You should get output similar to the following:</p> Click to expand <pre><code>  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3-SNAPSHOT)\n\n2021-09-10 22:16:38.247 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-10 22:16:38.269 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"type\":\"long\",\n                            \"value\":19890604\n                        },\n                        {\n                            \"type\":\"date\",\n                            \"value\":\"1989-06-04 11:22:33\"\n                        },\n                        {\n                            \"type\":\"bool\",\n                            \"value\":true\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        }\n                    ],\n                    \"sliceRecordCount\":1000\n                },\n                \"name\":\"streamreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"path\":\"/tmp/out\",\n                    \"fileName\":\"test\",\n                    \"header\":[\n                        \"str\",\n                        \"\u957f\u5ea6\",\n                        \"\u65e5\u671f\",\n                        \"\u662f\u5426\u4e3a\u771f\",\n                        \"\u5b57\u8282\u7c7b\u578b\"\n                    ],\n                    \"writeMode\":\"truncate\"\n                },\n                \"name\":\"excelwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-09-10 22:16:38.287 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-10 22:16:38.287 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-10 22:16:38.289 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-10 22:16:38.303 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-09-10 22:16:38.305 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] splits to [1] tasks.\n2021-09-10 22:16:38.325 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-10 22:16:38.332 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-09-10 22:16:38.335 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-10 22:16:38.336 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-10 22:16:41.345 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do post work.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-09-10 22:16:41.348 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-10 22:16:41.349 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 1000 records, 26000 bytes | Speed 8.46KB/s, 333 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.528s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-09-10 22:16:41.350 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-10 22:16:38\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-10 22:16:41\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            8.46KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :            333rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                1000\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"en/writer/excelwriter/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description path Yes string None Specify the directory to save files, create if directory doesn't exist fileName Yes string None Excel filename to generate, detailed description below header No list None Excel header"},{"location":"en/writer/excelwriter/#filename","title":"fileName","text":"<p>For detailed fileName configuration, please refer to the original Excel Writer documentation.</p>"},{"location":"en/writer/ftpwriter/","title":"FTP Writer","text":"<p>FTP Writer provides the ability to write files to remote FTP/SFTP servers, currently only supports writing text files.</p>"},{"location":"en/writer/ftpwriter/#configuration-example","title":"Configuration Example","text":"job/stream2ftp.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"ftpwriter\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"***\",\n          \"port\": 22,\n          \"username\": \"xxx\",\n          \"password\": \"xxx\",\n          \"timeout\": \"60000\",\n          \"connectPattern\": \"PASV\",\n          \"path\": \"/tmp/data/\",\n          \"fileName\": \"test\",\n          \"writeMode\": \"truncate|append|nonConflict\",\n          \"fieldDelimiter\": \",\",\n          \"encoding\": \"UTF-8\",\n          \"nullFormat\": \"null\",\n          \"dateFormat\": \"yyyy-MM-dd\",\n          \"fileFormat\": \"csv\",\n          \"useKey\": false,\n          \"keyPath\": \"\",\n          \"keyPass\": \"\",\n          \"header\": []\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/ftpwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description protocol Yes string <code>ftp</code> Server protocol, currently supports ftp and sftp transport protocols host Yes string None Server address port No int 22/21 FTP default is 21, SFTP default is 22 timeout No int <code>60000</code> Connection timeout for FTP server, in milliseconds (ms) connectPattern No string <code>PASV</code> Connection mode, only supports <code>PORT</code>, <code>PASV</code> modes. Used for FTP protocol username Yes string None Username password Yes string None Access password useKey No boolean false Whether to use private key login, only valid for SFTP login keyPath No string <code>~/.ssh/id_rsa</code> Private key address keyPass No string None Private key password, no need to configure if no private key password is set path Yes string None Remote FTP file system path information, FtpWriter will write multiple files under Path directory fileName Yes string None Name of file to write, this filename will have random suffix added as actual filename for each thread writeMode Yes string None Data cleanup processing mode before writing, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading"},{"location":"en/writer/greenplumwriter/","title":"GreenPlum Writer","text":"<p>GreenPlum Writer plugin implements writing data to GreenPlum database.</p>"},{"location":"en/writer/greenplumwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to GreenPlum database. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/hanawriter/","title":"HANA Writer","text":"<p>HANA Writer plugin implements the functionality of writing data to SAP HANA destination tables.</p>"},{"location":"en/writer/hanawriter/#example","title":"Example","text":"<p>Assume the HANA table to be written has the following DDL statement:</p> <pre><code>create table system.addax_tbl\n(\ncol1 varchar(200) ,\ncol2 int(4),\ncol3 date,\ncol4 boolean,\ncol5 clob\n);\n</code></pre> <p>Here we use data generated from memory to import into HANA.</p> job/hanawriter.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"hanawriter\",\n        \"parameter\": {\n          \"username\": \"system\",\n          \"password\": \"HXEHana1\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hana2stream.json</code></p>"},{"location":"en/writer/hanawriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"en/writer/hanawriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/hbase11xsqlwriter/","title":"HBase11x SQL Writer","text":"<p>HBase11x SQL Writer plugin implements writing data to HBase 1.x via Phoenix SQL interface.</p>"},{"location":"en/writer/hbase11xsqlwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 1.x via Phoenix. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p> <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase11xsqlwriter\",\n        \"parameter\": {\n          \"batchSize\": \"256\",\n          \"column\": [\n            \"UID\",\n            \"TS\",\n            \"EVENTID\",\n            \"CONTENT\"\n          ],\n          \"haveKerberos\": \"true\",\n          \"kerberosPrincipal\": \"hive@EXAMPLE.COM\",\n          \"kerberosKeytabFilePath\": \"/tmp/hive.headless.keytab\",\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"node1,node2,node3:2181\",\n            \"zookeeper.znode.parent\": \"/hbase-secure\"\n          },\n          \"nullMode\": \"skip\",\n          \"table\": \"TEST_TBL\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/hbase11xwriter/","title":"HBase11x Writer","text":"<p>HBase11x Writer plugin implements writing data to HBase 1.x version.</p>"},{"location":"en/writer/hbase11xwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 1.x database. For detailed configuration and parameters, please refer to the original HBase11x Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase11xwriter\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"***\"\n          },\n          \"table\": \"writer\",\n          \"mode\": \"normal\",\n          \"rowkeyColumn\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": -1,\n              \"type\": \"string\",\n              \"value\": \"_\"\n            }\n          ],\n          \"column\": [\n            {\n              \"index\": 1,\n              \"name\": \"cf1:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"name\": \"cf1:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"name\": \"cf1:q3\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"name\": \"cf2:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"name\": \"cf2:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"name\": \"cf2:q3\",\n              \"type\": \"string\"\n            }\n          ],\n          \"versionColumn\": {\n            \"index\": -1,\n            \"value\": \"123456789\"\n          },\n          \"encoding\": \"utf-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/hbase11xwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to HBase 1.x with configurable connection, table, and row key options.</p>"},{"location":"en/writer/hbase20xsqlwriter/","title":"HBase20x SQL Writer","text":"<p>HBase20x SQL Writer plugin implements writing data to HBase 2.x via Phoenix SQL interface.</p>"},{"location":"en/writer/hbase20xsqlwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 2.x via Phoenix. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p> <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase20xsqlwriter\",\n        \"parameter\": {\n          \"batchSize\": \"100\",\n          \"column\": [\n            \"UID\",\n            \"TS\",\n            \"EVENTID\",\n            \"CONTENT\"\n          ],\n          \"queryServerAddress\": \"http://127.0.0.1:8765\",\n          \"nullMode\": \"skip\",\n          \"table\": \"TEST_TBL\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/hdfswriter/","title":"HDFS Writer","text":"<p>HDFS Writer provides the ability to write files in formats like <code>TextFile</code>, <code>ORCFile</code>, <code>Parquet</code> etc. to specified paths in HDFS file system. File content can be associated with tables in Hive.</p>"},{"location":"en/writer/hdfswriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"['tag1', 'tag2', 'tag3']\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"{'loc':'HZ','num':'12'}\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        },\n        \"writer\": {\n          \"name\": \"hdfswriter\",\n          \"parameter\": {\n            \"defaultFS\": \"hdfs://xxx:port\",\n            \"fileType\": \"orc\",\n            \"path\": \"/user/hive/warehouse/writerorc.db/orcfull\",\n            \"fileName\": \"xxxx\",\n            \"column\": [\n              {\n                \"name\": \"col1\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col2\",\n                \"type\": \"int\"\n              },\n              {\n                \"name\": \"col3\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col4\",\n                \"type\": \"boolean\"\n              },\n              {\n                \"name\": \"col5\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col6\",\n                \"type\": \"array&lt;string&gt;\"\n              },\n              {\n                \"name\": \"col7\",\n                \"type\": \"map&lt;string,string&gt;\"\n              }\n            ],\n            \"writeMode\": \"overwrite\",\n            \"fieldDelimiter\": \"\\u0001\",\n            \"compress\": \"SNAPPY\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/hdfswriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File path to read defaultFS Yes string None Detailed description below fileType Yes string None File type, detailed description below fileName Yes string None Filename to write, used as prefix column Yes <code>list&lt;map&gt;</code> None List of fields to write writeMode Yes string None Write mode, detailed description below skipTrash No boolean false Whether to skip trash, related to <code>writeMode</code> configuration fieldDelimiter No string <code>,</code> Field delimiter for text files, not needed for binary files encoding No string <code>utf-8</code> File encoding configuration, currently only supports <code>utf-8</code> nullFormat No string None Define characters representing null, e.g. if user configures <code>\"\\\\N\"</code>, then if source data is <code>\"\\N\"</code>, treat as <code>null</code> field haveKerberos No boolean false Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Credential file path for Kerberos authentication, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Credential principal for Kerberos authentication, e.g. <code>addax/node1@WGZHAO.COM</code> compress No string None File compression format, see below hadoopConfig No map None Can configure some advanced parameters related to Hadoop, such as HA configuration preShell No <code>list</code> None Shell commands to execute before writing data, e.g. <code>hive -e \"truncate table test.hello\"</code>"},{"location":"en/writer/icebergwriter/","title":"Iceberg Writer","text":"<p>Iceberg Writer plugin implements writing data to Apache Iceberg.</p>"},{"location":"en/writer/icebergwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Iceberg tables. For detailed configuration and parameters, please refer to the original Iceberg Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"streamreader\",\n          \"parameter\": {\n            \"column\": [\n              {\n                \"value\": \"1\",\n                \"type\": \"long\"\n              },\n              {\n                \"value\": \"1989-06-04 00:00:00\",\n                \"type\": \"timestamp\"\n              },\n              {\n                \"value\": \"test1\",\n                \"type\": \"string\"\n              }\n            ],\n            \"sliceRecordCount\": 1000\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test.test1\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\": \"hadoop\",\n            \"warehouse\": \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/iceberg\",\n            \"hadoopConfig\": {\n              \"fs.s3a.endpoint\": \"http://localhost:9000\",\n              \"fs.s3a.access.key\": \"gy0dX5lALP176g6c9fYf\",\n              \"fs.s3a.secret.key\": \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\",\n              \"fs.s3a.connection.ssl.enabled\": \"false\",\n              \"fs.s3a.path.style.access\": \"true\",\n              \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/writer/icebergwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Iceberg with configurable catalog, table, and partition options.</p>"},{"location":"en/writer/influxdb2writer/","title":"InfluxDB2 Writer","text":"<p>InfluxDB2 Writer plugin implements writing data to InfluxDB 2.0 and above versions.</p>"},{"location":"en/writer/influxdb2writer/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to InfluxDB 2.0+ database. For detailed configuration and parameters, please refer to the original InfluxDB2 Writer documentation.</p>"},{"location":"en/writer/influxdb2writer/#parameters","title":"Parameters","text":"<p>This plugin supports writing time series data to InfluxDB 2.0+ with token-based authentication and organization/bucket structure.</p>"},{"location":"en/writer/influxdbwriter/","title":"InfluxDB Writer","text":"<p>InfluxDB Writer plugin implements writing data to InfluxDB time series database.</p>"},{"location":"en/writer/influxdbwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to InfluxDB database. For detailed configuration and parameters, please refer to the original InfluxDB Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"2001-01-01 00:00:00, 2016-07-07 23:59:59\",\n              \"type\": \"date\"\n            },\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"influxdbwriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"addax\",\n            \"table\": \"addax_tbl\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\",\n          \"column\": [\n            {\n              \"name\": \"time\",\n              \"type\": \"timestamp\"\n            },\n            {\n              \"name\": \"user_id\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"user_name\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"salary\",\n              \"type\": \"double\"\n            }\n          ],\n          \"preSql\": [\n            \"delete from addax_tbl\"\n          ],\n          \"batchSize\": 1024,\n          \"retentionPolicy\": {\n            \"name\": \"one_day_only\",\n            \"duration\": \"1d\",\n            \"replication\": 1\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/influxdbwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing time series data to InfluxDB with configurable database, measurement, and field options.</p>"},{"location":"en/writer/kafkawriter/","title":"Kafka Writer","text":"<p>Kafka Writer plugin implements the functionality of writing data to Kafka in JSON format.</p>"},{"location":"en/writer/kafkawriter/#example","title":"Example","text":"<p>The following configuration demonstrates how to read data from memory and write to specified topic in Kafka.</p>"},{"location":"en/writer/kafkawriter/#create-task-file","title":"Create Task File","text":"<p>First create a task file <code>stream2kafka.json</code> with the following content:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n        \"speed\": {\n            \"channel\": 1\n        }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n            \"name\": \"streamreader\",\n            \"parameter\": {\n              \"column\": [\n                    {\"random\": \"10,1000\", \"type\": \"long\"},\n                    {\"value\": \"1.1.1.1\", \"type\": \"string\"},\n                    {\"value\": 19890604.0, \"type\": \"double\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": \"hello world\", \"type\": \"string\"},\n                    {\"value\": \"long text\", \"type\": \"string\"},\n                    {\"value\": \"41.12,-71.34\", \"type\": \"string\"},\n                    {\"value\": \"2017-05-25 11:22:33\", \"type\": \"string\"}\n                    ],\n            \"sliceRecordCount\": 100\n            }\n        },\n        \"writer\": {\n          \"name\": \"kafkawriter\",\n          \"parameter\": {\n            \"brokerList\": \"localhost:9092\",\n            \"topic\": \"test-1\",\n            \"partitions\": 0,\n            \"batchSize\": 1000,\n            \"column\": [\"col1\", \"col2\",\"col3\",\"col4\",\"col5\", \"col6\", \"col7\", \"col8\", \"col9\"]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/writer/kafkawriter/#run","title":"Run","text":"<p>Execute <code>bin/addax.sh stream2kafka.json</code> command to get output similar to the following:</p> <pre><code>2022-02-26 21:59:22.975 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2022-02-26 21:59:22.985 [        main] INFO  Engine               - \n{\n    \"content\":{\n        \"reader\":{\n            \"parameter\":{\n                \"column\":[\n                    {\n                        \"random\":\"10,1000\",\n                        \"type\":\"long\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"1.1.1.1\"\n                    },\n                    {\n                        \"type\":\"double\",\n                        \"value\":19890604.0\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"hello world\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"long text\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"41.12,-71.34\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"2017-05-25 11:22:33\"\n                    }\n                ],\n                \"sliceRecordCount\":100\n            },\n            \"name\":\"streamreader\"\n        },\n        \"writer\":{\n            \"parameter\":{\n                \"partitions\":0,\n                \"column\":[\n                    \"col1\",\n                    \"col2\",\n                    \"col3\",\n                    \"col4\",\n                    \"col5\",\n                    \"col6\",\n                    \"col7\",\n                    \"col8\",\n                    \"col9\"\n                ],\n                \"topic\":\"test-1\",\n                \"batchSize\":1000,\n                \"brokerList\":\"localhost:9092\"\n            },\n            \"name\":\"kafkawriter\"\n        }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"channel\":1\n        }\n    }\n}\n\n2022-02-26 21:59:23.002 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2022-02-26 21:59:23.003 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2022-02-26 21:59:23.004 [        main] INFO  JobContainer         - Set jobId = 0\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channel(s).\n2022-02-26 21:59:23.018 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2022-02-26 21:59:23.019 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] splits to [1] tasks.\n2022-02-26 21:59:23.039 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2022-02-26 21:59:23.047 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2022-02-26 21:59:23.082 [0-0-0-writer] INFO  ProducerConfig       - ProducerConfig values: \n    acks = 1\n    batch.size = 1000\n    bootstrap.servers = [localhost:9092]\n    buffer.memory = 33554432\n    client.id = addax-kafka-writer\n    compression.type = none\n    connections.max.idle.ms = 540000\n    enable.idempotence = false\n    interceptor.classes = []\n    key.serializer = class org.apache.kafka.common.serialization.StringSerializer\n    linger.ms = 0\n    max.block.ms = 60000\n    max.in.flight.requests.per.connection = 5\n    max.request.size = 1048576\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n    partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\n    receive.buffer.bytes = 32768\n    reconnect.backoff.max.ms = 1000\n    reconnect.backoff.ms = 50\n    request.timeout.ms = 30000\n    retries = 0\n    retry.backoff.ms = 100\n    sasl.client.callback.handler.class = null\n    sasl.jaas.config = null\n    sasl.kerberos.kinit.cmd = /usr/bin/kinit\n    sasl.kerberos.min.time.before.relogin = 60000\n    sasl.kerberos.service.name = null\n    sasl.kerberos.ticket.renew.jitter = 0.05\n    sasl.kerberos.ticket.renew.window.factor = 0.8\n    sasl.login.callback.handler.class = null\n    sasl.login.class = null\n    sasl.login.refresh.buffer.seconds = 300\n    sasl.login.refresh.min.period.seconds = 60\n    sasl.login.refresh.window.factor = 0.8\n    sasl.login.refresh.window.jitter = 0.05\n    sasl.mechanism = GSSAPI\n    security.protocol = PLAINTEXT\n    send.buffer.bytes = 131072\n    ssl.cipher.suites = null\n    ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n    ssl.endpoint.identification.algorithm = https\n    ssl.key.password = null\n    ssl.keymanager.algorithm = SunX509\n    ssl.keystore.location = null\n    ssl.keystore.password = null\n    ssl.keystore.type = JKS\n    ssl.protocol = TLS\n    ssl.provider = null\n    ssl.secure.random.implementation = null\n    ssl.trustmanager.algorithm = PKIX\n    ssl.truststore.location = null\n    ssl.truststore.password = null\n    ssl.truststore.type = JKS\n    transaction.timeout.ms = 60000\n    transactional.id = null\n    value.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\n2022-02-26 21:59:23.412 [0-0-0-writer] INFO  AppInfoParser        - Kafka version : 2.0.0\n2022-02-26 21:59:23.413 [0-0-0-writer] INFO  AppInfoParser        - Kafka commitId : 3402a8361b734732\n2022-02-26 21:59:23.534 [kafka-producer-network-thread | addax-kafka-writer] INFO  Metadata             - Cluster ID: xPAQZFNDTp6y63nZO4LACA\n2022-02-26 21:59:26.061 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do post work.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2022-02-26 21:59:26.063 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2022-02-26 21:59:26.064 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 100 records, 9200 bytes | Speed 2.99KB/s, 33 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2022-02-26 21:59:26.065 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2022-02-26 21:59:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2022-02-26 21:59:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.99KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre> <p>We use Kafka's built-in <code>kafka-console-consumer.sh</code> to try reading data, output as follows:</p> <pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-1 --from-beginning\n\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":916}\n</code></pre>"},{"location":"en/writer/kuduwriter/","title":"Kudu Writer","text":"<p>Kudu Writer plugin implements writing data to Apache Kudu.</p>"},{"location":"en/writer/kuduwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Kudu database. For detailed configuration and parameters, please refer to the original Kudu Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"kuduwriter\",\n        \"parameter\": {\n          \"masterAddress\": \"127.0.0.1:7051,127.0.0.1:7151,127.0.0.1:7251\",\n          \"timeout\": 60,\n          \"table\": \"users\",\n          \"writeMode\": \"upsert\",\n          \"column\": [ \"user_id\", \"user_name\", \"salary\"],\n          \"batchSize\": 1024,\n          \"bufferSize\": 2048,\n          \"skipFail\": false,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/kuduwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Kudu with configurable master addresses, table operations, and data consistency options.</p>"},{"location":"en/writer/mongodbwriter/","title":"MongoDB Writer","text":"<p>MongoDB Writer plugin implements writing data to MongoDB.</p>"},{"location":"en/writer/mongodbwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to MongoDB database. For detailed configuration and parameters, please refer to the original MongoDB Writer documentation.</p>"},{"location":"en/writer/mongodbwriter/#parameters","title":"Parameters","text":"<p>This plugin provides comprehensive MongoDB writing capabilities with support for various data types and connection options.</p>"},{"location":"en/writer/mysqlwriter/","title":"MySQL Writer","text":"<p>MySQL Writer plugin implements the functionality of writing data to MySQL destination tables.</p>"},{"location":"en/writer/mysqlwriter/#example","title":"Example","text":"<p>Assume the MySQL table to be written has the following DDL statement:</p> <pre><code>create table test.addax_tbl\n(\n  col1 varchar(20) ,\n  col2 int(4),\n  col3 datetime,\n  col4 boolean,\n  col5 binary\n) default charset utf8;\n</code></pre> <p>Here we use data generated from memory to import into MySQL.</p> job/stream2mysql.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"mysqlwriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"session\": [\n            \"set session sql_mode='ANSI'\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2mysql.json</code></p>"},{"location":"en/writer/mysqlwriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2mysql.json\n</code></pre>"},{"location":"en/writer/mysqlwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds some MySQL-specific configuration items.</p> Configuration Required Type Default Value Description writeMode Yes string insert The way data is written to the table, see below batchSize No int 1024 Defines the number of batch data fetched between plugin and database server each time"},{"location":"en/writer/mysqlwriter/#driver","title":"driver","text":"<p>The current MySQL JDBC driver uses version 8.0 and above, with driver class name <code>com.mysql.cj.jdbc.Driver</code>, not <code>com.mysql.jdbc.Driver</code>. If you need to collect from MySQL server below <code>5.6</code> and need to use <code>Connector/J 5.1</code> driver, you can take the following steps:</p> <ol> <li> <p>Replace the built-in driver in the plugin   <code>rm -f plugin/writer/mysqlwriter/libs/mysql-connector-java-*.jar</code></p> </li> <li> <p>Copy the old driver to the plugin directory   <code>cp mysql-connector-java-5.1.48.jar plugin/writer/mysqlwriter/libs/</code></p> </li> <li> <p>Specify driver class name   In your json file, configure <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p> </li> </ol>"},{"location":"en/writer/mysqlwriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> means using <code>insert into</code></li> <li><code>replace</code> means using <code>replace into</code> method</li> <li><code>update</code> means using <code>ON DUPLICATE KEY UPDATE</code> statement</li> </ul>"},{"location":"en/writer/oraclewriter/","title":"Oracle Writer","text":"<p>Oracle Writer plugin implements the functionality of writing data to Oracle destination tables.</p>"},{"location":"en/writer/oraclewriter/#configuration-example","title":"Configuration Example","text":"<p>Here we use data generated from memory to import into Oracle.</p> job/stream2oracle.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"oraclewriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"preSql\": [\n            \"delete from test\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/oraclewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds some OracleWriter-specific configuration items.</p> Configuration Required Default Value Description writeMode No insert Write mode, supports insert, update, see below"},{"location":"en/writer/oraclewriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to Oracle tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, which is Oracle's <code>merge into</code> syntax, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre>"},{"location":"en/writer/paimonwriter/","title":"Paimon Writer","text":"<p>Paimon Writer plugin implements writing data to Apache Paimon.</p>"},{"location":"en/writer/paimonwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Paimon tables. For detailed configuration and parameters, please refer to the original Paimon Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id ,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\",]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"paimonwriter\",\n          \"parameter\": {\n            \"dbName\": \"test\",\n            \"tableName\": \"test2\",\n            \"writeMode\": \"truncate\",\n            \"paimonConfig\": {\n              \"warehouse\": \"file:///g:/paimon\",\n              \"metastore\": \"filesystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"en/writer/paimonwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Paimon with configurable catalog, table, and schema options.</p>"},{"location":"en/writer/postgresqlwriter/","title":"Postgresql Writer","text":"<p>Postgresql Writer plugin implements the functionality of writing data to PostgreSQL database tables.</p>"},{"location":"en/writer/postgresqlwriter/#example","title":"Example","text":"<p>The following configuration demonstrates reading data from a specified PostgreSQL table and inserting it into another table with the same table structure, to test the data types supported by this plugin.</p>"},{"location":"en/writer/postgresqlwriter/#table-structure-information","title":"Table Structure Information","text":"<p>Assume the table creation statement and input insertion statement are as follows:</p> <pre><code>create table if not exists addax_tbl\n(\n    c_bigint bigint,\n    c_bit bit(3),\n    c_bool boolean,\n    c_byte bytea,\n    c_char char(10),\n    c_varchar varchar(20),\n    c_date date,\n    c_double float8,\n    c_int integer,\n    c_json json,\n    c_number decimal(8, 3),\n    c_real real,\n    c_small smallint,\n    c_text text,\n    c_ts timestamp,\n    c_uuid uuid,\n    c_xml xml,\n    c_money money,\n    c_inet inet,\n    c_cidr cidr,\n    c_macaddr macaddr\n    );\n\ninsert into addax_tbl\nvalues (999988887777,\n        b'101',\n        TRUE,\n        '\\xDEADBEEF',\n        'hello',\n        'hello, world',\n        '2021-01-04',\n        999888.9972,\n        9876542,\n        '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'::json,\n        12345.123,\n        123.123,\n        126,\n        'this is a long text ',\n        '2020-01-04 12:13:14',\n        'A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'::uuid,\n        '&lt;foo&gt;bar&lt;/foo&gt;'::xml,\n        '52093.89'::money,\n        '192.168.1.1'::inet,\n        '192.168.1/24'::cidr,\n        '08002b:010203'::macaddr);\n</code></pre> <p>The statement to create the table to be inserted is as follows:</p> <pre><code>create table addax_tbl1 as select * from  addax_tbl where 1=2;\n</code></pre>"},{"location":"en/writer/postgresqlwriter/#task-configuration","title":"Task Configuration","text":"<p>The following is the configuration file</p> job/pg2pg.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/pg2pg.json</code></p>"},{"location":"en/writer/postgresqlwriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/pg2pg.json\n</code></pre>"},{"location":"en/writer/postgresqlwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/postgresqlwriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to PostgreSQL tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre> <p>Note: <code>update</code> mode was first added in version <code>3.1.6</code>, previous versions do not support it.</p>"},{"location":"en/writer/postgresqlwriter/#type-conversion","title":"Type Conversion","text":"<p>Currently PostgresqlWriter supports most PostgreSQL types, but there are also some cases that are not supported. Please check your types carefully.</p> <p>The following lists PostgresqlWriter's type conversion list for PostgreSQL:</p> Addax Internal Type PostgreSQL Data Type Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit, inet,cidr,macaddr,uuid,xml,json Date date, time, timestamp Boolean bool Bytes bytea"},{"location":"en/writer/postgresqlwriter/#known-limitations","title":"Known Limitations","text":"<p>Except for the data types listed above, other data types are theoretically converted to string type, but accuracy is not guaranteed.</p>"},{"location":"en/writer/rdbmswriter/","title":"RDBMS Writer","text":"<p>RDBMS Writer plugin supports writing data to traditional RDBMS. This is a generic relational database writer plugin that can support more relational database writing by registering database drivers.</p> <p>At the same time, RDBMS Writer is also the base class for other relational database writer plugins. The following writer plugins all depend on this plugin:</p> <ul> <li>Oracle Writer</li> <li>MySQL Writer</li> <li>PostgreSQL Writer</li> <li>ClickHouse Writer</li> <li>SQLServer Writer</li> <li>Access Writer</li> <li>Databend Writer</li> </ul> <p>Note: If a dedicated database writer plugin is already provided, it is recommended to use the dedicated plugin. If the database you need to write to does not have a dedicated plugin, consider using this generic plugin. Before use, you need to perform the following operations to run normally, otherwise exceptions will occur.</p>"},{"location":"en/writer/rdbmswriter/#configure-driver","title":"Configure Driver","text":"<p>Suppose you need to write data to IBM DB2. Since no dedicated writer plugin is provided, we can use this plugin to implement it. Before use, you need to perform the following two operations:</p> <ol> <li>Download the corresponding JDBC driver and copy it to the <code>plugin/writer/rdbmswriter/libs</code> directory</li> <li>Modify the task configuration file, find the <code>driver</code> item, and fill in the correct JDBC driver name, such as DB2's driver name <code>com.ibm.db2.jcc.DB2Driver</code></li> </ol> <p>The following lists common databases and their corresponding driver names:</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"en/writer/rdbmswriter/#configuration","title":"Configuration","text":"<p>Configure a job to write to RDBMS.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"rdbmswriter\",\n        \"parameter\": {\n          \"username\": \"username\",\n          \"password\": \"password\",\n          \"driver\": \"dm.jdbc.driver.DmDriver\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from XXX;\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:dm://ip:port/database\",\n            \"table\": [\n              \"table\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/rdbmswriter/#parameters","title":"Parameters","text":"<p>This plugin provides configuration for writing to relational databases. For detailed parameter descriptions, please refer to the original RDBMS Writer documentation.</p>"},{"location":"en/writer/rediswriter/","title":"Redis Writer","text":"<p>Redis Writer plugin is used to write data to Redis database.</p>"},{"location":"en/writer/rediswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin provides the ability to write data to Redis database. For detailed configuration and parameters, please refer to the original Redis Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"uri\": \"tcp://127.0.0.1:7003\"\n            }\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"redisCluster\": false,\n          \"flushDB\": false\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/rediswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing various data types to Redis with configurable connection and data format options.</p>"},{"location":"en/writer/s3writer/","title":"S3 Writer","text":"<p>S3 Writer plugin is used to write data to Amazon AWS S3 storage, as well as S3 protocol compatible storage, such as MinIO.</p> <p>In implementation, this plugin is written based on S3's official SDK 2.0.</p>"},{"location":"en/writer/s3writer/#configuration-example","title":"Configuration Example","text":"<p>The following configuration is used to read data from memory and write to specified S3 bucket.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"s3writer\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": \"upload.csv\",\n          \"region\": \"ap-northeast-1\",\n          \"encoding\": \"\",\n          \"fieldDelimiter\": \",\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/s3writer/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None S3 Server EndPoint address, e.g. <code>s3.xx.amazonaws.com</code> region Yes string None S3 Server Region address, e.g. <code>ap-southeast-1</code> accessId Yes string None Access ID accessKey Yes string None Access Key bucket Yes string None Bucket to write to object Yes string None Object to write to, see notes below fieldDelimiter No char <code>','</code> Field delimiter nullFormat No char <code>\\N</code> What character to use when value is null header No list None Write file header information, e.g. <code>[\"id\",\"title\",\"url\"]</code> maxFileSize No int <code>100000</code> Size of single object, in MB encoding No string <code>utf-8</code> File encoding format"},{"location":"en/writer/sqlitewriter/","title":"SQLite Writer","text":"<p>SQLite Writer plugin implements the functionality of writing data to SQLite database.</p>"},{"location":"en/writer/sqlitewriter/#example","title":"Example","text":"<p>Assume the table to be written is as follows:</p> <pre><code>create table addax_tbl\n(\n    col1 varchar(20) ,\n    col2 int(4),\n    col3 datetime,\n    col4 boolean,\n    col5 binary\n);\n</code></pre> <p>Here we use data generated from memory to SQLite.</p> job/stream2sqlite.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"sqlitewriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite://tmp/writer.sqlite3\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2sqlite.json</code></p>"},{"location":"en/writer/sqlitewriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2sqlite.json\n</code></pre>"},{"location":"en/writer/sqlitewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer. Since SQLite connection does not require username and password, the <code>username</code> and <code>password</code> that other database writer plugins need to configure are not needed here.</p>"},{"location":"en/writer/sqlitewriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> means using <code>insert into</code></li> <li><code>replace</code> means using <code>replace into</code> method</li> <li><code>update</code> means using <code>ON DUPLICATE KEY UPDATE</code> statement</li> </ul>"},{"location":"en/writer/sqlitewriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type SQLite Data Type Long integer Double real String varchar Date datetime Boolean bool Bytes blob, binary"},{"location":"en/writer/sqlserverwriter/","title":"SQLServer Writer","text":"<p>SQLServer Writer plugin implements the functionality of writing data to SQL Server database tables.</p>"},{"location":"en/writer/sqlserverwriter/#configuration-example","title":"Configuration Example","text":"<p>Here we use data generated from memory to import into SQL Server.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"sqlserverwriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"db_id\",\n            \"db_type\",\n            \"db_ip\",\n            \"db_port\",\n            \"db_role\",\n            \"db_name\",\n            \"db_username\",\n            \"db_password\",\n            \"db_modify_time\",\n            \"db_modify_user\",\n            \"db_description\",\n            \"db_tddl_info\"\n          ],\n          \"preSql\": [\n            \"delete from @table where db_id = -1;\"\n          ],\n          \"postSql\": [\n            \"update @table set db_modify_time = now() where db_id = 1;\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"db_info_for_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/sqlserverwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/sqlserverwriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to SQL Server tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, which is SQL Server's <code>MERGE INTO</code> syntax, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>{\n  \"writeMode\": \"update(id)\"\n}\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>{\n  \"writeMode\": \"update(col1, col2)\"\n}\n</code></pre>"},{"location":"en/writer/starrockswriter/","title":"StarRocks Writer","text":"<p>StarRocks Writer plugin implements writing data to StarRocks.</p>"},{"location":"en/writer/starrockswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to StarRocks database. For detailed configuration and parameters, please refer to the original StarRocks Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"starrockswriter\",\n        \"parameter\": {\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"database\": \"example_db\",\n          \"table\": \"table1\",\n          \"jdbcUrl\": \"jdbc:mysql://172.28.17.100:9030/\",\n          \"loadUrl\": [\n            \"172.28.17.100:8030\",\n            \"172.28.17.100:8030\"\n          ],\n          \"loadProps\": {\n            \"column_separator\": \"\\\\x01\",\n            \"row_delimiter\": \"\\\\x02\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/starrockswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to StarRocks with Stream Load capabilities and configurable connection options.</p>"},{"location":"en/writer/streamwriter/","title":"Stream Writer","text":"<p>Stream Writer is a plugin that writes data to memory, generally used to write acquired data to terminal for debugging data processing of read plugins.</p> <p>A typical Stream Writer configuration is as follows:</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"print\": true,\n    \"nullFormat\": \"NULL\"\n  }\n}\n</code></pre> <p>The above configuration will print the acquired data directly to terminal. Where <code>nullFormat</code> is used to specify how to represent null values in terminal, default is string <code>NULL</code>. If you don't want to print null values, you can set it to <code>\"\"</code>.</p> <p>This plugin also supports writing data to files, configured as follows:</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"path\": \"/tmp/out\",\n    \"fileName\": \"out.txt\",\n    \"fieldDelimiter\": \",\",\n    \"recordNumBeforeSleep\": \"100\",\n    \"sleepTime\": \"5\"\n  }\n}\n</code></pre> <p>In the above configuration:</p> <ul> <li><code>fieldDelimiter</code> represents field delimiter, default is tab character (<code>\\t</code>)</li> <li><code>recordNumBeforeSleep</code> represents how many records to acquire before executing sleep, default is 0, meaning this feature is disabled</li> <li><code>sleepTime</code> represents how long to sleep, in seconds, default is 0, meaning this feature is disabled</li> </ul> <p>The meaning of the above configuration is to write data to <code>/tmp/out/out.txt</code> file, sleep for 5 seconds after acquiring every 100 records.</p>"},{"location":"en/writer/sybasewriter/","title":"Sybase Writer","text":"<p>Sybase Writer plugin implements the functionality of writing data to Sybase database tables.</p>"},{"location":"en/writer/sybasewriter/#configuration-example","title":"Configuration Example","text":"<p>We can use Docker container to start a Sybase database</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>Then create a table as follows:</p> <pre><code>create table addax_writer \n(\n    id int,\n    name varchar(255),\n    salary float(2),\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>Then use the following task configuration file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"100,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"10,100\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"double\"\n            },\n            {\n              \"incr\": \"2022-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            },\n            {\n              \"incr\": \"2023-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"sybasewriter\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\",\n            \"salary\",\n            \"created_at\",\n            \"updated_at\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.addax_writer\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/sybasewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/tdenginewriter/","title":"TDengine Writer","text":"<p>TDengine Writer plugin implements writing data to TDengine database system. In the underlying implementation, TDengine Writer connects to remote TDengine database through JDBC JNI driver and executes corresponding SQL statements to batch write data to TDengine database.</p>"},{"location":"en/writer/tdenginewriter/#prerequisites","title":"Prerequisites","text":"<p>Considering performance issues, this plugin uses TDengine's JDBC-JNI driver, which directly calls the client API (<code>libtaos.so</code> or <code>taos.dll</code>) to send write and query requests to <code>taosd</code> instances. Therefore, dynamic library link files need to be configured before use.</p> <p>First copy <code>plugin/writer/tdenginewriter/libs/libtaos.so.2.0.16.0</code> to <code>/usr/lib64</code> directory, then execute the following commands to create soft links:</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"en/writer/tdenginewriter/#example","title":"Example","text":"<p>Assume the table to be written is as follows:</p> <pre><code>create table test.addax_test (\n    ts timestamp,\n    name nchar(100),\n    file_size int,\n    file_date timestamp,\n    flag_open bool,\n    memo nchar(100)\n);\n</code></pre> <p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"en/writer/txtfilewriter/","title":"TxtFile Writer","text":"<p>TxtFile Writer provides writing CSV-like format to one or more table files in local file system.</p>"},{"location":"en/writer/txtfilewriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/data\"\n          ],\n          \"encoding\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/result\",\n          \"fileName\": \"luohw\",\n          \"writeMode\": \"truncate\",\n          \"dateFormat\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"en/writer/txtfilewriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None Path information of local file system, write multiple files under Path directory fileName Yes string None Name of file to write, this filename will have random suffix added as actual filename for each thread writeMode Yes string None Data cleanup processing mode before writing, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, supports <code>zip</code>, <code>lzo</code>, <code>lzop</code>, <code>tgz</code>, <code>bzip2</code> encoding No string utf-8 Encoding configuration for reading files nullFormat No string <code>\\N</code> Define which strings can represent null dateFormat No string None Format when date type data is serialized to file, e.g. <code>\"yyyy-MM-dd\"</code> fileFormat No string text Format of file output, see below table Yes string None Table name to specify in SQL mode column No list None Optional column names to specify in SQL mode extendedInsert No boolean true Whether to use batch insert syntax in SQL mode, see below batchSize No int 2048 Batch size for batch insert syntax in SQL mode, see below header No list None Table header for text output, example <code>['id', 'name', 'age']</code>"},{"location":"en/writer/txtfilewriter/#writemode","title":"writeMode","text":"<p>Data cleanup processing mode before writing:</p> <ul> <li>truncate: Clean all files with fileName prefix under directory before writing.</li> <li>append: No processing before writing, write directly using filename and ensure no filename conflicts.</li> <li>nonConflict: If there are files with fileName prefix under directory, report error directly.</li> </ul>"},{"location":"en/writer/txtfilewriter/#fileformat","title":"fileFormat","text":"<p>Format of file output, including csv, text, and sql (introduced in version <code>4.1.3</code>). CSV is strict csv format, if data to be written includes column delimiter, it will be escaped according to csv escape syntax, with escape symbol being double quotes <code>\"</code>; text format simply separates data to be written with column delimiter, no escaping for data including column delimiter. SQL format means writing data to file in SQL statement (<code>INSERT INTO ... VALUES</code>) format.</p>"},{"location":"en/writer/txtfilewriter/#table","title":"table","text":"<p>Only required in sql file format, used to specify the table name to write to.</p>"},{"location":"en/writer/txtfilewriter/#column","title":"column","text":"<p>In sql file format, you can specify column names to write. If specified, the sql statement is like <code>INSERT INTO table (col1, col2, col3) VALUES (val1, val2, val3)</code>, otherwise it's <code>INSERT INTO table VALUES (val1, val2, val3)</code>.</p>"},{"location":"en/writer/txtfilewriter/#extendedinsert","title":"extendedInsert","text":"<p>Whether to enable batch insert syntax. If enabled, batchSize number of data will be written to file at once, otherwise each data is one line. This parameter borrows from the extended-insert parameter syntax of <code>mysqldump</code> tool.</p>"},{"location":"en/writer/txtfilewriter/#batchsize","title":"batchSize","text":"<p>Batch size for batch insert syntax. If extendedInsert is true, every batchSize data will be written to file at once, otherwise each data is one line.</p>"},{"location":"en/writer/txtfilewriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Local File Data Type Long Long Double Double string string Boolean Boolean Date Date"},{"location":"reader/accessreader/","title":"Access Reader","text":"<p>AccessReader \u5b9e\u73b0\u4e86\u4ece Access \u6570\u636e\u5e93\u4e0a\u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b\uff0c\u4ed6\u57fa\u4e8e Addax RDBMS Reader \u5b9e\u73b0\u3002</p>"},{"location":"reader/accessreader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u4e0b\u8f7d\u7528\u4e8e\u6d4b\u8bd5\u7528\u7684 AcessThemeDemo.zip \u6587\u4ef6\uff0c\u89e3\u538b\u540e\u5f97\u5230 <code>AccessThemeDemo.mdb</code> \u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u4e2d\u5305\u542b\u4e86\u4e00\u4e2a <code>tbl_Users</code> \u8868\uff0c\u6211\u4eec\u5c06\u8be5\u8868\u7684\u6570\u636e\u540c\u6b65\u5230\u7ec8\u7aef\u4e0a\u3002</p> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/access2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"accessreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"tbl_Users\"\n            ],\n            \"jdbcUrl\": \"jdbc:ucanaccess:///Users/wgzhao/Downloads/AccessThemeDemo.mdb\"\n          },\n          \"where\": \"\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"encoding\": \"utf-8\",\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/access2stream.json</code></p>"},{"location":"reader/accessreader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/access2stream.json\n</code></pre>"},{"location":"reader/accessreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>AccessReader \u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/cassandrareader/","title":"Cassandra Reader","text":"<p><code>CassandraReader</code> \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Cassandra \u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b\u3002</p>"},{"location":"reader/cassandrareader/#_1","title":"\u914d\u7f6e","text":"<p>\u4e0b\u9762\u662f\u914d\u7f6e\u4e00\u4e2a\u4ece Cassandra \u8bfb\u53d6\u6570\u636e\u5230\u7ec8\u7aef\u7684\u4f8b\u5b50</p> job/cassandra2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"cassandrareader\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"test\",\n          \"table\": \"addax_src\",\n          \"column\": [\n            \"textCol\",\n            \"blobCol\",\n            \"writetime(blobCol)\",\n            \"boolCol\",\n            \"smallintCol\",\n            \"tinyintCol\",\n            \"intCol\",\n            \"bigintCol\",\n            \"varintCol\",\n            \"floatCol\",\n            \"doubleCol\",\n            \"decimalCol\",\n            \"dateCol\",\n            \"timeCol\",\n            \"timeStampCol\",\n            \"uuidCol\",\n            \"inetCol\",\n            \"durationCol\",\n            \"listCol\",\n            \"mapCol\",\n            \"setCol\",\n            \"tupleCol\",\n            \"udtCol\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/cassandrareader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 host \u662f list \u65e0 \u8fde\u63a5\u7684\u57df\u540d\u6216 IP\uff0c\u591a\u4e2a\u8282\u70b9\u4e4b\u95f4\u7528\u9017\u53f7\u5206\u9694 port \u662f int 9042 \u7aef\u53e3 username \u5426 string \u65e0 \u7528\u6237\u540d password \u5426 string \u65e0 \u5bc6\u7801 useSSL \u5426 boolean false \u662f\u5426\u4f7f\u7528SSL\u8fde\u63a5 keyspace \u662f string \u65e0 \u9700\u8981\u540c\u6b65\u7684\u8868\u6240\u5728\u7684 keyspace table \u662f string \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u96c6\u5408,\u5176\u4e2d\u7684\u5143\u7d20\u53ef\u4ee5\u6307\u5b9a\u5217\u7684\u540d\u79f0\u6216 <code>writetime(column_name)</code>\uff0c\u540e\u4e00\u79cd\u5f62\u5f0f\u4f1a\u8bfb\u53d6<code>column_name</code>\u5217\u7684\u65f6\u95f4\u6233\u800c\u4e0d\u662f\u6570\u636e where \u5426 string \u65e0 \u6570\u636e\u7b5b\u9009\u6761\u4ef6\u7684 <code>cql</code> \u8868\u8fbe\u5f0f allowFiltering \u5426 boolean \u65e0 \u662f\u5426\u5728\u670d\u52a1\u7aef\u8fc7\u6ee4\u6570\u636e\uff0c\u8be6\u7ec6\u63cf\u8ff0\u53c2\u8003\u5b98\u65b9\u6587\u6863\u7684\u76f8\u5173\u63cf\u8ff0 consistencyLevel \u5426 string LOCAL_QUORUM \u6570\u636e\u4e00\u81f4\u6027\u7ea7\u522b, \u53ef\u9009 <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code>"},{"location":"reader/cassandrareader/#_3","title":"\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b","text":"<p>\u76ee\u524d\u652f\u6301\u9664 <code>counter</code> \u548c <code>Custom</code> \u7c7b\u578b\u4e4b\u5916\u7684\u6240\u6709\u7c7b\u578b\u3002</p> <p>\u4e0b\u9762\u5217\u51fa\u7c7b\u578b\u8f6c\u6362\u5217\u8868:</p> Addax \u5185\u90e8\u7c7b\u578b Cassandra \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint,varint,bigint,time,counter Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob"},{"location":"reader/clickhousereader/","title":"ClickHouse Reader","text":"<p><code>ClickHouseReader</code> \u63d2\u4ef6\u652f\u6301\u4ece ClickHouse\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u3002</p>"},{"location":"reader/clickhousereader/#_1","title":"\u793a\u4f8b","text":""},{"location":"reader/clickhousereader/#_2","title":"\u8868\u7ed3\u6784\u53ca\u6570\u636e\u4fe1\u606f","text":"<p>\u5047\u5b9a\u9700\u8981\u7684\u8bfb\u53d6\u7684\u8868\u7684\u7ed3\u6784\u4ee5\u53ca\u6570\u636e\u5982\u4e0b\uff1a</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre>"},{"location":"reader/clickhousereader/#json","title":"\u914d\u7f6e json \u6587\u4ef6","text":"<p>\u4e0b\u9762\u7684\u914d\u7f6e\u6587\u4ef6\u8868\u793a\u4ece ClickHouse \u6570\u636e\u5e93\u8bfb\u53d6\u6307\u5b9a\u7684\u8868\u6570\u636e\u5e76\u6253\u5370\u5230\u7ec8\u7aef</p> job/clickhouse2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/clickhouse2stream.json</code></p>"},{"location":"reader/clickhousereader/#_3","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/clickhouse2stream.json\n</code></pre> <p>\u5176\u8f93\u51fa\u4fe1\u606f\u5982\u4e0b\uff08\u5220\u9664\u4e86\u975e\u5173\u952e\u4fe1\u606f)</p> <pre><code>021-01-06 14:39:35.742 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n\n2021-01-06 14:39:35.767 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        \"*\"\n                    ],\n                    \"connection\":[\n                        {\n                            \"jdbcUrl\":[\n                                \"jdbc:clickhouse://127.0.0.1:8123/\"\n                            ],\n                            \"table\":[\n                                \"ck_addax\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"default\"\n                },\n                \"name\":\"clickhousereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n127 -32768  2147483647  -9223372036854775808    255 65535   4294967295  18446744073709551615    1   1   1234567891234567891234567891.1234567891Hello String 2c:16:db:a3:3a:4f   \n5f042a36-5b0c-4f71-adfd-4df4fca1b863    2021-01-01  2021-01-01 00:00:00 2021-01-01 00:00:00 hello\n\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-06 14:39:35\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-06 14:39:39\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               77B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              0rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   1\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/clickhousereader/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u53c2\u6570\u3002</p>"},{"location":"reader/clickhousereader/#_5","title":"\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b","text":"Addax \u5185\u90e8\u7c7b\u578b ClickHouse \u6570\u636e\u7c7b\u578b Long Uint8, Uint16, Uint32, Uint64, Int8, Int16, Int32, Int64, Enum8, Enum16 Double Float32, Float64, Decimal String String, FixedString(N) Date Date, DateTime, DateTime64 Boolean UInt8 Bytes String"},{"location":"reader/clickhousereader/#_6","title":"\u9650\u5236","text":"<p>\u9664\u4e0a\u8ff0\u7f57\u5217\u5b57\u6bb5\u7c7b\u578b\u5916\uff0c\u5176\u4ed6\u7c7b\u578b\u5747\u4e0d\u652f\u6301\uff0c\u5982 Array\u3001Nested \u7b49</p>"},{"location":"reader/databendreader/","title":"Databend Reader","text":"<p>DatabendReader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Databend \u8bfb\u53d6\u6570\u636e</p> <p>\u6ce8\u610f\uff0cdatabender \u6709\u517c\u5bb9 MySQL \u5ba2\u6237\u7aef\u7684\u534f\u8bae\u5b9e\u73b0\uff0c\u56e0\u6b64\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 MySQL Reader \u6765\u8bfb\u53d6 Databend \u6570\u636e\u3002</p>"},{"location":"reader/databendreader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u542f\u52a8 Databend \u6570\u636e\u5e93</p> <pre><code>docker run  -tid  --rm  -p 8000:8000 \\\n   -e QUERY_DEFAULT_USER=databend \\\n   -e QUERY_DEFAULT_PASSWORD=databend \\\n   datafuselabs/databend\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u4e00\u5f20\u9700\u8981\u8bfb\u53d6\u7684\u8868</p> <pre><code>(\n    id int,\n    name varchar(255),\n    salary float,\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>\u5e76\u586b\u5145\u5fc5\u8981\u7684\u6570\u636e</p> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/databend2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"databendreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://127.0.0.1:8000/default\",\n            \"table\": [\n              \"addax_reader\"\n            ]\n          },\n          \"username\": \"databend\",\n          \"password\": \"databend\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/databend2stream.json</code></p>"},{"location":"reader/databendreader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/databend2stream.json\n</code></pre>"},{"location":"reader/databendreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u53c2\u6570\u3002</p>"},{"location":"reader/databendreader/#_4","title":"\u9650\u5236","text":"<p>\u6682\u65e0</p>"},{"location":"reader/datareader/","title":"Data Reader","text":"<p><code>DataReader</code> \u63d2\u4ef6\u662f\u4e13\u95e8\u63d0\u4f9b\u7528\u4e8e\u5f00\u53d1\u548c\u6d4b\u8bd5\u73af\u5883\u4e2d\uff0c\u751f\u4ea7\u6ee1\u8db3\u4e00\u5b9a\u89c4\u5219\u8981\u6c42\u7684\u6570\u636e\u7684\u63d2\u4ef6\u3002</p> <p>\u5728\u5b9e\u9645\u5f00\u53d1\u548c\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6309\u7167\u4e00\u5b9a\u7684\u4e1a\u52a1\u89c4\u5219\u6765\u751f\u4ea7\u6d4b\u8bd5\u6570\u636e\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u968f\u673a\u5185\u5bb9\uff0c\u6bd4\u5982\u8eab\u4efd\u8bc1\u53f7\u7801\uff0c\u94f6\u884c\u8d26\u53f7\uff0c\u80a1\u7968\u4ee3\u7801\u7b49\u3002</p>"},{"location":"reader/datareader/#_1","title":"\u4e3a\u4ec0\u4e48\u8981\u91cd\u590d\u53d1\u660e\u8f6e\u5b50","text":"<p>\u8bda\u7136\uff0c\u7f51\u7edc\u4e0a\u6709\u76f8\u5f53\u591a\u7684\u4e13\u95e8\u7684\u6570\u636e\u751f\u4ea7\u5de5\u5177\uff0c\u5176\u4e2d\u4e0d\u4e4f\u529f\u80fd\u5f3a\u5927\u3001\u6027\u80fd\u4e5f\u5f3a\u608d\u3002 \u4f46\u8fd9\u4e9b\u5de5\u5177\u5927\u90e8\u5206\u662f\u8003\u8651\u5230\u4e86\u6570\u636e\u751f\u6210\u8fd9\u4e00\u6bb5\uff0c\u800c\u5ffd\u7565\u4e86\u6570\u636e\u5199\u5165\u5230\u76ee\u6807\u7aef\u7684\u95ee\u9898\uff0c\u6216\u8005\u8bf4\u6709\u4e9b\u8003\u8651\u5230\u4e86\uff0c\u4f46\u4ec5\u4ec5\u53ea\u8003\u8651\u4e86\u4e00\u79cd\u6216\u6709\u9650\u7684\u51e0\u79cd\u6570\u636e\u5e93\u3002</p> <p>\u6070\u597d Addax \u5de5\u5177\u80fd\u591f\u63d0\u4f9b\u8db3\u591f\u591a\u7684\u76ee\u6807\u7aef\u5199\u5165\u80fd\u529b\uff0c\u52a0\u4e0a\u4e4b\u524d\u7684\u5df2\u6709\u7684 Stream Reader \u5df2\u7ecf\u7b97\u662f\u4e00\u4e2a\u7b80\u5355\u7248\u7684\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u56e0\u6b64\u5728\u6b64\u529f\u80fd\u4e0a \u589e\u52a0\u4e00\u4e9b\u7279\u5b9a\u89c4\u5219\uff0c\u518d\u5229\u7528\u5199\u5165\u7aef\u591a\u6837\u6027\u7684\u80fd\u529b\uff0c\u81ea\u7136\u5c31\u6210\u4e3a\u4e86\u4e00\u4e2a\u8f83\u597d\u7684\u6570\u636e\u751f\u6210\u5de5\u5177\u3002</p>"},{"location":"reader/datareader/#_2","title":"\u914d\u7f6e\u793a\u4f8b","text":"<p>\u8fd9\u91cc\u6211\u628a\u76ee\u524d\u63d2\u4ef6\u652f\u6301\u7684\u89c4\u5219\u5168\u90e8\u5217\u4e3e\u5230\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d</p> datareader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"datareader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"1,100,\",\n              \"rule\": \"random\",\n              \"type\": \"double\"\n            },\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"1\",\n              \"rule\": \"incr\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989/06/04 00:00:01,-1\",\n              \"rule\": \"incr\",\n              \"type\": \"date\",\n              \"dateFormat\": \"yyyy/MM/dd hh:mm:ss\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"rule\": \"address\"\n            },\n            {\n              \"rule\": \"bank\"\n            },\n            {\n              \"rule\": \"company\"\n            },\n            {\n              \"rule\": \"creditCard\"\n            },\n            {\n              \"rule\": \"debitCard\"\n            },\n            {\n              \"rule\": \"idCard\"\n            },\n            {\n              \"rule\": \"lat\"\n            },\n            {\n              \"rule\": \"lng\"\n            },\n            {\n              \"rule\": \"name\"\n            },\n            {\n              \"rule\": \"job\"\n            },\n            {\n              \"rule\": \"phone\"\n            },\n            {\n              \"rule\": \"stockCode\"\n            },\n            {\n              \"rule\": \"stockAccount\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u4fdd\u5b58\u4e0a\u8ff0\u5185\u5bb9\u5230 <code>job/datareader2stream.json</code></p> <p>\u7136\u540e\u6267\u884c\u8be5\u4efb\u52a1\uff0c\u5176\u8f93\u51fa\u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>$ bin/addax.sh job/datareader2stream.json\n\n  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-13 17:02:00.888 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-13 17:02:00.910 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"rule\":\"random\",\n                            \"type\":\"double\",\n                            \"scale\": \"2\",\n                            \"value\":\"1,100,\"\n                        },\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"rule\":\"incr\",\n                            \"type\":\"long\",\n                            \"value\":\"1\"\n                        },\n                        {\n                            \"dateFormat\":\"yyyy/MM/dd hh:mm:ss\",\n                            \"rule\":\"incr\",\n                            \"type\":\"date\",\n                            \"value\":\"1989/06/04 00:00:01,-1\"\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        },\n                        {\n                            \"rule\":\"address\"\n                        },\n                        {\n                            \"rule\":\"bank\"\n                        },\n                        {\n                            \"rule\":\"company\"\n                        },\n                        {\n                            \"rule\":\"creditCard\"\n                        },\n                        {\n                            \"rule\":\"debitCard\"\n                        },\n                        {\n                            \"rule\":\"idCard\"\n                        },\n                        {\n                            \"rule\":\"lat\"\n                        },\n                        {\n                            \"rule\":\"lng\"\n                        },\n                        {\n                            \"rule\":\"name\"\n                        },\n                        {\n                            \"rule\":\"job\"\n                        },\n                        {\n                            \"rule\":\"phone\"\n                        },\n                        {\n                            \"rule\":\"stockCode\"\n                        },\n                        {\n                            \"rule\":\"stockAccount\"\n                        }\n                    ],\n                    \"sliceRecordCount\":10\n                },\n                \"name\":\"datareader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-08-13 17:02:00.937 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-13 17:02:00.938 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-13 17:02:00.940 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-13 17:02:00.976 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do prepare work .\n2021-08-13 17:02:00.977 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-13 17:02:00.978 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-13 17:02:00.979 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] splits to [1] tasks.\n2021-08-13 17:02:00.980 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-13 17:02:01.002 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-13 17:02:01.009 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n7.65    DataX   1   1989-06-04 00:00:01 test    \u5929\u6d25\u5e02\u5357\u4eac\u53bf\u957f\u5bff\u533a\u5149\u660e\u8def263\u53f7    \u4ea4\u901a\u94f6\u884c    \u6613\u52a8\u529b\u4fe1\u606f\u6709\u9650\u516c\u53f8   6227894836568607    6235712610856305437 450304194808316766  31.3732613  -125.3507716    \u9f9a\u519b  \u673a\u7535\u5de5\u7a0b\u5e08   13438631667 726929  8741848665\n18.58   DataX   2   1989-06-03 00:00:01 test    \u6c5f\u82cf\u7701\u592a\u539f\u5e02\u6d54\u9633\u533a\u4e1c\u5c71\u8def33\u53f7 \u4e2d\u56fd\u94f6\u884c    \u65f6\u7a7a\u76d2\u6570\u5b57\u4fe1\u606f\u6709\u9650\u516c\u53f8 4096666711928233    6217419359154239015 220301200008188547  48.6648764  104.8567048 \u5321\u98de  \u5316\u5986\u5e08 18093137306 006845  1815787371\n16.16   DataX   3   1989-06-02 00:00:01 test    \u53f0\u6e7e\u7701\u90af\u90f8\u5e02\u6e05\u6cb3\u533a\u4e07\u987a\u8def10\u53f7 \u5927\u540c\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u79d1\u6280\u6709\u9650\u516c\u53f8 4096713966912225    6212977716107080594 150223196408276322  29.0134395  142.6426842 \u652f\u6ce2  \u5ba1\u6838\u5458 13013458079 020695  3545552026\n63.89   DataX   4   1989-06-01 00:00:01 test    \u4e0a\u6d77\u5e02\u8f9b\u96c6\u53bf\u516d\u679d\u7279\u533a\u7518\u56ed\u8def119\u53f7   \u4e2d\u56fd\u519c\u4e1a\u94f6\u884c  \u6cf0\u9e92\u9e9f\u4f20\u5a92\u6709\u9650\u516c\u53f8   6227893481508780    6215686558778997167 220822196208286838  -71.6484635 111.8181273 \u656c\u5764  \u623f\u5730\u4ea7\u5ba2\u670d   13384928291 174445  0799668655\n79.18   DataX   5   1989-05-31 00:00:01 test    \u9655\u897f\u7701\u5357\u4eac\u5e02\u671d\u9633\u533a\u5927\u80dc\u8def170\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u6656\u6765\u8ba1\u7b97\u673a\u4fe1\u606f\u6709\u9650\u516c\u53f8 6227535683896707    6217255315590053833 350600198508222018  -24.9783587 78.017024   \u848b\u6768  \u56fa\u5b9a\u8d44\u4ea7\u4f1a\u8ba1  18766298716 402188  9633759917\n14.97   DataX   6   1989-05-30 00:00:01 test    \u6d77\u5357\u7701\u957f\u6625\u53bf\u74a7\u5c71\u533a\u78a7\u6d77\u8857147\u53f7    \u534e\u590f\u94f6\u884c    \u6d59\u5927\u4e07\u670b\u79d1\u6280\u6709\u9650\u516c\u53f8  6224797475369912    6215680436662199846 220122199608190275  -3.5088667  -40.2634359 \u8fb9\u6768  \u7763\u5bfc/\u5de1\u5e97   13278765923 092780  2408887582\n45.49   DataX   7   1989-05-29 00:00:01 test    \u53f0\u6e7e\u7701\u6f5c\u6c5f\u53bf\u6881\u5e73\u533a\u4e03\u661f\u8857201\u53f7    \u664b\u57ce\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u4fe1\u606f\u6709\u9650\u516c\u53f8 5257468530819766    6213336008535546044 141082197908244004  -72.9200596 120.6018163 \u6851\u660e  \u7cfb\u7edf\u5de5\u7a0b\u5e08   13853379719 175864  8303448618\n8.45    DataX   8   1989-05-28 00:00:01 test    \u6d77\u5357\u7701\u676d\u5dde\u53bf\u57ce\u5317\u533a\u5929\u5174\u8def11\u53f7 \u5927\u540c\u5546\u884c    \u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8  6227639043120062    6270259717880740332 430405198908214042  -16.5115338 -39.336119  \u8983\u5065  \u4eba\u4e8b\u603b\u76d1    13950216061 687461  0216734574\n15.01   DataX   9   1989-05-27 00:00:01 test    \u4e91\u5357\u7701\u60e0\u5dde\u5e02\u548c\u5e73\u533a\u6d77\u9e25\u8857201\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u9ec4\u77f3\u91d1\u627f\u4fe1\u606f\u6709\u9650\u516c\u53f8  6200358843233005    6235730928871528500 130300195008312067  -61.646097  163.0882369 \u536b\u5efa\u534e \u7535\u8bdd\u91c7\u7f16    15292600492 001658  1045093445\n55.14   DataX   10  1989-05-26 00:00:01 test    \u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7    \u5eca\u574a\u94f6\u884c    \u521b\u6c47\u79d1\u6280\u6709\u9650\u516c\u53f8    6227605280751588    6270262330691012025 341822200908168063  77.2165746  139.5431377 \u6c60\u6d69  \u591a\u5a92\u4f53\u8bbe\u8ba1   18693948216 201678  0692522928\n\n2021-08-13 17:02:04.020 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-13 17:02:04.021 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-13 17:02:04.022 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do post work.\n2021-08-13 17:02:04.025 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-13 17:02:04.028 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 1817 bytes | Speed 605B/s, 3 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-08-13 17:02:04.030 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-13 17:02:00\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-13 17:02:04\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              605B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/datareader/#_3","title":"\u914d\u7f6e\u8bf4\u660e","text":"<p><code>column</code> \u7684\u914d\u7f6e\u548c\u5176\u4ed6\u63d2\u4ef6\u7684\u914d\u7f6e\u7a0d\u6709\u4e0d\u540c\uff0c\u4e00\u4e2a\u5b57\u6bb5\u7531\u4ee5\u4e0b\u914d\u7f6e\u9879\u7ec4\u6210</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u9ed8\u8ba4\u503c \u793a\u4f8b \u8bf4\u660e value \u5426 \u65e0 <code>Addax</code> \u6570\u636e\u503c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e3a\u5fc5\u9009\u9879 rule \u5426 <code>constant</code> <code>idCard</code> \u6570\u636e\u751f\u4ea7\u89c4\u5219\uff0c\u8be6\u7ec6\u4e0b\u9762\u7684\u63cf\u8ff0 type \u5426 <code>string</code> <code>double</code> \u6570\u636e\u503c\u7c7b\u578b dateFormat \u5426 <code>yyyy-MM-dd HH:mm:ss</code> <code>yyyy/MM/dd HH:mm:ss</code> \u65e5\u671f\u683c\u5f0f\uff0c\u4ec5\u5728 <code>type</code> \u4e3a <code>date</code> \u65f6\u6709\u6548"},{"location":"reader/datareader/#rule","title":"rule \u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u7684\u5b57\u6bb5\u914d\u7f6e\u6838\u5fc3\u662f <code>rule</code> \u5b57\u6bb5\uff0c\u5b83\u7528\u6765\u6307\u793a\u5e94\u8be5\u751f\u6210\u4ec0\u4e48\u6837\u7684\u6570\u636e\uff0c\u5e76\u4f9d\u636e\u4e0d\u540c\u89c4\u5219\uff0c\u914d\u5408\u5176\u4ed6\u914d\u7f6e\u9009\u9879\u6765\u751f\u4ea7\u6ee1\u8db3\u671f\u671b\u7684\u6570\u636e\u3002 \u5f53\u524d <code>rule</code> \u7684\u914d\u7f6e\u5747\u4e3a\u5185\u7f6e\u652f\u6301\u7684\u89c4\u5219\uff0c\u6682\u4e0d\u652f\u6301\u81ea\u5b9a\u4e49\uff0c\u4ee5\u4e0b\u8be6\u7ec6\u8bf4\u660e</p>"},{"location":"reader/datareader/#constant","title":"constant","text":"<p><code>constant</code> \u662f <code>rule</code> \u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u8be5\u89c4\u5219\u610f\u5473\u7740\u8981\u751f\u6210\u7684\u6570\u636e\u503c\u7531 <code>value</code> \u914d\u7f6e\u9879\u51b3\u5b9a\uff0c\u5176\u4e0d\u505a\u4efb\u4f55\u53d8\u66f4\u3002\u6bd4\u5982</p> <pre><code>{\n  \"value\": \"Addax\",\n  \"type\": \"string\",\n  \"rule\": \"constant\"\n}\n</code></pre> <p>\u8868\u793a\u8be5\u5b57\u6bb5\u751f\u4ea7\u7684\u6570\u636e\u503c\u5747\u4e3a <code>Addax</code></p>"},{"location":"reader/datareader/#incr","title":"incr","text":"<p><code>incr</code> \u914d\u7f6e\u9879\u7684\u542b\u4e49\u548c <code>streamreader</code> \u63d2\u4ef6\u4e2d\u7684 <code>incr</code> \u542b\u4e49\u4e00\u81f4\uff0c\u8868\u793a\u8fd9\u662f\u4e00\u4e2a\u9012\u589e\u7684\u6570\u636e\u751f\u4ea7\u89c4\u5219\uff0c\u6bd4\u5982</p> <pre><code>{\n  \"value\": \"1,2\",\n  \"rule\": \"incr\",\n  \"type\": \"long\"\n}\n</code></pre> <p>\u8868\u793a\u8be5\u5b57\u6bb5\u7684\u6570\u636e\u662f\u4e00\u4e2a\u957f\u6574\u5f62\uff0c\u6570\u503c\u4ece 1 \u5f00\u59cb\uff0c\u6bcf\u6b21\u9012\u589e 2\uff0c\u4e5f\u5c31\u662f\u5f62\u6210 1 \u5f00\u59cb\uff0c\u6b65\u957f\u4e3a 2 \u7684\u9012\u589e\u6570\u5217\u3002</p> <p>\u8be5\u5b57\u6bb5\u66f4\u8be6\u7ec6\u7684\u914d\u7f6e\u89c4\u5219\u548c\u6ce8\u610f\u4e8b\u9879\uff0c\u53ef\u4ee5\u53c2\u8003 streamreader \u4e2d\u7684 <code>incr</code> \u8bf4\u660e\u3002</p>"},{"location":"reader/datareader/#random","title":"random","text":"<p><code>random</code> \u914d\u7f6e\u9879\u7684\u542b\u4e49\u548c <code>streamreader</code> \u63d2\u4ef6\u4e2d\u7684 <code>random</code> \u542b\u4e49\u4e00\u81f4\uff0c\u8868\u793a\u8fd9\u662f\u4e00\u4e2a\u9012\u589e\u7684\u6570\u636e\u751f\u4ea7\u89c4\u5219\uff0c\u6bd4\u5982</p> <pre><code>{\n  \"value\": \"1,10\",\n  \"rule\": \"random\",\n  \"type\": \"string\"\n}\n</code></pre> <p>\u8868\u793a\u8be5\u5b57\u6bb5\u7684\u6570\u636e\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a 1 \u5230 10 \uff081 \u548c 10 \u90fd\u5305\u62ec\uff09\u968f\u673a\u5b57\u7b26\u4e32\u3002</p> <p>\u8be5\u5b57\u6bb5\u66f4\u8be6\u7ec6\u7684\u914d\u7f6e\u89c4\u5219\u548c\u6ce8\u610f\u4e8b\u9879\uff0c\u53ef\u4ee5\u53c2\u8003 streamreader \u4e2d\u7684 <code>random</code> \u8bf4\u660e\u3002</p> \u89c4\u5219\u540d\u79f0 \u542b\u4e49 \u793a\u4f8b \u6570\u636e\u7c7b\u578b \u8bf4\u660e <code>address</code> \u968f\u673a\u751f\u6210\u4e00\u6761\u57fa\u672c\u6ee1\u8db3\u56fd\u5185\u5b9e\u9645\u60c5\u51b5\u7684\u5730\u5740\u4fe1\u606f <code>\u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7</code> string <code>bank</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u56fd\u5185\u94f6\u884c\u540d\u79f0 <code>\u534e\u590f\u94f6\u884c</code> string <code>company</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u516c\u53f8\u7684\u540d\u79f0 <code>\u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8</code> string <code>creditCard</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u4fe1\u7528\u5361\u5361\u53f7 <code>430405198908214042</code> string 16 \u4f4d <code>debitCard</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u50a8\u84c4\u5361\u5361\u53f7 <code>6227894836568607</code> string 19 \u4f4d <code>email</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u7535\u5b50\u90ae\u4ef6\u5730\u5740 <code>ok2a@gmail.com</code> string <code>idCard</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u56fd\u5185\u8eab\u4efd\u8bc1\u53f7\u7801 <code>350600198508222018</code> string 18 \u4f4d\uff0c\u8d1f\u8d23\u6821\u9a8c\u89c4\u5219\uff0c\u5934 6 \u4f4d\u7f16\u7801\u6ee1\u8db3\u884c\u653f\u533a\u5212\u8981\u6c42 <code>lat</code> \u968f\u673a\u751f\u6210\u7ef4\u5ea6\u6570\u636e <code>48.6648764</code> double \u56fa\u5b9a 7 \u4f4d\u5c0f\u6570 \uff0c\u4e5f\u53ef\u4ee5\u7528<code>latitude</code> \u8868\u793a <code>lng</code> \u968f\u673a\u751f\u6210\u7ecf\u5ea6\u6570\u636e <code>120.6018163</code> double \u56fa\u5b9a 7 \u4f4d\u5c0f\u6570\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528<code>longitude</code> \u8868\u793a <code>name</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u56fd\u5185\u540d\u5b57 <code>\u6c60\u6d69</code> string \u6682\u6ca1\u8003\u8651\u59d3\u6c0f\u5728\u56fd\u5185\u7684\u5360\u6bd4\u5ea6 <code>job</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u56fd\u5185\u5c97\u4f4d\u540d\u79f0 <code>\u7cfb\u7edf\u5de5\u7a0b\u5e08</code> string \u6570\u636e\u6765\u6e90\u4e8e\u62db\u8058\u7f51\u7ad9 <code>phone</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a\u56fd\u5185\u624b\u673a\u53f7\u7801 <code>15292600492</code> string \u6682\u4e0d\u8003\u8651\u865a\u62df\u624b\u673a\u53f7 <code>stockCode</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a 6 \u4f4d\u7684\u80a1\u7968\u4ee3\u7801 <code>687461</code> string \u524d\u4e24\u4f4d\u6ee1\u8db3\u56fd\u5185\u80a1\u7968\u4ee3\u7801\u7f16\u53f7\u89c4\u8303 <code>stockAccount</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a 10 \u4f4d\u7684\u80a1\u7968\u4ea4\u6613\u8d26\u6237 <code>0692522928</code> string \u5b8c\u5168\u968f\u673a\uff0c\u4e0d\u6ee1\u8db3\u8d26\u6237\u89c4\u8303 <code>uuid</code> \u968f\u673a\u751f\u6210\u4e00\u4e2a UUID \u5b57\u7b26\u4e32 <code>bc1cf125-929b-43b7-b324-d7c4cc5a75d2</code> string \u5b8c\u5168\u968f\u673a\uff0c\u4e0d\u6ee1\u8db3\u8d26\u6237\u89c4\u8303 <code>zipCode</code> \u968f\u673a\u751f\u4ea7\u4e00\u4e2a\u56fd\u5185\u90ae\u653f\u7f16\u53f7 <code>411105</code> long \u4e0d\u5b8c\u5168\u6ee1\u8db3\u56fd\u5185\u90ae\u653f\u7f16\u53f7\u89c4\u8303 <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u8868\u683c\u4e2d\u7684\u89c4\u5219\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b\u662f\u56fa\u5b9a\u7684\uff0c\u4e14\u4e0d\u652f\u6301\u4fee\u6539\uff0c\u56e0\u6b64 <code>type</code> \u65e0\u9700\u914d\u7f6e\uff0c\u914d\u7f6e\u7684\u7c7b\u578b\u4e5f\u4f1a\u88ab\u5ffd\u7565\uff0c\u56e0\u4e3a\u6570\u636e\u751f\u6210\u6765\u81ea\u5185\u90e8\u89c4\u5219\uff0c\u6240\u4ee5 <code>value</code> \u4e5f\u65e0\u9700\u914d\u7f6e\uff0c\u914d\u7f6e\u7684\u5185\u5bb9\u4e5f\u4f1a\u88ab\u5ffd\u7565\u3002</p>"},{"location":"reader/dbfreader/","title":"Dbf Reader","text":"<p><code>DbfReader</code> \u63d2\u4ef6\u652f\u6301\u8bfb\u53d6 DBF \u683c\u5f0f\u6587\u4ef6</p>"},{"location":"reader/dbfreader/#_1","title":"\u914d\u7f6e\u8bf4\u660e","text":"<p>\u4ee5\u4e0b\u662f\u8bfb\u53d6 DBF \u6587\u4ef6\u540e\u6253\u5370\u5230\u7ec8\u7aef\u7684\u914d\u7f6e\u6837\u4f8b</p> jobs/dbf2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"dbfreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"dbf\",\n              \"type\": \"string\"\n            }\n          ],\n          \"path\": [\n            \"/tmp/out\"\n          ],\n          \"encoding\": \"GBK\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/dbfreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p><code>parameter</code> \u914d\u7f6e\u9879\u652f\u6301\u4ee5\u4e0b\u914d\u7f6e</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f \u65e0 DBF \u6587\u4ef6\u8def\u5f84\uff0c\u652f\u6301\u5199\u591a\u4e2a\u8def\u5f84\uff0c\u8be6\u7ec6\u60c5\u51b5\u89c1\u4e0b column \u662f \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u96c6\u5408, \u662f <code>{type: value}</code> \u6216 <code>{type: index}</code> \u7684\u96c6\u5408\uff0c\u8be6\u7ec6\u914d\u7f6e\u89c1\u4e0b encoding \u5426 GBK DBF \u6587\u4ef6\u7f16\u7801\uff0c\u6bd4\u5982 <code>GBK</code>, <code>UTF-8</code> nullFormat \u5426 <code>\\N</code> \u5b9a\u4e49\u54ea\u4e2a\u5b57\u7b26\u4e32\u53ef\u4ee5\u8868\u793a\u4e3a null,"},{"location":"reader/dbfreader/#path","title":"path","text":"<p>\u63cf\u8ff0\uff1a\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\u3002</p> <ul> <li>\u5f53\u6307\u5b9a\u5355\u4e2a\u672c\u5730\u6587\u4ef6\uff0cDbfFileReader \u6682\u65f6\u53ea\u80fd\u4f7f\u7528\u5355\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u4e8c\u671f\u8003\u8651\u5728\u975e\u538b\u7f29\u6587\u4ef6\u60c5\u51b5\u4e0b\u9488\u5bf9\u5355\u4e2a File \u53ef\u4ee5\u8fdb\u884c\u591a\u7ebf\u7a0b\u5e76\u53d1\u8bfb\u53d6\u3002</li> <li>\u5f53\u6307\u5b9a\u591a\u4e2a\u672c\u5730\u6587\u4ef6\uff0cDbfFileReader \u652f\u6301\u4f7f\u7528\u591a\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u7ebf\u7a0b\u5e76\u53d1\u6570\u901a\u8fc7\u901a\u9053\u6570\u6307\u5b9a\u3002</li> <li>\u5f53\u6307\u5b9a\u901a\u914d\u7b26\uff0cDbfFileReader \u5c1d\u8bd5\u904d\u5386\u51fa\u591a\u4e2a\u6587\u4ef6\u4fe1\u606f\u3002\u4f8b\u5982: \u6307\u5b9a <code>/*</code> \u4ee3\u8868\u8bfb\u53d6/\u76ee\u5f55\u4e0b\u6240\u6709\u7684\u6587\u4ef6\uff0c\u6307\u5b9a <code>/foo/*</code> \u4ee3\u8868\u8bfb\u53d6 <code>foo</code> \u76ee\u5f55\u4e0b\u6e38\u6240\u6709\u7684\u6587\u4ef6\u3002 dbfFileReader \u76ee\u524d\u53ea\u652f\u6301 <code>*</code> \u4f5c\u4e3a\u6587\u4ef6\u901a\u914d\u7b26\u3002</li> </ul> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cAddax \u4f1a\u5c06\u4e00\u4e2a\u4f5c\u4e1a\u4e0b\u540c\u6b65\u7684\u6240\u6709 dbf File \u89c6\u4f5c\u540c\u4e00\u5f20\u6570\u636e\u8868\u3002\u7528\u6237\u5fc5\u987b\u81ea\u5df1\u4fdd\u8bc1\u6240\u6709\u7684 File \u80fd\u591f\u9002\u914d\u540c\u4e00\u5957 schema \u4fe1\u606f\u3002\u8bfb\u53d6\u6587\u4ef6\u7528\u6237\u5fc5\u987b\u4fdd\u8bc1\u4e3a\u7c7b dbf \u683c\u5f0f\uff0c\u5e76\u4e14\u63d0\u4f9b\u7ed9 Addax \u6743\u9650\u53ef\u8bfb\u3002</p> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5982\u679c Path \u6307\u5b9a\u7684\u8def\u5f84\u4e0b\u6ca1\u6709\u7b26\u5408\u5339\u914d\u7684\u6587\u4ef6\u62bd\u53d6\uff0cAddax \u5c06\u62a5\u9519\u3002</p>"},{"location":"reader/dbfreader/#column","title":"column","text":"<p>\u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>name</code> \u4e3a\u5b57\u6bb5\u540d,\u957f\u5ea6\u6700\u5927 8\uff0c<code>value</code> \u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece\u6e90\u5934\u6587\u4ef6\u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636e <code>value</code> \u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7528\u6237\u53ef\u4ee5\u5168\u90e8\u6309\u7167 <code>String</code> \u7c7b\u578b\u8bfb\u53d6\u6570\u636e\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\"*\"]\n}\n</code></pre> <p>\u7528\u6237\u53ef\u4ee5\u6307\u5b9a Column \u5b57\u6bb5\u4fe1\u606f\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"addax\"\n  }\n]\n</code></pre> <ul> <li><code>\"index\": 0</code> \u8868\u793a\u4ece\u672c\u5730 DBF \u6587\u4ef6\u7b2c\u4e00\u5217\u83b7\u53d6 int \u5b57\u6bb5</li> <li><code>\"value\": \"addax\"</code> \u8868\u793a\u4ece dbfFileReader \u5185\u90e8\u751f\u6210 <code>addax</code> \u7684\u5b57\u7b26\u4e32\u5b57\u6bb5\u4f5c\u4e3a\u5f53\u524d\u5b57\u6bb5 \u5bf9\u4e8e\u7528\u6237\u6307\u5b9a <code>column</code>\u4fe1\u606f\uff0c<code>type</code> \u5fc5\u987b\u586b\u5199\uff0c<code>index</code> \u548c <code>value</code> \u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</li> </ul>"},{"location":"reader/dbfreader/#_3","title":"\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b","text":"<p>\u672c\u5730\u6587\u4ef6\u672c\u8eab\u63d0\u4f9b\u6570\u636e\u7c7b\u578b\uff0c\u8be5\u7c7b\u578b\u662f Addax dbfFileReader \u5b9a\u4e49\uff1a</p> Addax \u5185\u90e8\u7c7b\u578b \u672c\u5730\u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long Double Double String String Boolean Boolean Date Date <p>\u5176\u4e2d\uff1a</p> <ul> <li>Long \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528\u6574\u5f62\u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>19901219</code>\u3002</li> <li>Double \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Double \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>3.1415</code>\u3002</li> <li>Boolean \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Boolean \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>true</code>\u3001<code>false</code>\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u3002</li> <li>Date \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Date \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>2014-12-31</code>\uff0c\u53ef\u4ee5\u914d\u7f6e <code>dateFormat</code> \u6307\u5b9a\u683c\u5f0f\u3002</li> </ul>"},{"location":"reader/elasticsearchreader/","title":"ElasticSearchReader","text":"<p>ElasticSearchReader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Elasticsearch \u8bfb\u53d6\u7d22\u5f15\u7684\u529f\u80fd\uff0c \u5b83\u901a\u8fc7 Elasticsearch \u63d0\u4f9b\u7684 Rest API \uff08\u9ed8\u8ba4\u7aef\u53e39200\uff09\uff0c\u6267\u884c\u6307\u5b9a\u7684\u67e5\u8be2\u8bed\u53e5\u6279\u91cf\u83b7\u53d6\u6570\u636e</p>"},{"location":"reader/elasticsearchreader/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u83b7\u53d6\u7684\u7d22\u5f15\u5185\u5bb9\u5982\u4e0b</p> <pre><code>{\n  \"took\": 14,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 1,\n    \"hits\": [\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"38\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      },\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"103\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>\u914d\u7f6e\u4e00\u4e2a\u4ece Elasticsearch \u8bfb\u53d6\u6570\u636e\u5e76\u6253\u5370\u5230\u7ec8\u7aef\u7684\u4efb\u52a1</p> job/es2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"elasticsearchreader\",\n        \"parameter\": {\n          \"endpoint\": \"http://127.0.0.1:9200\",\n          \"accessId\": \"\",\n          \"accesskey\": \"\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"searchType\": \"dfs_query_then_fetch\",\n          \"headers\": {},\n          \"scroll\": \"3m\",\n          \"search\": [\n            {\n              \"query\": {\n                \"match\": {\n                  \"col_ip\": \"1.1.1.1\"\n                }\n              },\n              \"aggregations\": {\n                \"top_10_states\": {\n                  \"terms\": {\n                    \"field\": \"col_date\",\n                    \"size\": 10\n                  }\n                }\n              }\n            }\n          ],\n          \"column\": [\n            \"col_ip\",\n            \"col_double\",\n            \"col_long\",\n            \"col_integer\",\n            \"col_keyword\",\n            \"col_text\",\n            \"col_geo_point\",\n            \"col_date\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u5185\u5bb9\u4fdd\u5b58\u4e3a <code>job/es2stream.json</code></p> <p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u91c7\u96c6</p> <pre><code>bin/addax.sh job/es2stream.json\n</code></pre> <p>\u5176\u8f93\u51fa\u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff08\u8f93\u51fa\u8bb0\u5f55\u6570\u6709\u5220\u51cf)</p> <pre><code>2021-02-19 13:38:15.860 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-02-19 13:38:15.895 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"accessId\":\"\",\n                    \"headers\":{},\n                    \"endpoint\":\"http://127.0.0.1:9200\",\n                    \"search\":[\n                      {\n                        \"query\": {\n                          \"match\": {\n                            \"col_ip\": \"1.1.1.1\"\n                          }\n                        },\n                        \"aggregations\": {\n                          \"top_10_states\": {\n                            \"terms\": {\n                              \"field\": \"col_date\",\n                              \"size\": 10\n                            }\n                          }\n                        }\n                      }\n                    ],\n                    \"accesskey\":\"*****\",\n                    \"searchType\":\"dfs_query_then_fetch\",\n                    \"scroll\":\"3m\",\n                    \"column\":[\n                        \"col_ip\",\n                        \"col_double\",\n                        \"col_long\",\n                        \"col_integer\",\n                        \"col_keyword\",\n                        \"col_text\",\n                        \"col_geo_point\",\n                        \"col_date\"\n                    ],\n                    \"index\":\"test-1\",\n                    \"type\":\"default\"\n                },\n                \"name\":\"elasticsearchreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-02-19 13:38:15.934 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-19 13:38:15.934 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-19 13:38:15.937 [main] INFO  JobContainer - Set jobId = 0\n\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n\n2021-02-19 13:38:19.845 [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.\n2021-02-19 13:38:19.848 [job-0] INFO  JobContainer - Addax Writer.Job [streamwriter] do post work.\n2021-02-19 13:38:19.849 [job-0] INFO  JobContainer - Addax Reader.Job [elasticsearchreader] do post work.\n2021-02-19 13:38:19.855 [job-0] INFO  JobContainer - PerfTrace not enable!\n2021-02-19 13:38:19.858 [job-0] INFO  StandAloneJobContainerCommunicator - Total 95 records, 8740 bytes | Speed 2.84KB/s, 31 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.103s | Percentage 100.00%\n2021-02-19 13:38:19.861 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-19 13:38:15\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-19 13:38:19\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.84KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             31rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   2\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/elasticsearchreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 ElasticSearch\u7684\u8fde\u63a5\u5730\u5740 accessId \u5426 string <code>\"\"</code> http auth\u4e2d\u7684user accessKey \u5426 string <code>\"\"</code> http auth\u4e2d\u7684password index \u662f string \u65e0 elasticsearch\u4e2d\u7684index\u540d type \u5426 string index\u540d elasticsearch\u4e2dindex\u7684type\u540d search \u662f list <code>[]</code> json\u683c\u5f0fapi\u641c\u7d22\u6570\u636e\u4f53 column \u662f list \u65e0 \u9700\u8981\u8bfb\u53d6\u7684\u5b57\u6bb5 timeout \u5426 int 60 \u5ba2\u6237\u7aef\u8d85\u65f6\u65f6\u95f4(\u5355\u4f4d\uff1a\u79d2) discovery \u5426 boolean false \u542f\u7528\u8282\u70b9\u53d1\u73b0(\u8f6e\u8be2)\u5e76\u5b9a\u671f\u66f4\u65b0\u5ba2\u6237\u673a\u4e2d\u7684\u670d\u52a1\u5668\u5217\u8868 compression \u5426 boolean true http\u8bf7\u6c42\uff0c\u5f00\u542f\u538b\u7f29 multiThread \u5426 boolean true http\u8bf7\u6c42\uff0c\u662f\u5426\u6709\u591a\u7ebf\u7a0b searchType \u5426 string <code>dfs_query_then_fetch</code> \u641c\u7d22\u7c7b\u578b headers \u5426 map <code>{}</code> http\u8bf7\u6c42\u5934 scroll \u5426 string <code>\"\"</code> \u6eda\u52a8\u5206\u9875\u914d\u7f6e"},{"location":"reader/elasticsearchreader/#search","title":"search","text":"<p>search \u914d\u7f6e\u9879\u5141\u8bb8\u914d\u7f6e\u4e3a\u6ee1\u8db3 Elasticsearch API \u67e5\u8be2\u8981\u6c42\u7684\u5185\u5bb9\uff0c\u6bd4\u5982\u8fd9\u6837\uff1a</p> <pre><code>{\n  \"query\": {\n    \"match\": {\n      \"message\": \"myProduct\"\n    }\n  },\n  \"aggregations\": {\n    \"top_10_states\": {\n      \"terms\": {\n        \"field\": \"state\",\n        \"size\": 10\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/elasticsearchreader/#searchtype","title":"searchType","text":"<p>searchType \u76ee\u524d\u652f\u6301\u4ee5\u4e0b\u51e0\u79cd\uff1a</p> <ul> <li>dfs_query_then_fetch</li> <li>query_then_fetch</li> <li>count</li> <li>scan</li> </ul>"},{"location":"reader/excelreader/","title":"Excel Reader","text":"<p><code>Excel Reader</code> \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Microsoft Excel \u6587\u4ef6\u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b\u3002</p>"},{"location":"reader/excelreader/#_1","title":"\u914d\u7f6e","text":""},{"location":"reader/excelreader/#_2","title":"\u83b7\u53d6\u6837\u4f8b\u6587\u4ef6","text":"<p>\u4ece\u8fd9\u91cc\u4e0b\u8f7d\u7528\u4e8e\u6f14\u793a\u7684 Excel \u538b\u7f29\u6587\u4ef6\uff0c\u5e76\u89e3\u538b\u7f29\u653e\u7f6e\u5230 <code>/tmp/in</code> \u76ee\u5f55\u4e0b\u3002 \u4e09\u4e2a\u6587\u4ef6\u5939\u7684\u5185\u5bb9\u76f8\u540c\uff0c\u5176\u4e2d</p> <ul> <li><code>demo.xlsx</code> \u662f Excel \u65b0\u683c\u5f0f</li> <li><code>demo.xls</code> \u662f Excel \u8001\u683c\u5f0f</li> <li><code>demo_gbk.xlsx</code> \u662f\u5728 Windows \u4e0b\u521b\u5efa\uff0c\u5df2 GBK \u7f16\u7801\u5b58\u50a8\u7684\u6587\u4ef6</li> </ul> <p>\u6587\u4ef6\u5185\u5bb9\uff0c\u5982\u4e0b\u8868\u6240\u793a\uff1a</p> \u7f16\u53f7 \u6574\u6570\u7c7b\u578b \u6d6e\u70b9\u6570\u7c7b\u578b \u5b57\u7b26\u4e32\u7c7b\u578b \u65e5\u671f\u7c7b\u578b \u516c\u5f0f\u8ba1\u7b97 \u5355\u5143\u683c\u5f0f\u5316 1 11 1102.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/10 5544.17 \u00a51,102.23 2 12 1103.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/11 5552.17 \u00a51,103.23 3 13 1104.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/12 5560.17 \u00a51,104.23 4 14 1105.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/13 5568.17 \u00a51,105.23 5 15 1106.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/14 5576.17 \u00a51,106.23 6 16 1107.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/15 5584.17 \u00a51,107.23 7 17 1108.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/16 5592.17 \u00a51,108.23 8 18 1109.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/17 5600.17 \u00a51,109.23 9 19 1110.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/18 5608.17 \u00a51,110.23 10 20 1111.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/19 5616.17 \u00a51,111.23 11 21 1112.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/20 5624.17 \u00a51,112.23 12 22 1113.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/21 5632.17 \u00a51,113.23 13 23 1114.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/22 5640.17 \u00a51,114.23 14 24 1115.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/23 5648.17 \u00a51,115.23 15 25 1116.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/24 5656.17 \u00a51,116.23 16 26 1117.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/25 5664.17 \u00a51,117.23 17 27 1118.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/26 5672.17 \u00a51,118.23 18 28 1119.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/27 5680.17 \u00a51,119.23 19 29 1120.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/28 5688.17 \u00a51,120.23 20 30 1121.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/29 5696.17 \u00a51,121.23 <p>\u8868\u5934\u5927\u81f4\u8bf4\u660e\u4e86\u5355\u5143\u6570\u636e\u7684\u7279\u5f81</p>"},{"location":"reader/excelreader/#job","title":"\u521b\u5efa job \u6587\u4ef6","text":"<p>\u521b\u5efa\u5982\u4e0b json \u6587\u4ef6</p> excel2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"excelreader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/in\"\n          ],\n          \"header\": true,\n          \"skipRows\": 0\n        }\n      },\n      \"writer\": {\n        \"parameter\": {\n          \"print\": true\n        },\n        \"name\": \"streamwriter\"\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u8f93\u51fa\u5185\u5bb9\u5b58\u4fdd\u4e3a\u5230 <code>job/excel2stream.json</code> \u6587\u4ef6\u4e2d\uff0c\u6267\u884c\u91c7\u96c6\u547d\u4ee4\uff1a</p> <pre><code>$ bin/addax.sh job/excel2stream.json\n</code></pre> <p>\u5982\u679c\u6ca1\u6709\u62a5\u9519\uff0c\u5e94\u8be5\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a</p> \u70b9\u51fb\u5c55\u5f00 <pre><code> ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3)\n\n2021-09-09 14:43:42.579 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-09 14:43:42.621 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"path\":[\n                        \"/tmp/in\"\n                    ],\n                    \"column\":[\n                        {\n                            \"name\":\"no\",\n                            \"type\":\"long\"\n                        },\n                        {\n                            \"name\":\"birth\",\n                            \"format\":\"yyyy-MM-dd HH:mm:ss\",\n                            \"type\":\"date\"\n                        },\n                        {\n                            \"name\":\"kk\",\n                            \"type\":\"string\"\n                        }\n                    ],\n                    \"header\":true,\n                    \"skipHeader\":true,\n                    \"encoding\":\"UTF-8\",\n                    \"fieldDelimiter\":\",\"\n                },\n                \"name\":\"excelreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":2\n        }\n    }\n}\n\n2021-09-09 14:43:42.653 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-09 14:43:42.653 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-09 14:43:42.655 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_old.xls] as a candidate to be read.\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_gbk.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - The number of files to read is: [3]\n2021-09-09 14:43:42.677 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do prepare work .\n2021-09-09 14:43:42.678 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-09-09 14:43:42.679 [       job-0] INFO  JobContainer         - Job set Channel-Number to 2 channels.\n2021-09-09 14:43:42.681 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] splits to [3] tasks.\n2021-09-09 14:43:42.682 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [3] tasks.\n2021-09-09 14:43:42.727 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-09 14:43:42.736 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [2] channels for [3] tasks.\n2021-09-09 14:43:42.741 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-09 14:43:42.742 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo.xlsx\n2021-09-09 14:43:42.757 [0-0-0-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.758 [0-0-0-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_gbk.xlsx\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n1   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n1   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n1   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n1   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n1   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n1   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n1   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n1   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n1   20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n1   21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n1   22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n1   23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n1   24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n1   25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n1   26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n1   27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n1   28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n1   29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n1   30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_old.xls\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:45.753 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-09 14:43:45.754 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-09-09 14:43:45.756 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do post work.\n2021-09-09 14:43:45.761 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-09 14:43:45.762 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 60 records, 3360 bytes | Speed 1.09KB/s, 20 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.993s | Percentage 100.00%\n2021-09-09 14:43:45.764 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-09 14:43:42\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-09 14:43:45\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            1.09KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             20rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  60\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/excelreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f string/list \u65e0 \u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u5939\uff0c\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a header \u5426 boolean false \u6587\u4ef6\u662f\u5426\u5305\u542b\u5934 skipRows \u5426 int 0 \u8981\u8df3\u8fc7\u524d\u591a\u5c11\u884c"},{"location":"reader/excelreader/#header","title":"header","text":"<p>Excel \u6587\u4ef6\u662f\u5426\u5305\u542b\u5934\uff0c\u5982\u679c\u5305\u542b\uff0c\u5219\u8df3\u8fc7</p>"},{"location":"reader/excelreader/#skiprows","title":"skipRows","text":"<p>\u6307\u5b9a\u8981\u8df3\u8fc7\u7684\u884c\u6570\uff0c \u9ed8\u8ba4\u4e3a 0\uff0c\u8868\u793a\u4e0d\u8df3\u8fc7\u3002\u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5047\u5b9a \u8bbe\u7f6e\u4e86 <code>header</code> \u4e3a true\uff0c\u540c\u65f6\u8bbe\u7f6e <code>skipRows</code> \u4e3a 2\u3002\u5219\u8868\u793a\u524d\u4e09\u884c\u90fd\u8df3\u8fc7\u3002 \u5982\u679c <code>header</code> \u4e3a false\uff0c \u5219\u8868\u793a\u8df3\u8fc7\u524d\u4e24\u884c\u3002</p>"},{"location":"reader/excelreader/#_4","title":"\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b","text":"<p>Excel \u8bfb\u53d6\u529f\u80fd\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8e Apache POI \u9879\u76ee\uff0c\u8be5\u5b9e\u73b0\u5bf9\u5355\u5143\u683c\u7684\u6570\u636e\u7c7b\u578b\u5b9a\u4e49\u5f88\u5bbd\u6cdb\u3002 \u4ec5\u5b9a\u4e49\u4e86\u5e03\u5c14\u578b(Boolean)\uff0c\u6570\u503c\u578b (Double)\uff0c\u5b57\u7b26\u4e32\u578b(String) \u4e09\u79cd\u3002\u5176\u4e2d\u6570\u503c\u578b\u5305\u62ec\u6240\u6709\u6574\u6570\uff0c\u5c0f\u6570\u548c\u65e5\u671f\u3002 \u76ee\u524d\u5bf9\u4e8e\u6570\u503c\u7c7b\u578b\u505a\u4e86\u7b80\u5355\u533a\u5206</p> <ol> <li>\u4f7f\u7528\u5e93\u5de5\u5177\u7c7b\u63a2\u6d4b\u662f\u5426\u4e3a\u65e5\u671f\u7c7b\u578b\uff0c\u5982\u679c\u662f\uff0c\u5219\u5224\u65ad\u4e3a\u65e5\u671f\u7c7b\u578b</li> <li>\u5c06\u6570\u503c\u8f6c\u6362\u4e3a\u957f\u6574\u5f62\uff0c\u5e76\u548c\u539f\u503c\u6bd4\u8f83\uff0c\u5982\u679c\u5927\u5c0f\u76f8\u7b49\uff0c\u5219\u5224\u65ad\u4e3a\u957f\u6574\u578b(Long)</li> <li>\u5426\u5219\u5224\u65ad\u4e3a\u6d6e\u70b9\u578b\uff08Double\uff09</li> </ol>"},{"location":"reader/excelreader/#_5","title":"\u9650\u5236","text":"<ol> <li>\u5f53\u524d\u4ec5\u8bfb\u53d6\u6587\u4ef6\u7684\u7b2c\u4e00\u4e2a Sheet \u800c\u5ffd\u7565\u5176\u4ed6 Sheets</li> <li>\u6682\u4e0d\u652f\u6301\u6307\u5b9a\u5217\u8bfb\u53d6</li> <li>\u6682\u4e0d\u652f\u6301\u8df3\u8fc7\u5c3e\u90e8\u884c\u6570\uff08\u6bd4\u5982\u6709\u603b\u7ed3\u7684\u5c3e\u884c\u53ef\u80fd\u5e76\u4e0d\u7b26\u5408\u8981\u6c42\uff09</li> <li>\u6682\u4e0d\u5224\u65ad\u6bcf\u4e00\u884c\u7684\u5217\u6570\u662f\u5426\u76f8\u7b49\uff0c\u9700\u8981 Excel \u81ea\u884c\u4fdd\u8bc1</li> <li>\u4ec5\u4f1a\u8bfb\u53d6\u6307\u5b9a\u76ee\u5f55\u4e0b\u6587\u4ef6\u540e\u7f00\u4e3a <code>xlsx</code> \u6216 <code>xls</code> \u7684\u6587\u4ef6\uff0c\u5176\u4ed6\u540e\u7f00\u6587\u4ef6\u5c06\u4f1a\u5ffd\u7565\u5e76\u7ed9\u51fa\u544a\u8b66\u6d88\u606f</li> </ol>"},{"location":"reader/ftpreader/","title":"Ftp Reader","text":"<p>Ftp Reader \u63d0\u4f9b\u4e86\u8bfb\u53d6\u8fdc\u7a0b FTP/SFTP \u6587\u4ef6\u7cfb\u7edf\u6570\u636e\u5b58\u50a8\u7684\u80fd\u529b\u3002</p>"},{"location":"reader/ftpreader/#_1","title":"\u529f\u80fd\u8bf4\u660e","text":""},{"location":"reader/ftpreader/#_2","title":"\u914d\u7f6e\u6837\u4f8b","text":"job/ftp2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"ftpreader\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"127.0.0.1\",\n          \"port\": 22,\n          \"username\": \"xx\",\n          \"password\": \"xxx\",\n          \"path\": [\n            \"/var/ftp/test.txt\",\n            \"/var/tmp/*.txt\",\n            \"/public/ftp\",\n            \"/public/a??.txt\"\n          ],\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"ftpWriter\",\n        \"parameter\": {\n          \"path\": \"/var/ftp/FtpWriter/result\",\n          \"fileName\": \"shihf\",\n          \"writeMode\": \"truncate\",\n          \"format\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/ftpreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 protocol \u662f string \u65e0 \u670d\u52a1\u5668\u534f\u8bae\uff0c\u76ee\u524d\u652f\u6301\u4f20\u8f93\u534f\u8bae\u6709 <code>ftp</code> \u548c <code>sftp</code> host \u662f string \u65e0 \u670d\u52a1\u5668\u5730\u5740 port \u5426 int 22/21 \u82e5\u4f20\u8f93\u534f\u8bae\u662f <code>sftp</code> \u534f\u8bae\uff0c\u9ed8\u8ba4\u503c\u662f 22\uff1b\u82e5\u4f20\u8f93\u534f\u8bae\u662f\u6807\u51c6 ftp \u534f\u8bae\uff0c\u9ed8\u8ba4\u503c\u662f 21 timeout \u5426 int 60000 \u8fde\u63a5 ftp \u670d\u52a1\u5668\u8fde\u63a5\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u6beb\u79d2(ms) connectPattern \u5426 string PASV \u8fde\u63a5\u6a21\u5f0f\uff0c\u4ec5\u652f\u6301 <code>PORT</code>, <code>PASV</code> \u6a21\u5f0f\u3002\u8be5\u53c2\u6570\u4ec5\u5728 ftp \u534f\u8bae\u65f6\u4f7f\u7528 username \u662f string \u65e0 ftp \u670d\u52a1\u5668\u8bbf\u95ee\u7528\u6237\u540d password \u5426 string \u65e0 ftp \u670d\u52a1\u5668\u8bbf\u95ee\u5bc6\u7801 useKey \u5426 boolean false \u662f\u5426\u4f7f\u7528\u79c1\u94a5\u767b\u5f55\uff0c\u4ec5\u9488\u5bf9 sftp \u767b\u5f55\u6709\u6548 keyPath \u5426 string <code>~/.ssh/id_rsa</code> \u79c1\u94a5\u5730\u5740 keyPass \u5426 string \u65e0 \u79c1\u94a5\u5bc6\u7801\uff0c\u82e5\u6ca1\u6709\u8bbe\u7f6e\u79c1\u94a5\u5bc6\u7801\uff0c\u5219\u65e0\u9700\u914d\u7f6e\u8be5\u9879 path \u662f list \u65e0 \u8fdc\u7a0b FTP \u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1\u4e0b column \u662f <code>list&lt;map&gt;</code> \u65e0 \u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c\u8be6\u89c1\u4e0b\u6587 fieldDelimiter \u662f string <code>,</code> \u63cf\u8ff0\uff1a\u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26 compress \u5426 string \u65e0 \u6587\u672c\u538b\u7f29\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e0d\u586b\u5199\u610f\u5473\u7740\u6ca1\u6709\u538b\u7f29\u3002\u652f\u6301\u538b\u7f29\u7c7b\u578b\u4e3a <code>zip</code>\u3001<code>gz</code>\u3001<code>bzip2</code> encoding \u5426 string <code>utf-8</code> \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e skipHeader \u5426 boolean false \u7c7b CSV \u683c\u5f0f\u6587\u4ef6\u53ef\u80fd\u5b58\u5728\u8868\u5934\u4e3a\u6807\u9898\u60c5\u51b5\uff0c\u9700\u8981\u8df3\u8fc7\u3002\u9ed8\u8ba4\u4e0d\u8df3\u8fc7 nullFormat \u5426 char <code>\\N</code> \u5b9a\u4e49\u54ea\u4e9b\u5b57\u7b26\u4e32\u53ef\u4ee5\u8868\u793a\u4e3a null maxTraversalLevel \u5426 int 100 \u5141\u8bb8\u904d\u5386\u6587\u4ef6\u5939\u7684\u6700\u5927\u5c42\u6570 csvReaderConfig \u5426 map \u65e0 \u8bfb\u53d6 CSV \u7c7b\u578b\u6587\u4ef6\u53c2\u6570\u914d\u7f6e\uff0cMap \u7c7b\u578b\u3002\u4e0d\u914d\u7f6e\u5219\u4f7f\u7528\u9ed8\u8ba4\u503c,\u8be6\u89c1\u4e0b\u6587"},{"location":"reader/ftpreader/#path","title":"path","text":"<p>\u8fdc\u7a0b FTP \u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\uff0c\u793a\u4f8b\u914d\u7f6e\u4e2d\u6f14\u793a\u4e86\u5982\u4f55\u586b\u5199\u591a\u4e2a\u8def\u5f84\u3002</p> <pre><code>{\n  \"path\": [\n    \"/var/ftp/test.txt\", // \u8bfb\u53d6 /var/ftp \u76ee\u5f55\u4e0b\u7684 test.txt \u6587\u4ef6\n    \"/var/tmp/*.txt\", // \u8bfb\u53d6 /var/tmp \u76ee\u5f55\u4e0b\u6240\u6709 txt \u6587\u4ef6\n    \"/public/ftp\", // \u8bfb\u53d6 /public/ftp \u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6, \u5982\u679c ftp \u662f\u6587\u4ef6\u7684\u8bdd\uff0c\u5219\u76f4\u63a5\u8bfb\u53d6\n    \"/public/a??.txt\" // \u8bfb\u53d6 /public \u76ee\u5f55\u4e0b\u6240\u6709 a \u5f00\u5934\uff0c\u540e\u9762\u8ddf\u4e24\u4e2a\u5b57\u7b26\uff0c\u6700\u540e\u662f txt \u7ed3\u5c3e\u7684\u6587\u4ef6\n  ]\n}\n</code></pre> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cAddax \u4f1a\u5c06\u4e00\u4e2a\u4f5c\u4e1a\u4e0b\u540c\u6b65\u7684\u6240\u6709 Text File \u89c6\u4f5c\u540c\u4e00\u5f20\u6570\u636e\u8868\u3002\u7528\u6237\u5fc5\u987b\u81ea\u5df1\u4fdd\u8bc1\u6240\u6709\u7684 File \u80fd\u591f\u9002\u914d\u540c\u4e00\u5957 schema \u4fe1\u606f\u3002\u8bfb\u53d6\u6587\u4ef6\u7528\u6237\u5fc5\u987b\u4fdd\u8bc1\u4e3a\u7c7b CSV \u683c\u5f0f\uff0c\u5e76\u4e14\u63d0\u4f9b\u7ed9 Addax \u6743\u9650\u53ef\u8bfb\u3002</p> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5982\u679c Path \u6307\u5b9a\u7684\u8def\u5f84\u4e0b\u6ca1\u6709\u7b26\u5408\u5339\u914d\u7684\u6587\u4ef6\u62bd\u53d6\uff0cAddax \u5c06\u62a5\u9519\u3002</p>"},{"location":"reader/ftpreader/#column","title":"column","text":"<p>\u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0cindex \u6307\u5b9a\u5f53\u524d\u5217\u6765\u81ea\u4e8e\u6587\u672c\u7b2c\u51e0\u5217(\u4ee5 0 \u5f00\u59cb)\uff0cvalue \u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece\u6e90\u5934\u6587\u4ef6\u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636e value \u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7528\u6237\u53ef\u4ee5\u5168\u90e8\u6309\u7167 String \u7c7b\u578b\u8bfb\u53d6\u6570\u636e\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\"*\"]\n}\n</code></pre> <p>\u7528\u6237\u53ef\u4ee5\u6307\u5b9a Column \u5b57\u6bb5\u4fe1\u606f\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0,\n    \"description\": \"\u4ece\u8fdc\u7a0bFTP\u6587\u4ef6\u6587\u672c\u7b2c\u4e00\u5217\u83b7\u53d6int\u5b57\u6bb5\"\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"addax\",\n    \"description\": \"\u4eceFtpReader\u5185\u90e8\u751f\u6210alibaba\u7684\u5b57\u7b26\u4e32\u5b57\u6bb5\u4f5c\u4e3a\u5f53\u524d\u5b57\u6bb5\"\n  }\n]\n</code></pre> <p>\u5bf9\u4e8e\u7528\u6237\u6307\u5b9a Column \u4fe1\u606f\uff0ctype \u5fc5\u987b\u586b\u5199\uff0cindex/value \u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</p>"},{"location":"reader/ftpreader/#csvreaderconfig","title":"csvReaderConfig","text":"<p>\u5e38\u89c1\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"csvReaderConfig\": {\n    \"safetySwitch\": false,\n    \"skipEmptyRecords\": false,\n    \"useTextQualifier\": false\n  }\n}\n</code></pre> <p>\u6240\u6709\u914d\u7f6e\u9879\u53ca\u9ed8\u8ba4\u503c,\u914d\u7f6e\u65f6 csvReaderConfig \u7684 map \u4e2d\u8bf7 \u4e25\u683c\u6309\u7167\u4ee5\u4e0b\u5b57\u6bb5\u540d\u5b57\u8fdb\u884c\u914d\u7f6e\uff1a</p> <pre><code>boolean caseSensitive = true;\nchar textQualifier = 34;\nboolean trimWhitespace = true;\nboolean useTextQualifier = true;//\u662f\u5426\u4f7f\u7528csv\u8f6c\u4e49\u5b57\u7b26\nchar delimiter = 44;//\u5206\u9694\u7b26\nchar recordDelimiter = 0;\nchar comment = 35;\nboolean useComments = false;\nint escapeMode = 1;\nboolean safetySwitch = true;//\u5355\u5217\u957f\u5ea6\u662f\u5426\u9650\u5236100000\u5b57\u7b26\nboolean skipEmptyRecords = true;//\u662f\u5426\u8df3\u8fc7\u7a7a\u884c\nboolean captureRawRecord = true;\n</code></pre>"},{"location":"reader/ftpreader/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u8fdc\u7a0b FTP \u6587\u4ef6\u672c\u8eab\u4e0d\u63d0\u4f9b\u6570\u636e\u7c7b\u578b\uff0c\u8be5\u7c7b\u578b\u662f Addax FtpReader \u5b9a\u4e49\uff1a</p> Addax \u5185\u90e8\u7c7b\u578b \u8fdc\u7a0b FTP \u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long Double Double String String Boolean Boolean Date Date <p>\u5176\u4e2d\uff1a</p> <ul> <li>Long \u662f\u6307\u8fdc\u7a0b FTP \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528\u6574\u5f62\u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 \"19901219\"\u3002</li> <li>Double \u662f\u6307\u8fdc\u7a0b FTP \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Double \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 \"3.1415\"\u3002</li> <li>Boolean \u662f\u6307\u8fdc\u7a0b FTP \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Boolean \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 \"true\"\u3001\"false\"\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u3002</li> <li>Date \u662f\u6307\u8fdc\u7a0b FTP \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Date \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 \"2014-12-31\"\uff0cDate \u53ef\u4ee5\u6307\u5b9a format \u683c\u5f0f\u3002</li> </ul>"},{"location":"reader/ftpreader/#_5","title":"\u9650\u5236","text":"<ol> <li>\u5355\u4e2a File \u652f\u6301\u591a\u7ebf\u7a0b\u5e76\u53d1\u8bfb\u53d6\uff0c\u8fd9\u91cc\u6d89\u53ca\u5230\u5355\u4e2a File \u5185\u90e8\u5207\u5206\u7b97\u6cd5</li> <li>\u5355\u4e2a File \u5728\u538b\u7f29\u60c5\u51b5\u4e0b\uff0c\u4ece\u6280\u672f\u4e0a\u65e0\u6cd5\u652f\u6301\u591a\u7ebf\u7a0b\u5e76\u53d1\u8bfb\u53d6\u3002</li> </ol>"},{"location":"reader/hanareader/","title":"HANA Reader","text":"<p>HANA Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece SAP HANA \u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b</p>"},{"location":"reader/hanareader/#_1","title":"\u793a\u4f8b","text":"<p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/hanareader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hanareader\",\n          \"parameter\": {\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": {\n              \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n              \"table\": [\n                \"addax_tbl\"\n              ]\n            },\n            \"username\": \"system\",\n            \"password\": \"HXEHana1\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/hana2stream.json</code></p>"},{"location":"reader/hanareader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"reader/hanareader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u53c2\u6570\u3002</p>"},{"location":"reader/hbase11xreader/","title":"HBase11X Reader","text":"<p>HBase11X Reader \u63d2\u4ef6\u652f\u6301\u4ece HBase 1.x \u7248\u672c\u8bfb\u53d6\u6570\u636e\uff0c \u5176\u5b9e\u73b0\u65b9\u5f0f\u4e3a \u901a\u8fc7 HBase \u7684 Java \u5ba2\u6237\u7aef\u8fde\u63a5\u8fdc\u7a0b HBase \u670d\u52a1\uff0c\u5e76\u901a\u8fc7 Scan \u65b9\u5f0f\u8bfb\u53d6\u4f60\u6307\u5b9a <code>rowkey</code> \u8303\u56f4\u5185\u7684\u6570\u636e\u3002</p>"},{"location":"reader/hbase11xreader/#_1","title":"\u914d\u7f6e","text":""},{"location":"reader/hbase11xreader/#_2","title":"\u5efa\u8868\u4ee5\u53ca\u586b\u5145\u6570\u636e","text":"<p>\u4ee5\u4e0b\u6f14\u793a\u57fa\u4e8e\u4e0b\u9762\u521b\u5efa\u7684\u8868\u4ee5\u53ca\u6570\u636e</p> <pre><code>create 'users', 'address','info'\nput 'users', 'lisi', 'address:country', 'china'\nput 'users', 'lisi', 'address:province',    'beijing'\nput 'users', 'lisi', 'info:age',        27\nput 'users', 'lisi', 'info:birthday',   '1987-06-17'\nput 'users', 'lisi', 'info:company',    'baidu'\nput 'users', 'xiaoming', 'address:city',    'hangzhou'\nput 'users', 'xiaoming', 'address:country', 'china'\nput 'users', 'xiaoming', 'address:province',    'zhejiang'\nput 'users', 'xiaoming', 'info:age',        29\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17'\nput 'users', 'xiaoming', 'info:company',    'alibaba'\n</code></pre>"},{"location":"reader/hbase11xreader/#normal","title":"normal \u6a21\u5f0f","text":"<p>\u628a HBase \u4e2d\u7684\u8868\uff0c\u5f53\u6210\u666e\u901a\u4e8c\u7ef4\u8868\uff08\u6a2a\u8868\uff09\u8fdb\u884c\u8bfb\u53d6,\u8bfb\u53d6\u6700\u65b0\u7248\u672c\u6570\u636e\u3002\u5982\uff1a</p> <pre><code>hbase(main):017:0&gt; scan 'users'\nROW           COLUMN+CELL\n lisi         column=address:city, timestamp=1457101972764, value=beijing\n lisi         column=address:country, timestamp=1457102773908, value=china\n lisi         column=address:province, timestamp=1457101972736, value=beijing\n lisi         column=info:age, timestamp=1457101972548, value=27\n lisi         column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi         column=info:company, timestamp=1457101972653, value=baidu\n xiaoming     column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming     column=address:country, timestamp=1457082195729, value=china\n xiaoming     column=address:province, timestamp=1457082195773, value=zhejiang\n xiaoming     column=info:age, timestamp=1457082218735, value=29\n xiaoming     column=info:birthday, timestamp=1457082186830, value=1987-06-17\n xiaoming     column=info:company, timestamp=1457082189826, value=alibaba\n2 row(s) in 0.0580 seconds\n</code></pre> <p>\u8bfb\u53d6\u540e\u6570\u636e</p> rowKey addres:city address:country address:province info:age info:birthday info:company lisi beijing china beijing 27 1987-06-17 baidu xiaoming hangzhou china zhejiang 29 1987-06-17 alibaba"},{"location":"reader/hbase11xreader/#multiversionfixedcolumn","title":"multiVersionFixedColumn \u6a21\u5f0f","text":"<p>\u628a HBase \u4e2d\u7684\u8868\uff0c\u5f53\u6210\u7ad6\u8868\u8fdb\u884c\u8bfb\u53d6\u3002\u8bfb\u51fa\u7684\u6bcf\u6761\u8bb0\u5f55\u4e00\u5b9a\u662f\u56db\u5217\u5f62\u5f0f\uff0c\u4f9d\u6b21\u4e3a\uff1a<code>rowKey</code>\uff0c<code>family:qualifier</code>\uff0c<code>timestamp</code>\uff0c<code>value</code>\u3002</p> <p>\u8bfb\u53d6\u65f6\u9700\u8981\u660e\u786e\u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u5217\uff0c\u628a\u6bcf\u4e00\u4e2a cell \u4e2d\u7684\u503c\uff0c\u4f5c\u4e3a\u4e00\u6761\u8bb0\u5f55\uff08record\uff09\uff0c\u82e5\u6709\u591a\u4e2a\u7248\u672c\u5c31\u6709\u591a\u6761\u8bb0\u5f55\uff08record\uff09\u3002\u5982\uff1a</p> <pre><code>hbase(main):018:0&gt; scan 'users',{VERSIONS=&gt;5}\nROW               COLUMN+CELL\n lisi             column=address:city, timestamp=1457101972764, value=beijing\n lisi             column=address:contry, timestamp=1457102773908, value=china\n lisi             column=address:province, timestamp=1457101972736, value=beijing\n lisi             column=info:age, timestamp=1457101972548, value=27\n lisi             column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi             column=info:company, timestamp=1457101972653, value=baidu\n xiaoming         column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming         column=address:contry, timestamp=1457082195729, value=china\n xiaoming         column=address:province, timestamp=1457082195773, value=zhejiang\n xiaoming         column=info:age, timestamp=1457082218735, value=29\n xiaoming         column=info:age, timestamp=1457082178630, value=24\n xiaoming         column=info:birthday, timestamp=1457082186830, value=1987-06-17\n xiaoming         column=info:company, timestamp=1457082189826, value=alibaba\n2 row(s) in 0.0260 seconds\n</code></pre> <p>\u8bfb\u53d6\u540e\u6570\u636e(4 \u5217)</p> rowKey column:qualifier timestamp value lisi address:city 1457101972764 beijing lisi address:contry 1457102773908 china lisi address:province 1457101972736 beijing lisi info:age 1457101972548 27 lisi info:birthday 1457101972604 1987-06-17 lisi info:company 1457101972653 beijing xiaoming address:city 1457082196082 hangzhou xiaoming address:contry 1457082195729 china xiaoming address:province 1457082195773 zhejiang xiaoming info:age 1457082218735 29 xiaoming info:age 1457082178630 24 xiaoming info:birthday 1457082186830 1987-06-17 xiaoming info:company 1457082189826 alibaba"},{"location":"reader/hbase11xreader/#_3","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ece HBase \u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a\uff0c\u5206\u522b\u4e3a\u6807\u51c6\u6a21\u5f0f\u548c\u591a\u7248\u672c\u6a21\u5f0f</p> job/hbase11x2stream_normal.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hbase11xreader\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"xxxf\"\n          },\n          \"table\": \"users\",\n          \"encoding\": \"utf-8\",\n          \"mode\": \"normal\",\n          \"column\": [\n            {\n              \"name\": \"rowkey\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: age\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: birthday\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            {\n              \"name\": \"info: company\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: country\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: province\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: city\",\n              \"type\": \"string\"\n            }\n          ],\n          \"range\": {\n            \"startRowkey\": \"\",\n            \"endRowkey\": \"\",\n            \"isBinaryRowkey\": true\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/Users/shf/workplace/addax_test/hbase11xreader/result\",\n          \"fileName\": \"qiran\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre> job/hbase11x2srtream_version.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hbase11xreader\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"127.0.0.1:2181\"\n          },\n          \"table\": \"users\",\n          \"encoding\": \"utf-8\",\n          \"mode\": \"multiVersionFixedColumn\",\n          \"maxVersion\": \"-1\",\n          \"column\": [\n            {\n              \"name\": \"rowkey\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: age\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: birthday\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            {\n              \"name\": \"info: company\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: contry\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: province\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: city\",\n              \"type\": \"string\"\n            }\n          ],\n          \"range\": {\n            \"startRowkey\": \"\",\n            \"endRowkey\": \"\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/Users/shf/workplace/addax_test/hbase11xreader/result\",\n          \"fileName\": \"qiran\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/hbase11xreader/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 hbaseConfig \u662f map \u65e0 \u8fde\u63a5 HBase \u96c6\u7fa4\u9700\u8981\u7684\u914d\u7f6e\u4fe1\u606f, <code>hbase.zookeeper.quorum</code> \u4e3a\u5fc5\u586b\u9879\uff0c\u5176\u4ed6 client \u7684\u914d\u7f6e\u4e3a\u53ef\u9009\u9879 mode \u662f string \u65e0 \u8bfb\u53d6 HBase \u7684\u6a21\u5f0f\uff0c\u53ef\u586b\u5199 <code>normal</code> \u6216 <code>multiVersionFixedColumn</code> table \u662f string \u65e0 \u8981\u8bfb\u53d6\u7684 hbase \u8868\u540d\uff08\u5927\u5c0f\u5199\u654f\u611f\uff09 encoding \u5426 string UTF-8 \u7f16\u7801\u65b9\u5f0f\uff0c<code>UTF-8</code> \u6216\u662f <code>GBK</code>\uff0c\u7528\u4e8e\u5bf9\u4e8c\u8fdb\u5236\u5b58\u50a8\u7684 <code>HBase byte[]</code> \u8f6c\u4e3a String \u65f6\u7684\u7f16\u7801 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u8981\u8bfb\u53d6\u7684 hbase \u5b57\u6bb5\uff0cnormal \u6a21\u5f0f\u4e0e multiVersionFixedColumn \u6a21\u5f0f\u4e0b\u5fc5\u586b\u9879, \u8be6\u7ec6\u8bf4\u660e\u89c1\u4e0b\u6587 maxVersion \u662f string \u65e0 \u6307\u5b9a\u5728\u591a\u7248\u672c\u6a21\u5f0f\u4e0b\u8bfb\u53d6\u7684\u7248\u672c\u6570\uff0c<code>-1</code> \u8868\u793a\u8bfb\u53d6\u6240\u6709\u7248\u672c, <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b range \u5426 string \u65e0 \u6307\u5b9a\u8bfb\u53d6\u7684<code>rowkey</code> \u8303\u56f4, \u8be6\u89c1\u4e0b\u6587 scanCacheSize \u5426 int 256 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u884c\u6570 scanBatchSize \u5426 int 100 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u5217\u6570"},{"location":"reader/hbase11xreader/#column","title":"column","text":"<p>\u63cf\u8ff0\uff1a\u8981\u8bfb\u53d6\u7684 hbase \u5b57\u6bb5\uff0cnormal \u6a21\u5f0f\u4e0e multiVersionFixedColumn \u6a21\u5f0f\u4e0b\u5fc5\u586b\u9879\u3002</p>"},{"location":"reader/hbase11xreader/#normal_1","title":"normal \u6a21\u5f0f","text":"<p><code>name</code> \u6307\u5b9a\u8bfb\u53d6\u7684 hbase \u5217\uff0c\u9664\u4e86 <code>rowkey</code> \u5916\uff0c\u5fc5\u987b\u4e3a <code>\u5217\u65cf:\u5217\u540d</code> \u7684\u683c\u5f0f\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>format</code>\u6307\u5b9a\u65e5\u671f\u7c7b\u578b\u7684\u683c\u5f0f\uff0c <code>value</code> \u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece hbase \u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636e <code>value</code> \u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    {\n      \"name\": \"rowkey\",\n      \"type\": \"string\"\n    },\n    {\n      \"value\": \"test\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre> <p>normal \u6a21\u5f0f\u4e0b\uff0c\u5bf9\u4e8e\u7528\u6237\u6307\u5b9a Column \u4fe1\u606f\uff0ctype \u5fc5\u987b\u586b\u5199\uff0cname/value \u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</p>"},{"location":"reader/hbase11xreader/#multiversionfixedcolumn_1","title":"multiVersionFixedColumn \u6a21\u5f0f","text":"<p><code>name</code> \u6307\u5b9a\u8bfb\u53d6\u7684 hbase \u5217\uff0c\u9664\u4e86 <code>rowkey</code> \u5916\uff0c\u5fc5\u987b\u4e3a <code>\u5217\u65cf:\u5217\u540d</code> \u7684\u683c\u5f0f\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>format</code>\u6307\u5b9a\u65e5\u671f\u7c7b\u578b\u7684\u683c\u5f0f \u3002 multiVersionFixedColumn \u6a21\u5f0f\u4e0b\u4e0d\u652f\u6301\u5e38\u91cf\u5217\u3002\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    {\n      \"name\": \"rowkey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"info: age\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reader/hbase11xreader/#range","title":"range","text":"<p>\u6307\u5b9a\u8bfb\u53d6\u7684 <code>rowkey</code> \u8303\u56f4</p> <ul> <li><code>startRowkey</code>\uff1a\u6307\u5b9a\u5f00\u59cb <code>rowkey</code></li> <li><code>endRowkey</code> \u6307\u5b9a\u7ed3\u675f <code>rowkey</code></li> <li><code>isBinaryRowkey</code>\uff1a\u6307\u5b9a\u914d\u7f6e\u7684 <code>startRowkey</code> \u548c <code>endRowkey</code> \u8f6c\u6362\u4e3a <code>byte[]</code> \u65f6\u7684\u65b9\u5f0f\uff0c\u9ed8\u8ba4\u503c\u4e3a false,\u82e5\u4e3a true\uff0c\u5219\u8c03\u7528 <code>Bytes.toBytesBinary(rowkey)</code> \u65b9\u6cd5\u8fdb\u884c\u8f6c\u6362; \u82e5\u4e3a false\uff1a\u5219\u8c03\u7528 <code>Bytes.toBytes(rowkey)</code></li> </ul> <p>\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"range\": {\n    \"startRowkey\": \"aaa\",\n    \"endRowkey\": \"ccc\",\n    \"isBinaryRowkey\": false\n  }\n}\n</code></pre>"},{"location":"reader/hbase11xreader/#_5","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u4e0b\u9762\u5217\u51fa\u652f\u6301\u7684\u8bfb\u53d6 HBase \u6570\u636e\u7c7b\u578b\uff0cHbaseReader \u9488\u5bf9 HBase \u7c7b\u578b\u8f6c\u6362\u5217\u8868:</p> Addax \u5185\u90e8\u7c7b\u578b HBase \u6570\u636e\u7c7b\u578b Long int, short ,long Double float, double String string, binarystring Date date Boolean boolean <p>\u8bf7\u6ce8\u610f:</p> <p><code>\u9664\u4e0a\u8ff0\u7f57\u5217\u5b57\u6bb5\u7c7b\u578b\u5916\uff0c\u5176\u4ed6\u7c7b\u578b\u5747\u4e0d\u652f\u6301</code></p>"},{"location":"reader/hbase11xreader/#_6","title":"\u9650\u5236","text":"<ol> <li>\u76ee\u524d\u4e0d\u652f\u6301\u52a8\u6001\u5217\u7684\u8bfb\u53d6\u3002\u8003\u8651\u7f51\u7edc\u4f20\u8f93\u6d41\u91cf\uff08\u652f\u6301\u52a8\u6001\u5217\uff0c\u9700\u8981\u5148\u5c06 hbase \u6240\u6709\u5217\u7684\u6570\u636e\u8bfb\u53d6\u51fa\u6765\uff0c\u518d\u6309\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff09\uff0c\u73b0\u652f\u6301\u7684\u4e24\u79cd\u8bfb\u53d6\u6a21\u5f0f\u4e2d\u9700\u8981\u7528\u6237\u660e\u786e\u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u5217\u3002</li> <li>\u5173\u4e8e\u540c\u6b65\u4f5c\u4e1a\u7684\u5207\u5206\uff1a\u76ee\u524d\u7684\u5207\u5206\u65b9\u5f0f\u662f\u6839\u636e\u7528\u6237 hbase \u8868\u6570\u636e\u7684 region \u5206\u5e03\u8fdb\u884c\u5207\u5206\u3002\u5373\uff1a\u5728\u7528\u6237\u586b\u5199\u7684 <code>[startrowkey\uff0cendrowkey\uff3d</code> \u8303\u56f4\u5185\uff0c\u4e00\u4e2a region \u4f1a\u5207\u5206\u6210\u4e00\u4e2a task\uff0c\u5355\u4e2a region \u4e0d\u8fdb\u884c\u5207\u5206\u3002</li> <li>multiVersionFixedColumn \u6a21\u5f0f\u4e0b\u4e0d\u652f\u6301\u589e\u52a0\u5e38\u91cf\u5217</li> </ol>"},{"location":"reader/hbase11xsqlreader/","title":"HBase11x SQL Reader","text":"<p>HBase11x SQL Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Phoenix(HBase SQL)\u8bfb\u53d6\u6570\u636e, \u652f\u6301\u7684 HBase \u7248\u672c\u4e3a 1.x</p>"},{"location":"reader/hbase11xsqlreader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ecePhoenix\u540c\u6b65\u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a:</p> <pre><code>{\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"byte\":-1,\n              \"channel\": 1\n            }\n        },\n        \"content\": [ {\n                \"reader\": {\n                    \"name\": \"hbase11xsqlreader\",\n                    \"parameter\": {\n                        \"hbaseConfig\": {\n                            \"hbase.zookeeper.quorum\": \"node1,node2,node3\"\n                        },\n                        \"table\": \"US_POPULATION\",\n                        \"column\": [],\n                        \"where\": \"1=1\",\n                        \"querySql\": \"\"\n                    }\n                },\n                \"writer\": {\n                    \"name\": \"streamwriter\",\n                    \"parameter\": {\n                        \"print\":true,\n                        \"encoding\": \"UTF-8\"\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"reader/hbase11xsqlreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 hbaseConfig \u662f map \u65e0 \u9700\u8981\u901a\u8fc7 Phoenix \u5ba2\u6237\u7aef\u53bb\u8fde\u63a5 hbase \u96c6\u7fa4\uff0c\u56e0\u6b64\u8fd9\u91cc\u9700\u8981\u586b\u5199\u5bf9\u5e94 hbase \u96c6\u7fa4\u7684 <code>zkurl</code>\u5730\u5740 table \u662f string \u65e0 \u6307\u5b9a Phoenix \u4e2d\u7684\u8868\u540d,\u5982\u679c\u6709 namespace\uff0c\u8be5\u503c\u8bbe\u7f6e\u4e3a <code>namespace.tablename</code> querySql \u5426 string \u65e0 \u4e0d\u662f\u76f4\u63a5\u67e5\u8be2\u8868\uff0c\u800c\u662f\u63d0\u4f9b\u5177\u4f53\u7684\u67e5\u8be2\u8bed\u53e5\uff0c\u5982\u679c\u8be5\u53c2\u6570\u548c <code>table</code> \u53c2\u6570\u540c\u65f6\u5b58\u5728\uff0c\u5219\u4f18\u5148\u4f7f\u7528\u8be5\u53c2\u6570 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u586b\u5199\u9700\u8981\u4ecephoenix\u8868\u4e2d\u8bfb\u53d6\u7684\u5217\u540d\u96c6\u5408\uff0c\u4f7f\u7528JSON\u7684\u6570\u7ec4\u63cf\u8ff0\u5b57\u6bb5\u4fe1\u606f\uff0c\u7a7a\u503c\u6216 <code>\"*\"</code> \u8868\u793a\u8bfb\u53d6\u6240\u6709\u5217 where \u5426 string \u65e0 <code>where</code> \u6761\u4ef6"},{"location":"reader/hbase11xsqlreader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u76ee\u524d\u652f\u6301\u5927\u90e8\u5206 Phoenix\u7c7b\u578b\uff0c\u4f46\u4e5f\u5b58\u5728\u90e8\u5206\u4e2a\u522b\u7c7b\u578b\u6ca1\u6709\u652f\u6301\u7684\u60c5\u51b5\uff0c\u8bf7\u6ce8\u610f\u68c0\u67e5\u4f60\u7684\u7c7b\u578b\u3002</p> <p>\u4e0b\u9762\u5217\u51fa\u7c7b\u578b\u8f6c\u6362\u5217\u8868:</p> Addax \u5185\u90e8\u7c7b\u578b Phoenix \u6570\u636e\u7c7b\u578b String CHAR, VARCHAR Bytes BINARY, VARBINARY Bool BOOLEAN Long INTEGER, TINYINT, SMALLINT, BIGINT Double FLOAT, DECIMAL, DOUBLE, Date DATE, TIME, TIMESTAMP"},{"location":"reader/hbase20xreader/","title":"HBase20 Reader","text":"<p>HBase20 Reader \u63d2\u4ef6\u652f\u6301\u4ece HBase 2.x \u7248\u672c\u8bfb\u53d6\u6570\u636e\uff0c \u5176\u5b9e\u73b0\u65b9\u5f0f\u4e3a \u901a\u8fc7 HBase \u7684 Java \u5ba2\u6237\u7aef\u8fde\u63a5\u8fdc\u7a0b HBase \u670d\u52a1\uff0c\u5e76\u901a\u8fc7 Scan \u65b9\u5f0f\u8bfb\u53d6\u4f60\u6307\u5b9a <code>rowkey</code> \u8303\u56f4\u5185\u7684\u6570\u636e\u3002</p>"},{"location":"reader/hbase20xreader/#_1","title":"\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u6f14\u793a\u57fa\u4e8e\u4e0b\u9762\u521b\u5efa\u7684\u8868\u4ee5\u53ca\u6570\u636e</p> <pre><code>create 'users', {NAME=&gt;'address', VERSIONS=&gt;100},{NAME=&gt;'info',VERSIONS=&gt;1000}\nput 'users', 'lisi', 'address:country', 'china1', 20200101\nput 'users', 'lisi', 'address:province',    'beijing1', 20200101\nput 'users', 'lisi', 'info:age',        27, 20200101\nput 'users', 'lisi', 'info:birthday',   '1987-06-17', 20200101\nput 'users', 'lisi', 'info:company',    'baidu1', 20200101\nput 'users', 'xiaoming', 'address:city',    'hangzhou1', 20200101\nput 'users', 'xiaoming', 'address:country', 'china1', 20200101\nput 'users', 'xiaoming', 'address:province',    'zhejiang1',20200101\nput 'users', 'xiaoming', 'info:age',        29, 20200101\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17',20200101\nput 'users', 'xiaoming', 'info:company',    'alibaba1', 20200101\nput 'users', 'lisi', 'address:country', 'china2', 20200102\nput 'users', 'lisi', 'address:province',    'beijing2', 20200102\nput 'users', 'lisi', 'info:age',        27, 20200102\nput 'users', 'lisi', 'info:birthday',   '1987-06-17', 20200102\nput 'users', 'lisi', 'info:company',    'baidu2', 20200102\nput 'users', 'xiaoming', 'address:city',    'hangzhou2', 20200102\nput 'users', 'xiaoming', 'address:country', 'china2', 20200102\nput 'users', 'xiaoming', 'address:province',    'zhejiang2', 20200102\nput 'users', 'xiaoming', 'info:age',        29, 20200102\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17', 20200102\nput 'users', 'xiaoming', 'info:company',    'alibaba2', 20200102\n</code></pre>"},{"location":"reader/hbase20xreader/#normal","title":"normal \u6a21\u5f0f","text":"<p>\u628aHBase\u4e2d\u7684\u8868\uff0c\u5f53\u6210\u666e\u901a\u4e8c\u7ef4\u8868\uff08\u6a2a\u8868\uff09\u8fdb\u884c\u8bfb\u53d6,\u8bfb\u53d6\u6700\u65b0\u7248\u672c\u6570\u636e\u3002\u5982\uff1a</p> <pre><code>hbase(main):017:0&gt; scan 'users'\nROW           COLUMN+CELL\n lisi         column=address:city, timestamp=1457101972764, value=beijing\n lisi         column=address:country, timestamp=1457102773908, value=china\n lisi         column=address:province, timestamp=1457101972736, value=beijing\n lisi         column=info:age, timestamp=1457101972548, value=27\n lisi         column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi         column=info:company, timestamp=1457101972653, value=baidu\n xiaoming     column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming     column=address:country, timestamp=1457082195729, value=china\n xiaoming     column=address:province, timestamp=1457082195773, value=zhejiang\n xiaoming     column=info:age, timestamp=1457082218735, value=29\n xiaoming     column=info:birthday, timestamp=1457082186830, value=1987-06-17\n xiaoming     column=info:company, timestamp=1457082189826, value=alibaba\n2 row(s) in 0.0580 seconds\n</code></pre> <p>\u8bfb\u53d6\u540e\u6570\u636e</p> rowKey addres:city address:country address:province info:age info:birthday info:company lisi beijing china beijing 27 1987-06-17 baidu xiaoming hangzhou china zhejiang 29 1987-06-17 alibaba"},{"location":"reader/hbase20xreader/#multiversionfixedcolumn","title":"multiVersionFixedColumn \u6a21\u5f0f","text":"<p>\u628aHBase\u4e2d\u7684\u8868\uff0c\u5f53\u6210\u7ad6\u8868\u8fdb\u884c\u8bfb\u53d6\u3002\u8bfb\u51fa\u7684\u6bcf\u6761\u8bb0\u5f55\u4e00\u5b9a\u662f\u56db\u5217\u5f62\u5f0f\uff0c\u4f9d\u6b21\u4e3a\uff1a<code>rowKey</code>\uff0c<code>family:qualifier</code>\uff0c<code>timestamp</code>\uff0c<code>value</code>\u3002</p> <p>\u8bfb\u53d6\u65f6\u9700\u8981\u660e\u786e\u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u5217\uff0c\u628a\u6bcf\u4e00\u4e2a cell \u4e2d\u7684\u503c\uff0c\u4f5c\u4e3a\u4e00\u6761\u8bb0\u5f55\uff08record\uff09\uff0c\u82e5\u6709\u591a\u4e2a\u7248\u672c\u5c31\u6709\u591a\u6761\u8bb0\u5f55\uff08record\uff09\u3002\u5982\uff1a</p> <pre><code>hbase(main):018:0&gt; scan 'users',{VERSIONS=&gt;5}\nROW              COLUMN+CELL\n lisi            column=address:city, timestamp=1457101972764, value=beijing\n lisi            column=address:contry, timestamp=1457102773908, value=china\n lisi            column=address:province, timestamp=1457101972736, value=beijing\n lisi            column=info:age, timestamp=1457101972548, value=27\n lisi            column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi            column=info:company, timestamp=1457101972653, value=baidu\n xiaoming        column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming        column=address:contry, timestamp=1457082195729, value=china\n xiaoming        column=address:province, timestamp=1457082195773, value=zhejiang\n xiaoming        column=info:age, timestamp=1457082218735, value=29\n xiaoming        column=info:age, timestamp=1457082178630, value=24\n xiaoming        column=info:birthday, timestamp=1457082186830, value=1987-06-17\n xiaoming        column=info:company, timestamp=1457082189826, value=alibaba\n2 row(s) in 0.0260 seconds\n</code></pre> <p>\u8bfb\u53d6\u540e\u6570\u636e(4\u5217)</p> rowKey column:qualifier timestamp value lisi address:city 1457101972764 beijing lisi address:contry 1457102773908 china lisi address:province 1457101972736 beijing lisi info:age 1457101972548 27 lisi info:birthday 1457101972604 1987-06-17 lisi info:company 1457101972653 beijing xiaoming address:city 1457082196082 hangzhou xiaoming address:contry 1457082195729 china xiaoming address:province 1457082195773 zhejiang xiaoming info:age 1457082218735 29 xiaoming info:age 1457082178630 24 xiaoming info:birthday 1457082186830 1987-06-17 xiaoming info:company 1457082189826 alibaba <p>\u914d\u7f6e\u4e00\u4e2a\u4ece HBase \u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a\uff0c\u5206\u522b\u4e3a\u6807\u51c6\u6a21\u5f0f\u548c\u591a\u7248\u672c\u6a21\u5f0f</p> hbase20x2stream_normal.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hbase11xreader\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"xxxf\"\n          },\n          \"table\": \"users\",\n          \"encoding\": \"utf-8\",\n          \"mode\": \"normal\",\n          \"column\": [\n            {\n              \"name\": \"rowkey\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: age\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: birthday\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            {\n              \"name\": \"info: company\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: country\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: province\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: city\",\n              \"type\": \"string\"\n            }\n          ],\n          \"range\": {\n            \"startRowkey\": \"\",\n            \"endRowkey\": \"\",\n            \"isBinaryRowkey\": true\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/Users/shf/workplace/addax_test/hbase11xreader/result\",\n          \"fileName\": \"qiran\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre> hbase20x2stream_version.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hbase11xreader\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"xxx\"\n          },\n          \"table\": \"users\",\n          \"encoding\": \"utf-8\",\n          \"mode\": \"multiVersionFixedColumn\",\n          \"maxVersion\": \"-1\",\n          \"column\": [\n            {\n              \"name\": \"rowkey\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: age\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"info: birthday\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            {\n              \"name\": \"info: company\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: contry\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: province\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"address: city\",\n              \"type\": \"string\"\n            }\n          ],\n          \"range\": {\n            \"startRowkey\": \"\",\n            \"endRowkey\": \"\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/Users/shf/workplace/addax_test/hbase11xreader/result\",\n          \"fileName\": \"qiran\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/hbase20xreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 hbaseConfig \u662f map \u65e0 \u8fde\u63a5HBase\u96c6\u7fa4\u9700\u8981\u7684\u914d\u7f6e\u4fe1\u606f, <code>hbase.zookeeper.quorum</code> \u4e3a\u5fc5\u586b\u9879\uff0c\u5176\u4ed6\u4e3a\u53ef\u9009\u9879 mode \u662f string \u65e0 \u8bfb\u53d6hbase\u7684\u6a21\u5f0f\uff0c\u53ef\u586b\u5199 <code>normal</code> \u6216 <code>multiVersionFixedColumn</code> table \u662f string \u65e0 \u8981\u8bfb\u53d6\u7684 hbase \u8868\u540d\uff08\u5927\u5c0f\u5199\u654f\u611f\uff09 encoding \u5426 string UTF-8 \u7f16\u7801\u65b9\u5f0f\uff0c<code>UTF-8</code> \u6216\u662f <code>GBK</code>\uff0c\u7528\u4e8e\u5bf9\u4e8c\u8fdb\u5236\u5b58\u50a8\u7684 <code>HBase byte[]</code> \u8f6c\u4e3a <code>String</code> \u65f6\u7684\u7f16\u7801 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u8981\u8bfb\u53d6\u7684\u5b57\u6bb5\uff0c<code>normal</code> \u6a21\u5f0f\u4e0e <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b\u9879, \u8be6\u7ec6\u8bf4\u660e\u89c1\u4e0b\u6587 maxVersion \u662f string \u65e0 \u6307\u5b9a\u5728\u591a\u7248\u672c\u6a21\u5f0f\u4e0b\u8bfb\u53d6\u7684\u7248\u672c\u6570\uff0c<code>-1</code> \u8868\u793a\u8bfb\u53d6\u6240\u6709\u7248\u672c, <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b range \u5426 string \u65e0 \u6307\u5b9a\u8bfb\u53d6\u7684 <code>rowkey</code> \u8303\u56f4, \u8be6\u89c1\u4e0b\u6587 scanCacheSize \u5426 int 256 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u884c\u6570 scanBatchSize \u5426 int 100 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u5217\u6570"},{"location":"reader/hbase20xreader/#column","title":"column","text":"<p>\u63cf\u8ff0\uff1a\u8981\u8bfb\u53d6\u7684\u5b57\u6bb5\uff0c<code>normal</code> \u6a21\u5f0f\u4e0e <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b\u9879\u3002</p>"},{"location":"reader/hbase20xreader/#normal_1","title":"normal \u6a21\u5f0f","text":"<p><code>name</code> \u6307\u5b9a\u8bfb\u53d6\u7684 hbase \u5217\uff0c\u9664\u4e86 <code>rowkey</code> \u5916\uff0c\u5fc5\u987b\u4e3a <code>\u5217\u65cf:\u5217\u540d</code> \u7684\u683c\u5f0f\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>format</code>\u6307\u5b9a\u65e5\u671f\u7c7b\u578b\u7684\u683c\u5f0f\uff0c <code>value</code> \u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece hbase \u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636e <code>value</code>  \u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    {\n      \"name\": \"rowkey\",\n      \"type\": \"string\"\n    },\n    {\n      \"value\": \"test\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre> <p>normal \u6a21\u5f0f\u4e0b\uff0c\u5bf9\u4e8e\u7528\u6237\u6307\u5b9aColumn\u4fe1\u606f\uff0ctype\u5fc5\u987b\u586b\u5199\uff0cname/value\u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</p>"},{"location":"reader/hbase20xreader/#multiversionfixedcolumn_1","title":"multiVersionFixedColumn \u6a21\u5f0f","text":"<p><code>name</code> \u6307\u5b9a\u8bfb\u53d6\u7684 hbase \u5217\uff0c\u9664\u4e86 <code>rowkey</code> \u5916\uff0c\u5fc5\u987b\u4e3a <code>\u5217\u65cf:\u5217\u540d</code> \u7684\u683c\u5f0f\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>format</code>\u6307\u5b9a\u65e5\u671f\u7c7b\u578b\u7684\u683c\u5f0f \u3002 <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u4e0d\u652f\u6301\u5e38\u91cf\u5217\u3002 \u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"mode\": \"multiVersionFixedColumn\",\n  \"maxVersion\": 3,\n  \"column\": [\n    {\n      \"name\": \"rowkey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"info: age\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reader/hbase20xreader/#range","title":"range","text":"<p>\u6307\u5b9a\u8bfb\u53d6\u7684 <code>rowkey</code> \u8303\u56f4</p> <ul> <li><code>startRowkey</code>\uff1a\u6307\u5b9a\u5f00\u59cb <code>rowkey</code></li> <li><code>endRowkey</code>: \u6307\u5b9a\u7ed3\u675f <code>rowkey</code></li> <li><code>isBinaryRowkey</code>\uff1a\u6307\u5b9a\u914d\u7f6e\u7684 <code>startRowkey</code> \u548c <code>endRowkey</code> \u8f6c\u6362\u4e3a<code>byte[]</code>\u65f6\u7684\u65b9\u5f0f\uff0c\u9ed8\u8ba4\u503c\u4e3afalse,\u82e5\u4e3atrue\uff0c\u5219\u8c03\u7528<code>Bytes.toBytesBinary(rowkey)</code>\u65b9\u6cd5\u8fdb\u884c\u8f6c\u6362;\u82e5\u4e3afalse\uff1a\u5219\u8c03\u7528<code>Bytes.toBytes(rowkey)</code></li> </ul> <p>\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"range\": {\n    \"startRowkey\": \"aaa\",\n    \"endRowkey\": \"ccc\",\n    \"isBinaryRowkey\": false\n  }\n}\n</code></pre>"},{"location":"reader/hbase20xreader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u4e0b\u9762\u5217\u51fa\u652f\u6301\u7684\u8bfb\u53d6HBase\u6570\u636e\u7c7b\u578b:</p> Addax \u5185\u90e8\u7c7b\u578b HBase \u6570\u636e\u7c7b\u578b Long int, short ,long Double float, double String string, binarystring Date date Boolean boolean <p>\u8bf7\u6ce8\u610f:</p> <p><code>\u9664\u4e0a\u8ff0\u7f57\u5217\u5b57\u6bb5\u7c7b\u578b\u5916\uff0c\u5176\u4ed6\u7c7b\u578b\u5747\u4e0d\u652f\u6301</code></p>"},{"location":"reader/hbase20xreader/#_4","title":"\u9650\u5236","text":"<ol> <li>\u76ee\u524d\u4e0d\u652f\u6301\u52a8\u6001\u5217\u7684\u8bfb\u53d6\u3002\u8003\u8651\u7f51\u7edc\u4f20\u8f93\u6d41\u91cf\uff08\u652f\u6301\u52a8\u6001\u5217\uff0c\u9700\u8981\u5148\u5c06hbase\u6240\u6709\u5217\u7684\u6570\u636e\u8bfb\u53d6\u51fa\u6765\uff0c\u518d\u6309\u89c4\u5219\u8fdb\u884c\u8fc7\u6ee4\uff09\uff0c\u73b0\u652f\u6301\u7684\u4e24\u79cd\u8bfb\u53d6\u6a21\u5f0f\u4e2d\u9700\u8981\u7528\u6237\u660e\u786e\u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u5217\u3002</li> <li>\u5173\u4e8e\u540c\u6b65\u4f5c\u4e1a\u7684\u5207\u5206\uff1a\u76ee\u524d\u7684\u5207\u5206\u65b9\u5f0f\u662f\u6839\u636e\u7528\u6237hbase\u8868\u6570\u636e\u7684region\u5206\u5e03\u8fdb\u884c\u5207\u5206\u3002\u5373\uff1a\u5728\u7528\u6237\u586b\u5199\u7684 <code>[startrowkey\uff0cendrowkey]</code> \u8303\u56f4\u5185\uff0c\u4e00\u4e2aregion\u4f1a\u5207\u5206\u6210\u4e00\u4e2atask\uff0c\u5355\u4e2aregion\u4e0d\u8fdb\u884c\u5207\u5206\u3002</li> <li><code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u4e0d\u652f\u6301\u589e\u52a0\u5e38\u91cf\u5217</li> </ol>"},{"location":"reader/hbase20xsqlreader/","title":"HBase20 SQL Reader","text":"<p>HBase20 SQL Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Phoenix(HBase SQL) \u8bfb\u53d6\u6570\u636e\uff0c\u5bf9\u5e94\u7248\u672c\u4e3a HBase2.X \u548c Phoenix5.X\u3002</p>"},{"location":"reader/hbase20xsqlreader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ece Phoenix \u540c\u6b65\u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hbase20xsqlreader\",\n          \"parameter\": {\n            \"queryServerAddress\": \"http://127.0.0.1:8765\",\n            \"serialization\": \"PROTOBUF\",\n            \"table\": \"TEST\",\n            \"column\": [\"ID\", \"NAME\"],\n            \"splitKey\": \"ID\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/hbase20xsqlreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 queryServerAddress \u662f string \u65e0 Phoenix QueryServer \u5730\u5740, \u8be5\u63d2\u4ef6\u901a\u8fc7 PQS \u8fdb\u884c\u8fde\u63a5 serialization \u5426 string PROTOBUF QueryServer \u4f7f\u7528\u7684\u5e8f\u5217\u5316\u534f\u8bae table \u662f string \u65e0 \u6240\u8981\u8bfb\u53d6\u8868\u540d schema \u5426 string \u65e0 \u8868\u6240\u5728\u7684 schema column \u5426 list `` \u586b\u5199\u9700\u8981\u4ece phoenix \u8868\u4e2d\u8bfb\u53d6\u7684\u5217\u540d\u96c6\u5408\uff0c\u7a7a\u503c\u8868\u793a\u8bfb\u53d6\u6240\u6709\u5217 splitKey \u662f string \u65e0 \u6839\u636e\u6570\u636e\u7279\u5f81\u52a8\u6001\u6307\u5b9a\u5207\u5206\u70b9\uff0c\u5bf9\u8868\u6570\u636e\u6309\u7167\u6307\u5b9a\u7684\u5217\u7684\u6700\u5927\u3001\u6700\u5c0f\u503c\u8fdb\u884c\u5207\u5206,\u4ec5\u652f\u6301\u6574\u578b\u548c\u5b57\u7b26\u4e32\u7c7b\u578b splitPoints \u5426 string \u65e0 \u6309\u7167\u8868\u7684 split \u8fdb\u884c\u5207\u5206 where \u5426 string \u65e0 \u652f\u6301\u5bf9\u8868\u67e5\u8be2\u589e\u52a0\u8fc7\u6ee4\u6761\u4ef6\uff0c\u6bcf\u4e2a\u5207\u5206\u90fd\u4f1a\u643a\u5e26\u8be5\u8fc7\u6ee4\u6761\u4ef6 querySql \u5426 string \u65e0 \u652f\u6301\u6307\u5b9a\u591a\u4e2a\u67e5\u8be2\u8bed\u53e5\uff0c\u4f46\u67e5\u8be2\u5217\u7c7b\u578b\u548c\u6570\u76ee\u5fc5\u987b\u4fdd\u6301\u4e00\u81f4"},{"location":"reader/hbase20xsqlreader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u76ee\u524d\u652f\u6301\u5927\u90e8\u5206 Phoenix \u7c7b\u578b\uff0c\u4f46\u4e5f\u5b58\u5728\u90e8\u5206\u4e2a\u522b\u7c7b\u578b\u6ca1\u6709\u652f\u6301\u7684\u60c5\u51b5\uff0c\u8bf7\u6ce8\u610f\u68c0\u67e5\u4f60\u7684\u7c7b\u578b\u3002</p> Addax \u5185\u90e8\u7c7b\u578b Phoenix \u6570\u636e\u7c7b\u578b String CHAR, VARCHAR Bytes BINARY, VARBINARY Bool BOOLEAN Long INTEGER, TINYINT, SMALLINT, BIGINT Double FLOAT, DECIMAL, DOUBLE, Date DATE, TIME, TIMESTAMP"},{"location":"reader/hbase20xsqlreader/#_4","title":"\u7ea6\u675f\u9650\u5236","text":"<ul> <li>\u5207\u5206\u8868\u65f6\u5207\u5206\u5217\u4ec5\u652f\u6301\u5355\u4e2a\u5217\uff0c\u4e14\u8be5\u5217\u5fc5\u987b\u662f\u8868\u4e3b\u952e</li> <li>\u4e0d\u8bbe\u7f6e <code>splitPoint</code> \u9ed8\u8ba4\u4f7f\u7528\u81ea\u52a8\u5207\u5206\uff0c\u6b64\u65f6\u5207\u5206\u5217\u4ec5\u652f\u6301\u6574\u5f62\u548c\u5b57\u7b26\u578b</li> <li>\u8868\u540d\u548c <code>SCHEMA</code> \u540d\u53ca\u5217\u540d\u5927\u5c0f\u5199\u654f\u611f\uff0c\u8bf7\u4e0e Phoenix \u8868\u5b9e\u9645\u5927\u5c0f\u5199\u4fdd\u6301\u4e00\u81f4</li> <li>\u4ec5\u652f\u6301\u901a\u8fc7 Phoenix QueryServer \u8bfb\u53d6\u6570\u636e\uff0c\u56e0\u6b64\u60a8\u7684 Phoenix \u5fc5\u987b\u542f\u52a8 QueryServer \u670d\u52a1\u624d\u80fd\u4f7f\u7528\u672c\u63d2\u4ef6</li> </ul>"},{"location":"reader/hdfsreader/","title":"HDFS Reader","text":"<p>HDFS Reader \u63d0\u4f9b\u4e86\u8bfb\u53d6\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf Hadoop HDFS \u6570\u636e\u5b58\u50a8\u7684\u80fd\u529b\u3002</p> <p>\u76ee\u524d HdfsReader \u652f\u6301\u7684\u6587\u4ef6\u683c\u5f0f\u5982\u4e0b\uff1a</p> <ul> <li>textfile\uff08text\uff09</li> <li>orcfile\uff08orc\uff09</li> <li>rcfile\uff08rc\uff09</li> <li>sequence file\uff08seq\uff09</li> <li>Csv(csv)</li> <li>parquet</li> </ul>"},{"location":"reader/hdfsreader/#_1","title":"\u529f\u80fd\u4e0e\u9650\u5236","text":"<ol> <li>\u652f\u6301 textfile, orcfile, parquet, rcfile, sequence file \u548c csv \u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u4e14\u8981\u6c42\u6587\u4ef6\u5185\u5bb9\u5b58\u653e\u7684\u662f\u4e00\u5f20\u903b\u8f91\u610f\u4e49\u4e0a\u7684\u4e8c\u7ef4\u8868\u3002</li> <li>\u652f\u6301\u591a\u79cd\u7c7b\u578b\u6570\u636e\u8bfb\u53d6(\u4f7f\u7528 String \u8868\u793a)\uff0c\u652f\u6301\u5217\u88c1\u526a\uff0c\u652f\u6301\u5217\u5e38\u91cf</li> <li>\u652f\u6301\u9012\u5f52\u8bfb\u53d6\u3001\u652f\u6301\u6b63\u5219\u8868\u8fbe\u5f0f\uff08<code>*</code>\u548c <code>?</code>\uff09\u3002</li> <li>\u652f\u6301\u5e38\u89c1\u7684\u538b\u7f29\u7b97\u6cd5\uff0c\u5305\u62ec GZIP\uff0c SNAPPY\uff0c ZLIB \u7b49\u3002</li> <li>\u591a\u4e2a File \u53ef\u4ee5\u652f\u6301\u5e76\u53d1\u8bfb\u53d6\u3002</li> <li>\u652f\u6301 sequence file \u6570\u636e\u538b\u7f29\uff0c\u76ee\u524d\u652f\u6301 lzo \u538b\u7f29\u65b9\u5f0f\u3002</li> <li>csv \u7c7b\u578b\u652f\u6301\u538b\u7f29\u683c\u5f0f\u6709\uff1agzip\u3001bz2\u3001zip\u3001lzo\u3001lzo_deflate\u3001snappy\u3002</li> <li>\u76ee\u524d\u63d2\u4ef6\u4e2d Hive \u7248\u672c\u4e3a <code>3.1.1</code>\uff0cHadoop \u7248\u672c\u4e3a<code>3.1.1</code>, \u5728 Hadoop <code>2.7.x</code>, Hadoop <code>3.1.x</code> \u548c Hive <code>2.x</code>, hive <code>3.1.x</code> \u6d4b\u8bd5\u73af\u5883\u4e2d\u5199\u5165\u6b63\u5e38\uff1b\u5176\u5b83\u7248\u672c\u7406\u8bba\u4e0a\u90fd\u652f\u6301\uff0c\u4f46\u5728\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u524d\uff0c\u8bf7\u8fdb\u4e00\u6b65\u6d4b\u8bd5\uff1b</li> <li>\u652f\u6301<code>kerberos</code> \u8ba4\u8bc1</li> </ol>"},{"location":"reader/hdfsreader/#_2","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hdfsreader\",\n        \"parameter\": {\n          \"path\": \"/user/hive/warehouse/mytable01/*\",\n          \"defaultFS\": \"hdfs://xxx:port\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"hello\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            }\n          ],\n          \"fileType\": \"orc\",\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/hdfsreader/#_3","title":"\u914d\u7f6e\u9879\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e path \u662f string \u65e0 \u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u8def\u5f84 defaultFS \u662f string \u65e0 HDFS <code>NAMENODE</code> \u8282\u70b9\u5730\u5740\uff0c\u5982\u679c\u914d\u7f6e\u4e86 HA \u6a21\u5f0f\uff0c\u5219\u4e3a <code>defaultFS</code> \u7684\u503c fileType \u662f string \u65e0 \u6587\u4ef6\u7684\u7c7b\u578b column \u662f <code>list&lt;map&gt;</code> \u65e0 \u8bfb\u53d6\u5b57\u6bb5\u5217\u8868 fieldDelimiter \u5426 char <code>,</code> \u6307\u5b9a\u6587\u672c\u6587\u4ef6\u7684\u5b57\u6bb5\u5206\u9694\u7b26\uff0c\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0d\u9700\u8981\u6307\u5b9a\u8be5\u9879 encoding \u5426 string <code>utf-8</code> \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e\uff0c \u76ee\u524d\u4ec5\u652f\u6301 <code>utf-8</code> nullFormat \u5426 string \u65e0 \u53ef\u4ee5\u8868\u793a\u4e3a\u7a7a\u7684\u5b57\u7b26\uff0c\u5982\u679c\u7528\u6237\u914d\u7f6e: <code>\"\\\\N\"</code> \uff0c\u90a3\u4e48\u5982\u679c\u6e90\u5934\u6570\u636e\u662f <code>\"\\N\"</code> \uff0c\u89c6\u4f5c <code>null</code> \u5b57\u6bb5 haveKerberos \u5426 boolean \u65e0 \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u9700\u8981\u540c\u65f6\u914d\u7f6e\u4e0b\u9762\u4e24\u9879 kerberosKeytabFilePath \u5426 string \u65e0 Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u6587\u4ef6\u8def\u5f84, \u6bd4\u5982 <code>/your/path/addax.service.keytab</code> kerberosPrincipal \u5426 string \u65e0 Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u4e3b\u4f53, \u6bd4\u5982 <code>addax/node1@WGZHAO.COM</code> compress \u5426 string \u65e0 \u6307\u5b9a\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u538b\u7f29\u683c\u5f0f hadoopConfig \u5426 map \u65e0 \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Hadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982 HA \u7684\u914d\u7f6e hdfsSitePath \u5426 string \u65e0 <code>hdfs-site.xml</code> \u7684\u8def\u5f84\uff0c\u8be6\u7ec6\u89e3\u91ca\u89c1\u4e0b"},{"location":"reader/hdfsreader/#path","title":"path","text":"<p>\u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u5982\u679c\u8981\u8bfb\u53d6\u591a\u4e2a\u6587\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f <code>*</code>\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\uff1a</p> <ol> <li>\u5f53\u6307\u5b9a\u5355\u4e2a Hdfs \u6587\u4ef6\uff0cHdfsReader \u6682\u65f6\u53ea\u80fd\u4f7f\u7528\u5355\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u4e8c\u671f\u8003\u8651\u5728\u975e\u538b\u7f29\u6587\u4ef6\u60c5\u51b5\u4e0b\u9488\u5bf9\u5355\u4e2a File \u53ef\u4ee5\u8fdb\u884c\u591a\u7ebf\u7a0b\u5e76\u53d1\u8bfb\u53d6\u3002</li> <li>\u5f53\u6307\u5b9a\u591a\u4e2a Hdfs \u6587\u4ef6\uff0cHdfsReader \u652f\u6301\u4f7f\u7528\u591a\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u7ebf\u7a0b\u5e76\u53d1\u6570\u901a\u8fc7\u901a\u9053\u6570\u6307\u5b9a\u3002</li> <li>\u5f53\u6307\u5b9a\u901a\u914d\u7b26\uff0cHdfsReader \u5c1d\u8bd5\u904d\u5386\u51fa\u591a\u4e2a\u6587\u4ef6\u4fe1\u606f\u3002\u4f8b\u5982: \u6307\u5b9a <code>/*</code> \u4ee3\u8868\u8bfb\u53d6 <code>/</code> \u76ee\u5f55\u4e0b\u6240\u6709\u7684\u6587\u4ef6\uff0c\u6307\u5b9a <code>/bazhen/*</code> \u4ee3\u8868\u8bfb\u53d6 bazhen \u76ee\u5f55\u4e0b\u6e38\u6240\u6709\u7684\u6587\u4ef6\u3002HdfsReader \u76ee\u524d\u53ea\u652f\u6301 <code>*</code>\u548c <code>?</code> \u4f5c\u4e3a\u6587\u4ef6\u901a\u914d\u7b26\u3002</li> </ol> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cAddax \u4f1a\u5c06\u4e00\u4e2a\u4f5c\u4e1a\u4e0b\u540c\u6b65\u7684\u6240\u6709\u7684\u6587\u4ef6\u89c6\u4f5c\u540c\u4e00\u5f20\u6570\u636e\u8868\u3002\u7528\u6237\u5fc5\u987b\u81ea\u5df1\u4fdd\u8bc1\u6240\u6709\u7684 File \u80fd\u591f\u9002\u914d\u540c\u4e00\u5957 schema \u4fe1\u606f\u3002\u5e76\u4e14\u63d0\u4f9b\u7ed9 Addax \u6743\u9650\u53ef\u8bfb\u3002</p>"},{"location":"reader/hdfsreader/#filetype","title":"fileType","text":"<p>\u63cf\u8ff0\uff1a\u6587\u4ef6\u7684\u7c7b\u578b\uff0c\u76ee\u524d\u53ea\u652f\u6301\u7528\u6237\u914d\u7f6e\u4e3a</p> <ul> <li>text \u8868\u793a <code>textfile</code> \u6587\u4ef6\u683c\u5f0f</li> <li>orc \u8868\u793a <code>orcfile</code> \u6587\u4ef6\u683c\u5f0f</li> <li>rc \u8868\u793a <code>rcfile</code> \u6587\u4ef6\u683c\u5f0f</li> <li>seq \u8868\u793a <code>sequence file</code> \u6587\u4ef6\u683c\u5f0f</li> <li>csv \u8868\u793a\u666e\u901a hdfs \u6587\u4ef6\u683c\u5f0f\uff08\u903b\u8f91\u4e8c\u7ef4\u8868\uff09</li> <li>parquet \u8868\u793a <code>parquet</code> \u6587\u4ef6\u683c\u5f0f</li> </ul> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cHdfsReader \u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u6587\u4ef6\u662f <code>orcfile</code>\u3001<code>textfile</code> \u6216\u8005\u8fd8\u662f\u5176\u5b83\u7c7b\u578b\u7684\u6587\u4ef6\uff0c\u4f46\u8be5\u9879\u662f\u5fc5\u586b\u9879\uff0cHdfsReader \u5219\u4f1a\u53ea\u8bfb\u53d6\u7528\u6237\u914d\u7f6e\u7684\u7c7b\u578b\u7684\u6587\u4ef6\uff0c\u5ffd\u7565\u8def\u5f84\u4e0b\u5176\u4ed6\u683c\u5f0f\u7684\u6587\u4ef6</p> <p>\u53e6\u5916\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e <code>textfile</code> \u548c <code>orcfile</code> \u662f\u4e24\u79cd\u5b8c\u5168\u4e0d\u540c\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u6240\u4ee5 HdfsReader \u5bf9\u8fd9\u4e24\u79cd\u6587\u4ef6\u7684\u89e3\u6790\u65b9\u5f0f\u4e5f\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u79cd\u5dee\u5f02\u5bfc\u81f4 hive \u652f\u6301\u7684\u590d\u6742\u590d\u5408\u7c7b\u578b(\u6bd4\u5982 map,array,struct,union)\u5728\u8f6c\u6362\u4e3a\u652f\u6301\u7684 String \u7c7b\u578b\u65f6\uff0c\u8f6c\u6362\u7684\u7ed3\u679c\u683c\u5f0f\u7565\u6709\u5dee\u5f02\uff0c\u6bd4\u5982\u4ee5 map \u7c7b\u578b\u4e3a\u4f8b\uff1a</p> <ul> <li> <p><code>orcfile</code>: map \u7c7b\u578b\u8f6c\u6362\u6210 string \u7c7b\u578b\u540e\uff0c\u7ed3\u679c\u4e3a <code>{job=80, team=60, person=70}</code></p> </li> <li> <p><code>textfile</code>: map \u7c7b\u578b\u8f6c\u6362\u6210 string \u7c7b\u578b\u540e\uff0c\u7ed3\u679c\u4e3a <code>job:80,team:60,person:70</code></p> </li> </ul> <p>\u4ece\u4e0a\u9762\u7684\u8f6c\u6362\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0c\u6570\u636e\u672c\u8eab\u6ca1\u6709\u53d8\u5316\uff0c\u4f46\u662f\u8868\u793a\u7684\u683c\u5f0f\u7565\u6709\u5dee\u5f02\uff0c\u6240\u4ee5\u5982\u679c\u7528\u6237\u914d\u7f6e\u7684\u6587\u4ef6\u8def\u5f84\u4e2d\u8981\u540c\u6b65\u7684\u5b57\u6bb5\u5728 Hive \u4e2d\u662f\u590d\u5408\u7c7b\u578b\u7684\u8bdd\uff0c\u5efa\u8bae\u914d\u7f6e\u7edf\u4e00\u7684\u6587\u4ef6\u683c\u5f0f\u3002</p> <p>\u5982\u679c\u9700\u8981\u7edf\u4e00\u590d\u5408\u7c7b\u578b\u89e3\u6790\u51fa\u6765\u7684\u683c\u5f0f\uff0c\u6211\u4eec\u5efa\u8bae\u7528\u6237\u5728 hive \u5ba2\u6237\u7aef\u5c06 <code>textfile</code> \u683c\u5f0f\u7684\u8868\u5bfc\u6210 <code>orcfile</code> \u683c\u5f0f\u7684\u8868</p>"},{"location":"reader/hdfsreader/#column","title":"column","text":"<p>\u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0c<code>type</code> \u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c<code>index</code> \u6307\u5b9a\u5f53\u524d\u5217\u6765\u81ea\u4e8e\u6587\u672c\u7b2c\u51e0\u5217(\u4ee5 0 \u5f00\u59cb)\uff0c<code>value</code> \u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece\u6e90\u5934\u6587\u4ef6\u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636e <code>value</code> \u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7528\u6237\u53ef\u4ee5\u5168\u90e8\u6309\u7167 String \u7c7b\u578b\u8bfb\u53d6\u6570\u636e\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\"*\"]\n}\n</code></pre> <p>\u7528\u6237\u53ef\u4ee5\u6307\u5b9a Column \u5b57\u6bb5\u4fe1\u606f\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0,\n    \"description\": \"\u4ece\u672c\u5730\u6587\u4ef6\u6587\u672c\u7b2c\u4e00\u5217\u83b7\u53d6int\u5b57\u6bb5\"\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"addax\",\n    \"description\": \"HdfsReader\u5185\u90e8\u751f\u6210alibaba\u7684\u5b57\u7b26\u4e32\u5b57\u6bb5\u4f5c\u4e3a\u5f53\u524d\u5b57\u6bb5\"\n  }\n]\n</code></pre> <p>\u5bf9\u4e8e\u7528\u6237\u6307\u5b9a Column \u4fe1\u606f\uff0ctype \u5fc5\u987b\u586b\u5199\uff0cindex/value \u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</p>"},{"location":"reader/hdfsreader/#compress","title":"compress","text":"<p>\u5f53 fileType\uff08\u6587\u4ef6\u7c7b\u578b\uff09\u4e3a csv \u4e0b\u7684\u6587\u4ef6\u538b\u7f29\u65b9\u5f0f\uff0c\u76ee\u524d\u4ec5\u652f\u6301 gzip\u3001bz2\u3001zip\u3001lzo\u3001lzo_deflate\u3001hadoop-snappy\u3001framing-snappy \u538b\u7f29\uff1b \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0clzo \u5b58\u5728\u4e24\u79cd\u538b\u7f29\u683c\u5f0f\uff1alzo \u548c lzo_deflate\uff0c\u7528\u6237\u5728\u914d\u7f6e\u7684\u65f6\u5019\u9700\u8981\u7559\u5fc3\uff0c\u4e0d\u8981\u914d\u9519\u4e86\uff1b</p> <p>\u53e6\u5916\uff0c\u7531\u4e8e snappy \u76ee\u524d\u6ca1\u6709\u7edf\u4e00\u7684 stream format\uff0caddax \u76ee\u524d\u53ea\u652f\u6301\u6700\u4e3b\u6d41\u7684\u4e24\u79cd\uff1ahadoop-snappy\uff08hadoop \u4e0a\u7684 snappy stream format\uff09 \u548c framing-snappy\uff08google \u5efa\u8bae\u7684 snappy stream format\uff09;</p>"},{"location":"reader/hdfsreader/#hadoopconfig","title":"hadoopConfig","text":"<p><code>hadoopConfig</code> \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Hadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982 HA \u7684\u914d\u7f6e</p> <pre><code>{\n  \"hadoopConfig\": {\n    \"dfs.nameservices\": \"cluster\",\n    \"dfs.ha.namenodes.cluster\": \"nn1,nn2\",\n    \"dfs.namenode.rpc-address.cluster.nn1\": \"node1.example.com:8020\",\n    \"dfs.namenode.rpc-address.cluster.nn2\": \"node2.example.com:8020\",\n    \"dfs.client.failover.proxy.provider.cluster\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"\n  }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684 <code>cluster</code> \u8868\u793a HDFS \u914d\u7f6e\u6210 HA \u65f6\u7684\u540d\u5b57\uff0c\u4e5f\u662f <code>defaultFS</code> \u914d\u7f6e\u9879\u4e2d\u7684\u540d\u5b57 \u5982\u679c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u540d\u5b57\u4e0d\u662f <code>cluster</code> \uff0c\u5219\u4e0a\u8ff0\u914d\u7f6e\u4e2d\u6240\u6709\u5199\u6709 <code>cluster</code> \u90fd\u9700\u8981\u66ff\u6362</p>"},{"location":"reader/hdfsreader/#csvreaderconfig","title":"csvReaderConfig","text":"<p>\u8bfb\u53d6 CSV \u7c7b\u578b\u6587\u4ef6\u53c2\u6570\u914d\u7f6e\uff0cMap \u7c7b\u578b\u3002\u8bfb\u53d6 CSV \u7c7b\u578b\u6587\u4ef6\u4f7f\u7528\u7684 CsvReader \u8fdb\u884c\u8bfb\u53d6\uff0c\u4f1a\u6709\u5f88\u591a\u914d\u7f6e\uff0c\u4e0d\u914d\u7f6e\u5219\u4f7f\u7528\u9ed8\u8ba4\u503c\u3002</p> <p>\u5e38\u89c1\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"csvReaderConfig\": {\n    \"safetySwitch\": false,\n    \"skipEmptyRecords\": false,\n    \"useTextQualifier\": false\n  }\n}\n</code></pre> <p>\u6240\u6709\u914d\u7f6e\u9879\u53ca\u9ed8\u8ba4\u503c,\u914d\u7f6e\u65f6 csvReaderConfig \u7684 map \u4e2d\u8bf7 \u4e25\u683c\u6309\u7167\u4ee5\u4e0b\u5b57\u6bb5\u540d\u5b57\u8fdb\u884c\u914d\u7f6e\uff1a</p> <pre><code>boolean caseSensitive = true;\nchar textQualifier = 34;\nboolean trimWhitespace = true;\nboolean useTextQualifier = true;//\u662f\u5426\u4f7f\u7528csv\u8f6c\u4e49\u5b57\u7b26\nchar delimiter = 44;//\u5206\u9694\u7b26\nchar recordDelimiter = 0;\nchar comment = 35;\nboolean useComments = false;\nint escapeMode = 1;\nboolean safetySwitch = true;//\u5355\u5217\u957f\u5ea6\u662f\u5426\u9650\u5236100000\u5b57\u7b26\nboolean skipEmptyRecords = true;//\u662f\u5426\u8df3\u8fc7\u7a7a\u884c\nboolean captureRawRecord = true;\n</code></pre>"},{"location":"reader/hdfsreader/#hdfssitepath","title":"hdfsSitePath","text":"<p>\u8fd9\u662f <code>4.2.4</code> \u5f15\u5165\u7684\u65b0\u914d\u7f6e\u60f3\uff0c\u7528\u4e8e\u6307\u5b9a <code>hdfs-site.xml</code> \u6587\u4ef6\u7684\u8def\u5f84\uff0c\u6bd4\u5982\u5bf9 HDP/CDH \u800c\u8a00\uff0c\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"hdfsSitePath\": \"/etc/hadoop/conf/hdfs-site.xml\"\n}\n</code></pre> <p>\u5982\u679c\u914d\u7f6e\u4e86 <code>hdfsSitePath</code> , \u5219\u63d2\u4ef6\u4f1a\u4ece\u8be5\u6587\u4ef6\u4e2d\u83b7\u5f97\u8bbf\u95ee HDFS \u6587\u4ef6\u7cfb\u7edf\u5fc5\u8981\u7684\u914d\u7f6e\uff0c\u4ece\u800c\u5728\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u4e0d\u5728\u9700\u8981\u914d\u7f6e <code>hadoopConfig</code>\uff0c\u51cf\u5c11\u914d\u7f6e\u91cf\u3002</p> <p>\u5bf9\u4e8e\u628a Addax \u90e8\u7f72\u5728 Hadoop \u96c6\u7fa4\u4e0a\u7684\u573a\u666f\uff0c\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002</p>"},{"location":"reader/hdfsreader/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b Hive \u8868 \u6570\u636e\u7c7b\u578b Long TINYINT, SMALLINT, INT, BIGINT Double FLOAT, DOUBLE String String, CHAR, VARCHAR, STRUCT, MAP, ARRAY, UNION, BINARY Boolean BOOLEAN Date Date, TIMESTAMP Bytes BINARY <p>\u5176\u4e2d\uff1a</p> <ul> <li>Long \u662f\u6307 Hdfs \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528\u6574\u5f62\u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>123456789</code></li> <li>Double \u662f\u6307 Hdfs \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Double \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>3.1415</code></li> <li>Boolean \u662f\u6307 Hdfs \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Boolean \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>true</code>\u3001<code>false</code>\u3002\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u3002</li> <li>Date \u662f\u6307 Hdfs \u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528 Date \u7684\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>2014-12-31</code></li> <li>Bytes \u662f\u6307 HDFS \u6587\u4ef6\u4e2d\u4f7f\u7528\u4e8c\u8fdb\u5236\u5b58\u50a8\u7684\u5185\u5bb9\uff0c\u6bd4\u5982\u4e00\u5f20\u56fe\u7247\u7684\u6570\u636e</li> </ul> <p>\u7279\u522b\u63d0\u9192\uff1a</p> <ul> <li>Hive \u652f\u6301\u7684\u6570\u636e\u7c7b\u578b TIMESTAMP \u53ef\u4ee5\u7cbe\u786e\u5230\u7eb3\u79d2\u7ea7\u522b\uff0c\u6240\u4ee5 <code>textfile</code>\u3001<code>orcfile</code> \u4e2d <code>TIMESTAMP</code> \u5b58\u653e\u7684\u6570\u636e\u7c7b\u4f3c\u4e8e <code>2015-08-21 22:40:47.397898389</code>\uff0c   \u5982\u679c\u8f6c\u6362\u7684\u7c7b\u578b\u914d\u7f6e\u4e3a Addax \u7684 Date\uff0c\u8f6c\u6362\u4e4b\u540e\u4f1a\u5bfc\u81f4\u7eb3\u79d2\u90e8\u5206\u4e22\u5931\uff0c\u6240\u4ee5\u5982\u679c\u9700\u8981\u4fdd\u7559\u7eb3\u79d2\u90e8\u5206\u7684\u6570\u636e\uff0c\u8bf7\u914d\u7f6e\u8f6c\u6362\u7c7b\u578b\u4e3a <code>String</code> \u7c7b\u578b\u3002</li> </ul>"},{"location":"reader/hdfsreader/#faq","title":"FAQ","text":"<p>Q: \u5982\u679c\u62a5 java.io.IOException: Maximum column length of 100,000 exceeded in column...\u5f02\u5e38\u4fe1\u606f\uff0c\u8bf4\u660e\u6570\u636e\u6e90 column \u5b57\u6bb5\u957f\u5ea6\u8d85\u8fc7\u4e86 100000 \u5b57\u7b26\u3002</p> <p>A: \u9700\u8981\u5728 json \u7684 reader \u91cc\u589e\u52a0\u5982\u4e0b\u914d\u7f6e</p> <pre><code>{\n  \"csvReaderConfig\": {\n    \"safetySwitch\": false,\n    \"skipEmptyRecords\": false,\n    \"useTextQualifier\": false\n  }\n}\n</code></pre> <p><code>safetySwitch = false</code> \u8868\u793a\u5355\u5217\u957f\u5ea6\u4e0d\u9650\u5236 100000 \u5b57\u7b26</p>"},{"location":"reader/hivereader/","title":"Hive Reader","text":"<p>Hive Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Apache Hive \u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b</p> <p>\u65b0\u589e\u8be5\u63d2\u4ef6\u7684\u4e3b\u8981\u76ee\u7684\u662f\u89e3\u51b3\u4f7f\u7528 RDBMS Reader \u63d2\u4ef6\u8bfb\u53d6 Hive \u6570\u636e\u5e93\u65f6\u4e0d\u80fd\u89e3\u51b3 Kerberos \u8ba4\u8bc1\u7684\u95ee\u9898\uff0c \u5982\u679c\u4f60\u7684 Hive \u6570\u636e\u5e93\u6ca1\u6709\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u90a3\u4e48\u76f4\u63a5\u4f7f\u7528 RDBMS Reader \u4e5f\u53ef\u4ee5\u3002 \u5982\u679c\u542f\u7528\u4e86 Kerberos \u8ba4\u8bc1\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u8be5\u63d2\u4ef6\u3002</p>"},{"location":"reader/hivereader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u5728 Hive \u7684 test \u5e93\u4e0a\u521b\u5efa\u5982\u4e0b\u8868\uff0c\u5e76\u63d2\u5165\u4e00\u6761\u8bb0\u5f55</p> <pre><code>create table default.hive_reader\n(\n    col1 int,\n    col2 string,\n    col3 timestamp\n)\nstored as orc;\n\n\ninsert into hive_reader values(1, 'hello', current_timestamp()), (2, 'world', current_timestamp());\n</code></pre> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/hive2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hivereader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:hive2://localhost:10000/default;principal=hive/_HOST@EXAMPLE.COM\",\n            \"table\": [\n              \"hive_reader\"\n            ]\n          },\n          \"where\": \"logdate='20211013'\",\n          \"haveKerberos\": true,\n          \"kerberosKeytabFilePath\": \"/etc/security/keytabs/hive.headless.keytab\",\n          \"kerberosPrincipal\": \"hive@EXAMPLE.COM\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/hive2stream.json</code></p>"},{"location":"reader/hivereader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/hive2stream.json\n</code></pre>"},{"location":"reader/hivereader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 jdbcUrl \u662f list \u65e0 \u5bf9\u7aef\u6570\u636e\u5e93\u7684 JDBC \u8fde\u63a5\u4fe1\u606f driver \u5426 string \u65e0 \u81ea\u5b9a\u4e49\u9a71\u52a8\u7c7b\u540d\uff0c\u89e3\u51b3\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801\uff0c\u82e5\u65e0\u5bc6\u7801\uff0c\u53ef\u4e0d\u6307\u5b9a table \u662f list \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d,\u4f7f\u7528 JSON \u6570\u636e\u683c\u5f0f column \u662f <code>list&lt;map&gt;</code> \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1 rdbmreader splitPk \u5426 string \u65e0 \u4f7f\u7528 splitPk \u4ee3\u8868\u7684\u5b57\u6bb5\u8fdb\u884c\u6570\u636e\u5206\u7247\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1 rdbmreader where \u5426 string \u65e0 \u9488\u5bf9\u8868\u7684\u7b5b\u9009\u6761\u4ef6 querySql \u5426 list \u65e0 \u4f7f\u7528 SQL \u6765\u83b7\u53d6\u6570\u636e\uff0c\u5f53\u914d\u7f6e\u4e86\u8fd9\u4e00\u9879\u4e4b\u540e\uff0c <code>table</code>\uff0c<code>column</code> \u914d\u7f6e\u9879\u65e0\u6548 haveKerberos \u5426 string \u65e0 \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u9700\u8981\u540c\u65f6\u914d\u7f6e\u4e0b\u9762\u4e24\u9879 kerberosKeytabFilePath \u5426 string \u65e0 Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u6587\u4ef6\u8def\u5f84, \u6bd4\u5982 <code>/your/path/addax.service.keytab</code> kerberosPrincipal \u5426 string \u65e0 Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u4e3b\u4f53, \u6bd4\u5982 <code>addax/node1@EXAMPLE.COM</code>"},{"location":"reader/hivereader/#jdbcurl","title":"jdbcUrl","text":"<p>\u8fde\u63a5 Hive \u7684 JDBC URL \u6709\u591a\u79cd\u5199\u6cd5\uff0c\u4e00\u79cd\u662f\u76f4\u63a5\u6307\u5b9a HiveServer/HiveServer2 \u670d\u52a1\u7684\u4e3b\u673a\u540d\u548c\u7aef\u53e3\u5373\u53ef\uff0c\u6bd4\u5982\uff1a <code>jdbc:hive2://node1:10000/default</code></p> <p>\u5982\u679c\u4f60\u6709\u591a\u4e2a HiveServer/HiveServer2 \u670d\u52a1\uff0c\u5e76\u91c7\u53d6\u7528\u4e86\u670d\u52a1\u53d1\u73b0\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a zookeeper \u7684\u65b9\u5f0f\u6765\u83b7\u5f97\u6545\u969c\u8f6c\u79fb\u529f\u80fd\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>jdbc:hive2://node1:2181,node2:2181,node3:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\n</code></pre> <p>\u5982\u679c\u4f60\u7684 Hive \u542f\u7528\u4e86 Kerberos \u8ba4\u8bc1\uff0c\u8fd8\u9700\u8981\u5728 URL \u540e\u6307\u5b9a <code>principal</code> \u53c2\u6570\uff0c\u4e00\u822c\u4e3a <code>principal=hive/_HOST@EXAMPLE.COM</code>\uff0c\u5176\u4e2d <code>EXAMPLE.COM</code> \u4e3a <code>realm</code> \u503c\u3002</p>"},{"location":"reader/hivereader/#driver","title":"driver","text":"<p>\u5f53\u524d Addax \u91c7\u7528\u7684 Hive JDBC \u9a71\u52a8\u4e3a 3.1.0 \u4ee5\u4e0a\u7248\u672c\uff0c\u9a71\u52a8\u7c7b\u540d\u4f7f\u7528\u7684 <code>org.apache.hive.jdbc.HiveDriver</code>\uff0c \u5982\u679c\u5f53\u524d\u7684 Hive JDBC \u9a71\u52a8\u4e0d\u517c\u5bb9 Hive \u6570\u636e\u5e93\uff0c \u5219\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u66ff\u6362\u9a71\u52a8\u3002</p> <p>\u66ff\u6362\u63d2\u4ef6\u5185\u7f6e\u7684\u9a71\u52a8</p> <p><code>rm -f plugin/reader/hivereader/libs/hive-jdbc-*.jar</code></p> <p>\u62f7\u8d1d\u517c\u5bb9\u9a71\u52a8\u5230\u63d2\u4ef6\u76ee\u5f55</p> <p><code>cp hive-jdbc-&lt;version&gt;.jar plugin/reader/hivereader/libs/</code></p> <p>\u6307\u5b9a\u9a71\u52a8\u7c7b\u540d\u79f0</p> <p>\u5728\u4f60\u7684 json \u6587\u4ef6\u7c7b\uff0c\u914d\u7f6e <code>\"driver\": \"&lt;your jdbc class name&gt;\"</code></p>"},{"location":"reader/hivereader/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u76ee\u524d HiveReader \u652f\u6301\u5927\u90e8\u5206 Hive \u7c7b\u578b\uff0c\u4f46\u4e5f\u5b58\u5728\u90e8\u5206\u4e2a\u522b\u7c7b\u578b\u6ca1\u6709\u652f\u6301\u7684\u60c5\u51b5\uff0c\u8bf7\u6ce8\u610f\u68c0\u67e5\u4f60\u7684\u7c7b\u578b\u3002</p> <p>\u4e0b\u9762\u5217\u51fa HiveReader \u9488\u5bf9 Hive \u7c7b\u578b\u8f6c\u6362\u5217\u8868:</p> Addax \u5185\u90e8\u7c7b\u578b Hive \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, string Date date, timestamp Boolean boolean Bytes binary"},{"location":"reader/hivereader/#_5","title":"\u76f8\u5173\u9650\u5236","text":"<p>\u7ecf\u8fc7\u6d4b\u8bd5\uff0cHiveReader \u76ee\u524d\u652f\u6301\u7684 Hive \u7248\u672c\u4e3a 2.1.1 \u53ca\u4ee5\u4e0a\u7248\u672c\u5e76\u652f\u6301 4.0.0 \u7248\u672c\u3002</p>"},{"location":"reader/httpreader/","title":"HTTP Reader","text":"<p>HTTP Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u8bfb\u53d6 Restful API \u6570\u636e\u7684\u80fd\u529b</p>"},{"location":"reader/httpreader/#_1","title":"\u793a\u4f8b","text":""},{"location":"reader/httpreader/#_2","title":"\u793a\u4f8b\u63a5\u53e3\u4e0e\u6570\u636e","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u6f14\u793a\u4e86\u5982\u4f55\u4ece\u4e00\u4e2a\u6307\u5b9a\u7684 API \u4e2d\u83b7\u53d6\u6570\u636e\uff0c\u5047\u5b9a\u8bbf\u95ee\u7684\u63a5\u53e3\u4e3a\uff1a</p> <p>http://127.0.0.1:9090/mock/17/LDJSC/ASSET</p> <p>\u63a5\u53e3\u63a5\u53d7 GET \u8bf7\u6c42\uff0c\u8bf7\u6c42\u7684\u53c2\u6570\u6709</p> \u53c2\u6570\u540d\u79f0 \u53c2\u6570\u503c\u793a\u4f8b CURR_DATE 2021-01-17 DEPT 9400 USERNAME andi <p>\u4ee5\u4e0b\u662f\u8bbf\u95ee\u7684\u6570\u636e\u6837\u4f8b\uff0c\uff08\u5b9e\u9645\u8fd4\u56de\u6570\u636e\u7565\u6709\u4e0d\u540c\uff09</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": \"9700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1581.03,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 36.75,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.009448781026677719,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015153586011995693,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": 0.0652347643813081,\n      \"TMMARKET_VALUE_SHARECOM\": 0.024853621341525287,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.005242133578517903,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1645.1193961136973,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.16690149257388515,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.017886267801303465,\n      \"POTENTIAL_LOST_ASSETS\": 56.76,\n      \"TOTAL_LIABILITIES\": 57.81,\n      \"TOTAL_ASSETS\": 1306.33,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 4.79,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.006797058194980485,\n      \"NEW_ASSETS_DAY\": 14.92,\n      \"NEW_ASSETS_MON\": 90.29,\n      \"NEW_ASSETS_YEAR\": 297.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.04015576541561927,\n      \"NEW_FUNDS_DAY\": 18.16,\n      \"INFLOW_FUNDS_DAY\": 2.12,\n      \"OUTFLOW_FUNDS_DAY\": 9.73,\n      \"OVERALL_POSITION\": 0.810298404938773,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.03521615634095476,\n      \"NEW_CUST_FUNDS_MON\": 69.44,\n      \"INFLOW_FUNDS_MONTH\": 62.26,\n      \"OUTFLOW_FUNDS_MONTH\": 32.59\n    },\n    {\n      \"CURR_DATE\": \"2019-08-30\",\n      \"DEPT\": \"8700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1596.74,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 41.86,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": 0.03470208565515685,\n      \"TMMARKET_VALUE_GROWTH_MON\": 0.07818120801111743,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.05440250244736409,\n      \"TMMARKET_VALUE_SHARECOM\": 0.09997733019626448,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.019726478499825697,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1007.9314679742108,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.15123738798885086,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.04694052069678048,\n      \"POTENTIAL_LOST_ASSETS\": 52.48,\n      \"TOTAL_LIABILITIES\": 55.28,\n      \"TOTAL_ASSETS\": 1366.72,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 10.12,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.009708491982487952,\n      \"NEW_ASSETS_DAY\": 12.42,\n      \"NEW_ASSETS_MON\": 41.14,\n      \"NEW_ASSETS_YEAR\": 279.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.025878627161898062,\n      \"NEW_FUNDS_DAY\": 3.65,\n      \"INFLOW_FUNDS_DAY\": 14.15,\n      \"OUTFLOW_FUNDS_DAY\": 17.08,\n      \"OVERALL_POSITION\": 0.9098432997243932,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02111922282868306,\n      \"NEW_CUST_FUNDS_MON\": 57.21,\n      \"INFLOW_FUNDS_MONTH\": 61.16,\n      \"OUTFLOW_FUNDS_MONTH\": 15.83\n    },\n    {\n      \"CURR_DATE\": \"2019-06-30\",\n      \"DEPT\": \"6501\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1506.72,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": -13.23,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.0024973354204176554,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015530793150701896,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.08556724628979398,\n      \"TMMARKET_VALUE_SHARECOM\": 0.15000077963967678,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.049629446804825755,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1250.1040863177336,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19098445630488178,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": -0.007881179708853471,\n      \"POTENTIAL_LOST_ASSETS\": 50.53,\n      \"TOTAL_LIABILITIES\": 56.62,\n      \"TOTAL_ASSETS\": 1499.53,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 29.56,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.02599813232345556,\n      \"NEW_ASSETS_DAY\": 28.81,\n      \"NEW_ASSETS_MON\": 123.24,\n      \"NEW_ASSETS_YEAR\": 263.63,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.0073986669331394875,\n      \"NEW_FUNDS_DAY\": 18.52,\n      \"INFLOW_FUNDS_DAY\": 3.26,\n      \"OUTFLOW_FUNDS_DAY\": 6.92,\n      \"OVERALL_POSITION\": 0.8713692113306709,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02977644553289545,\n      \"NEW_CUST_FUNDS_MON\": 85.14,\n      \"INFLOW_FUNDS_MONTH\": 23.35,\n      \"OUTFLOW_FUNDS_MONTH\": 92.95\n    },\n    {\n      \"CURR_DATE\": \"2019-12-07\",\n      \"DEPT\": \"8705\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1575.85,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 8.94,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.04384846980627058,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.022962456288549656,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.005047009316021089,\n      \"TMMARKET_VALUE_SHARECOM\": 0.07819484815809447,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.008534369960890256,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1340.0339240689955,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19019952857677042,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.01272353909992914,\n      \"POTENTIAL_LOST_ASSETS\": 54.63,\n      \"TOTAL_LIABILITIES\": 53.17,\n      \"TOTAL_ASSETS\": 1315.08,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 49.31,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.0016538407028265922,\n      \"NEW_ASSETS_DAY\": 29.17,\n      \"NEW_ASSETS_MON\": 44.75,\n      \"NEW_ASSETS_YEAR\": 172.87,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.045388692595736746,\n      \"NEW_FUNDS_DAY\": 18.46,\n      \"INFLOW_FUNDS_DAY\": 12.93,\n      \"OUTFLOW_FUNDS_DAY\": 10.38,\n      \"OVERALL_POSITION\": 0.8083127036694828,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.02847453515632541,\n      \"NEW_CUST_FUNDS_MON\": 49.74,\n      \"INFLOW_FUNDS_MONTH\": 81.93,\n      \"OUTFLOW_FUNDS_MONTH\": 18.17\n    }\n  ]\n}\n</code></pre> <p>\u6211\u4eec\u9700\u8981\u628a <code>result</code> \u7ed3\u679c\u4e2d\u7684\u90e8\u5206 key \u503c\u6570\u636e\u83b7\u53d6</p>"},{"location":"reader/httpreader/#_3","title":"\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u5b9e\u73b0\u4ece\u63a5\u53e3\u83b7\u53d6\u6570\u636e\u5e76\u6253\u5370\u5230\u7ec8\u7aef</p> job/httpreader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"httpreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"url\": \"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\",\n              \"proxy\": {\n                \"host\": \"http://127.0.0.1:3128\",\n                \"auth\": \"user:pass\"\n              }\n            }\n          ],\n          \"reqParams\": {\n            \"CURR_DATE\": \"2021-01-18\",\n            \"DEPT\": \"9700\"\n          },\n          \"resultKey\": \"result\",\n          \"method\": \"GET\",\n          \"column\": [\n            \"CURR_DATE\",\n            \"DEPT\",\n            \"TOTAL_MANAGED_MARKET_VALUE\",\n            \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n          ],\n          \"username\": \"user\",\n          \"password\": \"passw0rd\",\n          \"headers\": {\n            \"X-Powered-by\": \"Addax\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u5185\u5bb9\u4fdd\u5b58\u4e3a <code>job/httpreader2stream.json</code> \u6587\u4ef6\u3002</p>"},{"location":"reader/httpreader/#_4","title":"\u6267\u884c","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fdb\u884c\u91c7\u96c6</p> <pre><code>bin/addax.sh job/httpreader2stream.json\n</code></pre> <p>\u4e0a\u8ff0\u547d\u4ee4\u7684\u8f93\u51fa\u7ed3\u679c\u5927\u81f4\u5982\u4e0b\uff1a</p> <pre><code>2021-01-20 09:07:41.864 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-01-20 09:07:41.877 [main] INFO  Engine - the machine info  =&gt;\n\n    osInfo:     Mac OS X x86_64 10.15.1\n    jvmInfo:    AdoptOpenJDK 14 14.0.2+12\n    cpu num:    8\n\n    totalPhysicalMemory:    -0.00G\n    freePhysicalMemory: -0.00G\n    maxFileDescriptorCount: -1\n    currentOpenFileDescriptorCount: -1\n\n    GC Names    [G1 Young Generation, G1 Old Generation]\n\n    MEMORY_NAME                    | allocation_size                | init_size\n    CodeHeap 'profiled nmethods'   | 117.21MB                       | 2.44MB\n    G1 Old Gen                     | 2,048.00MB                     | 39.00MB\n    G1 Survivor Space              | -0.00MB                        | 0.00MB\n    CodeHeap 'non-profiled nmethods' | 117.21MB                       | 2.44MB\n    Compressed Class Space         | 1,024.00MB                     | 0.00MB\n    Metaspace                      | -0.00MB                        | 0.00MB\n    G1 Eden Space                  | -0.00MB                        | 25.00MB\n    CodeHeap 'non-nmethods'        | 5.57MB                         | 2.44MB\n\n\n2021-01-20 09:07:41.903 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"reqParams\":{\n                        \"CURR_DATE\":\"2021-01-18\",\n                        \"DEPT\":\"9700\"\n                    },\n                    \"method\":\"GET\",\n                    \"column\":[\n                        \"CURR_DATE\",\n                        \"DEPT\",\n                        \"TOTAL_MANAGED_MARKET_VALUE\",\n                        \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n                    ],\n                    \"resultKey\":\"result\",\n                    \"connection\":[\n                        {\n                            \"url\":\"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\"\n                        }\n                    ]\n                },\n                \"name\":\"httpreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":\"true\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-01-20 09:07:41.926 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-01-20 09:07:41.927 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-01-20 09:07:41.928 [main] INFO  JobContainer - Set jobId = 0\n2021-01-20 09:07:42.002 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started\n\n2019-08-30  9700    1539.85 -14.78\n2019-10-01  9700    1531.71 47.66\n2020-12-03  9700    1574.38 7.34\n2020-11-31  9700    1528.13 41.62\n2019-03-01  9700    1554.28 -9.29\n\n2021-01-20 09:07:45.006 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-20 09:07:41\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-20 09:07:44\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               42B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              1rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   5\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/httpreader/#_5","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e url \u662f string \u65e0 \u8981\u8bbf\u95ee\u7684 HTTP \u5730\u5740 reqParams \u5426 map \u65e0 \u63a5\u53e3\u8bf7\u6c42\u53c2\u6570 resultKey \u5426 string \u65e0 \u8981\u83b7\u53d6\u7ed3\u679c\u7684\u90a3\u4e2a key \u503c\uff0c\u5982\u679c\u662f\u83b7\u53d6\u6574\u4e2a\u8fd4\u56de\u503c\uff0c\u5219\u53ef\u4ee5\u4e0d\u7528\u586b\u5199 method \u5426 string get \u8bf7\u6c42\u6a21\u5f0f\uff0c\u4ec5\u652f\u6301 GET\uff0cPOST \u4e24\u79cd\uff0c\u4e0d\u533a\u5206\u5927\u5c0f\u5199 column \u662f list \u65e0 \u8981\u83b7\u53d6\u7684 key\uff0c\u914d\u7f6e\u4e3a <code>\"*\"</code> \u8868\u793a\u83b7\u53d6\u6240\u6709 key \u503c username \u5426 string \u65e0 \u63a5\u53e3\u8bf7\u6c42\u9700\u8981\u7684\u8ba4\u8bc1\u5e10\u53f7(\u5982\u6709) password \u5426 string \u65e0 \u63a5\u53e3\u8bf7\u6c42\u9700\u8981\u7684\u5bc6\u7801(\u5982\u6709) proxy \u5426 map \u65e0 \u4ee3\u7406\u5730\u5740,\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 headers \u5426 map \u65e0 \u5b9a\u5236\u7684\u8bf7\u6c42\u5934\u4fe1\u606f isPage \u5426 boolean \u65e0 \u63a5\u53e3\u662f\u5426\u5206\u652f\u5206\u9875 pageParams \u5426 map \u65e0 \u5206\u9875\u53c2\u6570"},{"location":"reader/httpreader/#reqparams","title":"reqParams","text":"<p>reqParams \u662f\u8bf7\u6c42\u53c2\u6570\uff0c \u5982\u679c\u8bf7\u6c42\u662f <code>GET</code> \u65b9\u5f0f\uff0c\u5219\u4f1a\u4ee5 <code>k=v</code> \u7684\u65b9\u5f0f\u62fc\u63a5\u5728 <code>url</code> \u7684\u540e\u9762\u3002 \u5982\u679c\u8bf7\u6c42\u7684\u662f <code>POST</code> \u6a21\u5f0f\uff0c\u5219\u4f1a\u628a <code>reqParams</code> \u5f53\u505a JSON \u5185\u5bb9\u4f5c\u4e3a\u8bf7\u6c42\u4f53\u53d1\u9001\u3002 \u7279\u522b\u7684\uff0c\u5982\u679c\u5728 <code>POST</code> \u6a21\u5f0f\u4e0b\uff0c\u5982\u679c\u4f60\u53d1\u9001\u7684\u8bf7\u6c42\u63d0\u5e76\u4e0d\u662f\u4e00\u4e2a <code>k-v</code> \u7ed3\u6784\uff0c\u5219\u53ef\u4ee5\u628a <code>key</code> \u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"reqParams\": {\n    \"\": [123,3456]\n  }\n}\n</code></pre> <p>\u7a0b\u5e8f\u4f1a\u9488\u5bf9\u8fd9\u79cd\u8fdb\u884c\u8fdb\u884c\u7279\u522b\u5904\u7406\u3002</p>"},{"location":"reader/httpreader/#proxy","title":"proxy","text":"<p>\u5982\u679c\u8bbf\u95ee\u7684\u63a5\u53e3\u9700\u8981\u901a\u8fc7\u4ee3\u7406\uff0c\u5219\u53ef\u4ee5\u914d\u7f6e <code>proxy</code> \u914d\u7f6e\u9879\uff0c\u8be5\u914d\u7f6e\u9879\u662f\u4e00\u4e2a json \u5b57\u5178\uff0c\u5305\u542b\u4e00\u4e2a\u5fc5\u9009\u7684 <code>host</code> \u5b57\u6bb5\u548c\u4e00\u4e2a\u53ef\u9009\u7684 <code>auth</code> \u5b57\u6bb5\u3002</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"http://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p>\u5982\u679c\u662f <code>sock</code> \u4ee3\u7406 (V4,v5)\uff0c\u5219\u53ef\u4ee5\u5199</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"socks://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p><code>host</code> \u662f\u4ee3\u7406\u5730\u5740\uff0c\u5305\u542b\u4ee3\u7406\u7c7b\u578b\uff0c\u76ee\u524d\u4ec5\u652f\u6301 <code>http</code> \u4ee3\u7406\u548c <code>socks</code>(V4, V5 \u5747\u53ef) \u4ee3\u7406\u3002 \u5982\u679c\u4ee3\u7406\u9700\u8981\u8ba4\u8bc1\uff0c\u5219\u53ef\u4ee5\u914d\u7f6e <code>auth</code> , \u5b83\u7531\u7528\u6237\u540d\u548c\u5bc6\u7801\u7ec4\u6210\uff0c\u4e24\u8005\u4e4b\u95f4\u7528\u5192\u53f7(<code>:</code>) \u9694\u5f00\u3002</p>"},{"location":"reader/httpreader/#column","title":"column","text":"<p><code>column</code> \u9664\u4e86\u76f4\u63a5\u6307\u5b9a key \u4e4b\u5916\uff0c\u8fd8\u5141\u8bb8\u7528 JSON Xpath \u98ce\u683c\u6765\u6307\u5b9a\u9700\u8981\u83b7\u53d6\u7684 key \u503c\uff0c\u5047\u5b9a\u4f60\u8981\u8bfb\u53d6\u7684 JSON \u6587\u4ef6\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": {\n        \"ID\": \"9700\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 1\n        },\n        {\n          \"COL2\": 2\n        }\n      ]\n    },\n    {\n      \"CURR_DATE\": \"2021-11-09\",\n      \"DEPT\": {\n        \"ID\": \"6500\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 3\n        },\n        {\n          \"COL2\": 4\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>\u6211\u4eec\u5e0c\u671b\u628a <code>CURR_DATE</code>, <code>ID</code>, <code>COL1</code>, <code>COL2</code> \u5f53\u4f5c\u56db\u4e2a\u5b57\u6bb5\u8bfb\u53d6\uff0c\u90a3\u4e48\u4f60\u7684 <code>column</code> \u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"column\": [\n    \"CURR_DATE\",\n    \"DEPT.ID\",\n    \"KK[0].COL1\",\n    \"KK[1].COL2\"\n  ]\n}\n</code></pre> <p>\u5176\u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff1a</p> <pre><code>...\n2021-10-30 14:01:50.273 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n2019-12-09  9700    1   2\n2021-11-09  6500    3   4\n\n2021-10-30 14:01:53.283 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Reader.Job [httpreader] do post work.\n2021-10-30 14:01:53.286 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-10-30 14:01:53.289 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-10-30 14:01:50\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-10-30 14:01:53\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               10B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              0rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   2\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre> <p>\u6ce8\u610f\uff1a \u5982\u679c\u4f60\u6307\u5b9a\u4e86\u4e0d\u5b58\u5728\u7684 Key\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\u4e3a NULL \u503c\u3002</p>"},{"location":"reader/httpreader/#ispage","title":"isPage","text":"<p><code>isPage</code> \u53c2\u6570\u7528\u6765\u6307\u5b9a\u63a5\u53e3\u662f\u5426\u5206\u9875\uff0c\u5b83\u662f\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u5982\u679c\u4e3a <code>true</code> \u5219\u8868\u793a\u63a5\u53e3\u5206\u9875\uff0c\u5426\u5219\u8868\u793a\u4e0d\u5206\u9875\u3002</p> <p>\u5f53\u63a5\u53e3\u652f\u6301\u5206\u9875\u65f6\uff0c\u8be5\u76f4\u63a5\u4f1a\u81ea\u52a8\u5206\u9875\u8bfb\u53d6\uff0c\u76f4\u5230\u63a5\u53e3\u8fd4\u56de\u7684\u6700\u540e\u4e00\u6b21\u8fd4\u56de\u7684\u6570\u636e\u7684\u8bb0\u5f55\u6570\u5c0f\u4e8e\u6bcf\u9875\u7684\u8bb0\u5f55\u6570\u4e3a\u6b62\u3002</p>"},{"location":"reader/httpreader/#pageparams","title":"pageParams","text":"<p><code>pageParams</code> \u53c2\u6570\u4ec5\u5728 <code>isPage</code> \u53c2\u6570\u4e3a <code>true</code> \u65f6\u751f\u6548\uff0c\u5b83\u662f\u4e00\u4e2a JSON \u5b57\u5178\uff0c\u5305\u542b\u4e24\u4e2a\u53ef\u9009\u5b57\u6bb5 <code>pageIndex</code> \u548c <code>pageSize</code> \u3002</p> <p><code>pageIndex</code> \u7528\u6765\u8868\u793a\u7528\u4e8e\u5206\u9875\u6307\u793a\u7684\u5f53\u524d\u9875\u9762\uff0c\u4ed6\u662f\u4e00\u4e2a JSON \u5b57\u6bb5\uff0c\u5305\u542b\u4e24\u4e2a\u53ef\u9009\u5b57\u6bb5 <code>key</code> \u548c <code>value</code> \uff0c\u5176\u4e2d <code>key</code> \u7528\u6765\u6307\u5b9a\u8868\u793a\u9875\u7801\u7684\u53c2\u6570\u540d\uff0c<code>value</code> \u7528\u6765\u6307\u5b9a\u5f53\u524d\u9875\u7801\u7684\u503c\u3002</p> <p><code>pageSize</code> \u7528\u6765\u8868\u793a\u7528\u4e8e\u5206\u9875\u6307\u793a\u7684\u6bcf\u9875\u5927\u5c0f\uff0c\u4ed6\u662f\u4e00\u4e2a JSON \u5b57\u6bb5\uff0c\u5305\u542b\u4e24\u4e2a\u53ef\u9009\u5b57\u6bb5 <code>key</code> \u548c <code>value</code> \uff0c\u5176\u4e2d <code>key</code> \u7528\u6765\u6307\u5b9a\u8868\u793a\u6bcf\u9875\u5927\u5c0f\u7684\u53c2\u6570\u540d\uff0c<code>value</code> \u7528\u6765\u6307\u5b9a\u6bcf\u9875\u5927\u5c0f\u7684\u503c\u3002</p> <p>\u8fd9\u4e24\u4e2a\u53c2\u6570\u7684\u9ed8\u8ba4\u503c\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"pageIndex\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"pageSize\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>\u5982\u679c\u4f60\u7684\u63a5\u53e3\u5206\u9875\u53c2\u6570\u4e0d\u662f <code>pageIndex</code> \u548c <code>pageSize</code> \uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 <code>pageParams</code> \u53c2\u6570\u6765\u6307\u5b9a\u3002\u6bd4\u5982</p> <pre><code>{\n  \"isPage\": true,\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"page\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"size\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>\u8fd9\u8868\u793a\u4f60\u4f20\u9012\u7ed9\u63a5\u53e3\u7684\u5206\u9875\u53c2\u6570\u4e3a <code>page=1&amp;size=100</code> \u3002</p>"},{"location":"reader/httpreader/#_6","title":"\u9650\u5236\u8bf4\u660e","text":"<ol> <li>\u8fd4\u56de\u7684\u7ed3\u679c\u5fc5\u987b\u662f JSON \u7c7b\u578b</li> <li>\u5f53\u524d\u6240\u6709 key \u7684\u503c\u5747\u5f53\u4f5c\u5b57\u7b26\u4e32\u7c7b\u578b</li> <li>\u6682\u4e0d\u652f\u6301\u63a5\u53e3 Token \u9274\u6743\u6a21\u5f0f</li> <li>\u6682\u4e0d\u652f\u6301\u5206\u9875\u83b7\u53d6</li> <li>\u4ee3\u7406\u4ec5\u652f\u6301 <code>http</code> \u6a21\u5f0f</li> </ol>"},{"location":"reader/influxdb2reader/","title":"InfluxDB2 Reader","text":"<p>InfluxDB2 Reader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece InfluxDB 2.0 \u53ca\u4ee5\u4e0a\u7248\u672c\u8bfb\u53d6\u6570\u636e\u3002</p> <p>\u6ce8\u610f\uff0c\u5982\u679c\u4f60\u7684 InfluxDB \u662f 1.8\u53ca\u4ee5\u4e0b\u7248\u672c\uff0c\u5219\u5e94\u8be5\u4f7f\u7528 InfluxDBReader \u63d2\u4ef6</p>"},{"location":"reader/influxdb2reader/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u7528\u6765\u6f14\u793a\u8be5\u63d2\u4ef6\u5982\u4f55\u4ece\u6307\u5b9a\u8868(\u5373\u6307\u6807)\u4e0a\u8bfb\u53d6\u6570\u636e\u5e76\u8f93\u51fa\u5230\u7ec8\u7aef</p>"},{"location":"reader/influxdb2reader/#job","title":"\u521b\u5efa job \u6587\u4ef6","text":"<p>\u521b\u5efa <code>job/influx2stream.json</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/influx2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdb2reader\",\n        \"parameter\": {\n          \"column\": [\n            \"location\",\n            \"height\",\n            \"wet\"\n          ],\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"bucket\": \"test\",\n            \"table\": [\n              \"temperature\"\n            ],\n            \"org\": \"com.wgzhao\"\n          },\n          \"token\": \"YOUR_SECURE_TOKEN\",\n          \"range\": [\n            \"-1h\",\n            \"-5m\"\n          ],\n          \"limit\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/influxdb2reader/#_2","title":"\u8fd0\u884c","text":"<p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/influx2stream.json\n</code></pre>"},{"location":"reader/influxdb2reader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 InfluxDB \u8fde\u63a5\u4e32 \uff5c token \u662f string \u65e0 \u8bbf\u95ee\u6570\u636e\u5e93\u7684 token table \u5426 list \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d(\u5373\u6307\u6807) org \u662f string \u65e0 \u6307\u5b9a InfluxDB \u7684 org \u540d\u79f0 bucket \u662f string \u65e0 \u6307\u5b9a InfluxDB \u7684 bucket \u540d\u79f0 column \u5426 list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1 rdbmreader range \u662f list \u65e0 \u8bfb\u53d6\u6570\u636e\u7684\u65f6\u95f4\u8303\u56f4 limit \u5426 int \u65e0 \u9650\u5236\u83b7\u53d6\u8bb0\u5f55\u6570"},{"location":"reader/influxdb2reader/#column","title":"column","text":"<p>\u5982\u679c\u4e0d\u6307\u5b9a <code>column</code>, \u6216\u8005\u6307\u5b9a <code>column</code> \u4e3a <code>[\"*\"]</code> \uff0c \u5219\u4f1a\u8bfb\u53d6\u6240\u6709\u6709\u6548\u7684 <code>_field</code> \u5b57\u6bb5\u4ee5\u53ca <code>_time</code> \u5b57\u6bb5 \u5426\u5219\u6309\u7167\u6307\u5b9a\u5b57\u6bb5\u8bfb\u53d6</p>"},{"location":"reader/influxdb2reader/#range","title":"range","text":"<p><code>range</code> \u7528\u6765\u6307\u5b9a\u8bfb\u53d6\u6307\u6807\u7684\u65f6\u95f4\u8303\u56f4\uff0c\u5176\u683c\u5f0f\u5982\u4e0b:</p> <pre><code>{\n  \"range\": [\"start_time\", \"end_time\"]\n}\n</code></pre> <p><code>range</code>  \u7531\u4e24\u4e2a\u5b57\u7b26\u4e32\u7ec4\u6210\u7684\u5217\u8868\u7ec4\u6210\uff0c\u7b2c\u4e00\u4e2a\u5b57\u7b26\u4e32\u8868\u793a\u5f00\u59cb\u65f6\u95f4\uff0c\u7b2c\u4e8c\u4e2a\u8868\u793a\u7ed3\u675f\u65f6\u95f4\u3002\u5176\u65f6\u95f4\u8868\u8fbe\u65b9\u5f0f\u8981\u6c42\u7b26\u5408 Flux \u683c\u5f0f\u8981\u6c42,\u7c7b\u4f3c\u8fd9\u6837:</p> <pre><code>{\n  \"range\": [\"-15h\", \"-2h\"]\n}\n</code></pre> <p>\u5176\u4e2d\u7b2c\u4e8c\u4e2a\u7ed3\u675f\u65f6\u95f4\u5982\u679c\u4e0d\u60f3\u6307\u5b9a\uff0c\u53ef\u4ee5\u4e0d\u5199\uff0c\u7c7b\u4f3c\u8fd9\u6837\uff1a</p> <pre><code>{\n  \"range\": [\"-15h\"]\n}\n</code></pre>"},{"location":"reader/influxdb2reader/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u5f53\u524d\u5b9e\u73b0\u662f\u5c06\u6240\u6709\u5b57\u6bb5\u5f53\u4f5c\u5b57\u7b26\u4e32\u5904\u7406</p>"},{"location":"reader/influxdb2reader/#_5","title":"\u9650\u5236","text":"<ol> <li>\u5f53\u524d\u63d2\u4ef6\u4ec5\u652f\u6301 2.0 \u53ca\u4ee5\u4e0a\u7248\u672c</li> </ol>"},{"location":"reader/influxdbreader/","title":"InfluxDB Reader","text":"<p>InfluxDBReader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece InfluxDB \u8bfb\u53d6\u6570\u636e\u3002\u5e95\u5c42\u5b9e\u73b0\u4e0a\uff0c\u662f\u901a\u8fc7\u8c03\u7528 InfluQL \u8bed\u8a00\u67e5\u8be2\u8868\uff0c\u7136\u540e\u83b7\u5f97\u8fd4\u56de\u6570\u636e\u3002</p>"},{"location":"reader/influxdbreader/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u7528\u6765\u6f14\u793a\u8be5\u63d2\u4ef6\u5982\u4f55\u4ece\u6307\u5b9a\u8868(\u5373\u6307\u6807)\u4e0a\u8bfb\u53d6\u6570\u636e\u5e76\u8f93\u51fa\u5230\u7ec8\u7aef</p>"},{"location":"reader/influxdbreader/#_2","title":"\u521b\u5efa\u9700\u8981\u7684\u5e93\u8868\u548c\u6570\u636e","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u521b\u5efa\u9700\u8981\u8bfb\u53d6\u7684\u8868\u4ee5\u53ca\u6570\u636e</p> <pre><code># create database\ninflux --execute \"CREATE DATABASE NOAA_water_database\"\n# download sample data\ncurl https://s3.amazonaws.com/noaa.water-database/NOAA_data.txt -o NOAA_data.txt\n# import data via influx-cli\ninflux -import -path=NOAA_data.txt -precision=s -database=NOAA_water_database\n</code></pre>"},{"location":"reader/influxdbreader/#job","title":"\u521b\u5efa job \u6587\u4ef6","text":"<p>\u521b\u5efa <code>job/influxdb2stream.json</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/influxdb2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdbreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"where\": \"1=1\",\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"NOAA_water_database\",\n            \"table\": \"h2o_feet\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/influxdbreader/#_3","title":"\u8fd0\u884c","text":"<p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/influxdb2stream.json\n</code></pre>"},{"location":"reader/influxdbreader/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 InfluxDB \u8fde\u63a5\u4e32 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 database \u662f string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7684\u6570\u636e\u5e93 table \u662f string \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1 rdbmreader connTimeout \u5426 int 15 \u8bbe\u7f6e\u8fde\u63a5\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 readTimeout \u5426 int 20 \u8bbe\u7f6e\u8bfb\u53d6\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 writeTimeout \u5426 int 20 \u8bbe\u7f6e\u5199\u5165\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 where \u5426 string \u65e0 \u9488\u5bf9\u8868\u7684\u7b5b\u9009\u6761\u4ef6 querySql \u5426 string \u65e0 \u4f7f\u7528 SQL \u67e5\u8be2\u83b7\u53d6\u6570\u636e\uff0c\u5982\u914d\u7f6e\u8be5\u9879\uff0c\u5219 <code>table</code>\uff0c<code>column</code> \u914d\u7f6e\u9879\u65e0\u6548"},{"location":"reader/influxdbreader/#_5","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u5f53\u524d\u5b9e\u73b0\u662f\u5c06\u6240\u6709\u5b57\u6bb5\u5f53\u4f5c\u5b57\u7b26\u4e32\u5904\u7406</p>"},{"location":"reader/influxdbreader/#_6","title":"\u9650\u5236","text":"<ol> <li>\u5f53\u524d\u63d2\u4ef6\u4ec5\u652f\u6301 1.x \u7248\u672c\uff0c2.0 \u53ca\u4ee5\u4e0a\u5e76\u4e0d\u652f\u6301</li> </ol>"},{"location":"reader/jsonfilereader/","title":"JSON File Reader","text":"<p>JSON File Reader \u63d0\u4f9b\u4e86\u8bfb\u53d6\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u6570\u636e\u5b58\u50a8\u7684\u80fd\u529b\u3002</p>"},{"location":"reader/jsonfilereader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"job/json2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      },\n      \"reader\": {\n        \"name\": \"jsonfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/test*.json\"\n          ],\n          \"column\": [\n            {\n              \"index\": \"$.id\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.name\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": \"$.age\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.score.math\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.score.english\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.pubdate\",\n              \"type\": \"date\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"constant string\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5176\u4e2d <code>/tmp/test*.json</code> \u4e3a\u540c\u4e00\u4e2a json \u6587\u4ef6\u7684\u591a\u4e2a\u590d\u5236\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>{\"name\": \"zhangshan\",\"id\": 19890604,\"age\": 12,\"score\": {\"math\": 92.5,\"english\": 97.5,\"chinese\": 95},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"lisi\",\"id\": 19890605,\"age\": 12,\"score\": {\"math\": 90.5,\"english\": 77.5,\"chinese\": 90},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"wangwu\",\"id\": 19890606,\"age\": 12,\"score\": {\"math\": 89,\"english\": 100,\"chinese\": 92},\"pubdate\": \"2020-09-05\"}\n</code></pre>"},{"location":"reader/jsonfilereader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f list \u65e0 \u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84,\u8be6\u7ec6\u63cf\u8ff0\u89c1\u4e0b\u6587 column \u662f list \u65e0 \u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype\u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c\u8be6\u89c1\u4e0b\u6587 fieldDelimiter \u662f string <code>,</code> \u63cf\u8ff0\uff1a\u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26 compress \u5426 string \u65e0 \u6587\u672c\u538b\u7f29\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e0d\u586b\u5199\u610f\u5473\u7740\u6ca1\u6709\u538b\u7f29\u3002\u652f\u6301\u538b\u7f29\u7c7b\u578b\u4e3azip\u3001gzip\u3001bzip2 encoding \u5426 string utf-8 \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e singleLine \u5426 boolean true \u6bcf\u6761\u6570\u636e\u662f\u5426\u4e3a\u4e00\u884c\uff0c \u8be6\u89c1\u4e0b\u6587"},{"location":"reader/jsonfilereader/#path","title":"path","text":"<p>\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\uff0c\u6bd4\u5982\uff1a</p> <pre><code>{\n  \"path\": [\n    \"/var/ftp/test.json\", // \u8bfb\u53d6 /var/ftp \u76ee\u5f55\u4e0b\u7684 test.json \u6587\u4ef6\n    \"/var/tmp/*.json\", // \u8bfb\u53d6 /var/tmp \u76ee\u5f55\u4e0b\u6240\u6709 json \u6587\u4ef6\n    \"/public/ftp\", // \u8bfb\u53d6 /public/ftp \u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6, \u5982\u679c ftp \u662f\u6587\u4ef6\u7684\u8bdd\uff0c\u5219\u76f4\u63a5\u8bfb\u53d6\n    \"/public/a??.json\" // \u8bfb\u53d6 /public \u76ee\u5f55\u4e0b\u6240\u6709 a \u5f00\u5934\uff0c\u540e\u9762\u8ddf\u4e24\u4e2a\u5b57\u7b26\uff0c\u6700\u540e\u662f json \u7ed3\u5c3e\u7684\u6587\u4ef6\n  ]\n}\n</code></pre> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5982\u679cPath\u6307\u5b9a\u7684\u8def\u5f84\u4e0b\u6ca1\u6709\u7b26\u5408\u5339\u914d\u7684\u6587\u4ef6\u62bd\u53d6\uff0cAddax\u5c06\u62a5\u9519\u3002</p>"},{"location":"reader/jsonfilereader/#column","title":"column","text":"<p>\u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype\u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0cindex\u6307\u5b9a\u5f53\u524d\u5217\u6765\u81ea\u4e8ejson\u7684\u6307\u5b9a\uff0c\u8bed\u6cd5\u4e3a Jayway JsonPath \u7684\u8bed\u6cd5\uff0cvalue\u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece\u6e90\u5934\u6587\u4ef6\u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636evalue\u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002 \u7528\u6237\u5fc5\u987b\u6307\u5b9aColumn\u5b57\u6bb5\u4fe1\u606f</p> <p>\u5bf9\u4e8e\u7528\u6237\u6307\u5b9aColumn\u4fe1\u606f\uff0ctype\u5fc5\u987b\u586b\u5199\uff0cindex/value \u5fc5\u987b\u9009\u62e9\u5176\u4e00</p>"},{"location":"reader/jsonfilereader/#singleline","title":"singleLine","text":"<p>\u4f7f\u7528 JSON \u683c\u5f0f\u5b58\u50a8\u6570\u636e\uff0c\u4e1a\u754c\u6709\u4e24\u79cd\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f\u6bcf\u884c\u4e00\u4e2a JSON \u5bf9\u8c61\uff0c\u4e5f\u5c31\u662f <code>Single Line JSON(aka. JSONL or JSON Lines)</code>; \u53e6\u4e00\u79cd\u662f\u6574\u4e2a\u6587\u4ef6\u662f\u4e00\u4e2a JSON \u6570\u7ec4\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a JSON \u5bf9\u8c61\uff0c\u4e5f\u5c31\u662f <code>Multiline JSON</code>\u3002</p> <p>Addax \u9ed8\u8ba4\u652f\u6301\u6bcf\u884c\u4e00\u4e2a JSON \u5bf9\u8c61\u7684\u683c\u5f0f\uff0c\u5373 <code>singeLine = true</code>, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8981\u6ce8\u610f\u7684\u662f\uff1a</p> <ol> <li>\u6bcf\u884c JSON \u5bf9\u8c61\u7684\u672b\u5c3e\u4e0d\u80fd\u6709\u9017\u53f7\uff0c\u5426\u5219\u4f1a\u89e3\u6790\u5931\u8d25\u3002</li> <li>\u4e00\u4e2aJSON \u5bf9\u8c61\u4e0d\u80fd\u8de8\u884c\uff0c\u5426\u5219\u4f1a\u89e3\u6790\u5931\u8d25\u3002</li> </ol> <p>\u5982\u679c\u6570\u636e\u662f\u6574\u4e2a\u6587\u4ef6\u662f\u4e00\u4e2a JSON \u6570\u7ec4\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a JSON \u5bf9\u8c61\uff0c\u9700\u8981\u8bbe\u7f6e <code>singeLine</code> \u4e3a <code>false</code>\u3002 \u5047\u8bbe\u4e0a\u8ff0\u5217\u5b50\u4e2d\u7684\u6570\u636e\u7528\u4e0b\u9762\u7684\u683c\u5f0f\u8868\u793a\uff1a</p> <pre><code>{\n  \"result\": [\n    {\n      \"name\": \"zhangshan\",\n      \"id\": 19890604,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 92.5,\n        \"english\": 97.5,\n        \"chinese\": 95\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"lisi\",\n      \"id\": 19890605,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 90.5,\n        \"english\": 77.5,\n        \"chinese\": 90\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"wangwu\",\n      \"id\": 19890606,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 89,\n        \"english\": 100,\n        \"chinese\": 92\n      },\n      \"pubdate\": \"2020-09-05\"\n    }\n  ]\n}\n</code></pre> <p>\u56e0\u4e3a\u8fd9\u79cd\u683c\u5f0f\u662f\u5408\u6cd5\u7684 JSON \u683c\u5f0f\uff0c\u56e0\u6b64\u6bcf\u4e2a JSON \u5bf9\u8c61\u53ef\u4ee5\u8de8\u884c\u3002\u76f8\u5e94\u7684\uff0c\u8fd9\u7c7b\u6570\u636e\u8bfb\u53d6\u65f6\uff0c\u5176 <code>path</code> \u914d\u7f6e\u5e94\u8be5\u5982\u4e0b\u586b\u5199\uff1a</p> <pre><code>{\n  \"singleLine\": false,\n  \"column\": [\n    {\n      \"index\": \"$.result[*].id\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].name\",\n      \"type\": \"string\"\n    },\n    {\n      \"index\": \"$.result[*].age\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].score.math\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$.result[*].score.english\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$..result[*].pubdate\",\n      \"type\": \"date\"\n    },\n    {\n      \"type\": \"string\",\n      \"value\": \"constant string\"\n    }\n  ]\n}\n</code></pre> <p>\u66f4\u8be6\u7ec6\u7684\u4f7f\u7528\u8bf4\u660e\u8bf7\u53c2\u8003 Jayway JsonPath \u7684\u8bed\u6cd5\u3002</p> <p>\u6ce8\u610f: \u8fd9\u79cd\u6570\u636e\u5728\u4e00\u4e2a JSON \u6570\u7ec4\u91cc\u65f6\uff0c\u7a0b\u5e8f\u53ea\u80fd\u91c7\u53d6\u5c06\u6574\u4e2a\u6587\u4ef6\u8bfb\u53d6\u5230\u5185\u5b58\u4e2d\uff0c\u7136\u540e\u89e3\u6790\u7684\u65b9\u5f0f\uff0c\u56e0\u6b64\u4e0d\u9002\u5408\u5927\u6587\u4ef6\u7684\u8bfb\u53d6\u3002 \u5bf9\u4e8e\u5927\u6587\u4ef6\u7684\u8bfb\u53d6\uff0c\u5efa\u8bae\u4f7f\u7528\u6bcf\u884c\u4e00\u4e2a JSON \u5bf9\u8c61\u7684\u683c\u5f0f\uff0c\u4e5f\u5c31\u662f <code>Single Line JSON</code> \u7684\u683c\u5f0f\uff0c\u8fd9\u79cd\u683c\u5f0f\u53ef\u4ee5\u91c7\u53d6\u9010\u884c\u8bfb\u53d6\u7684\u65b9\u5f0f\uff0c\u4e0d\u4f1a\u5360\u7528\u592a\u591a\u5185\u5b58\u3002</p>"},{"location":"reader/jsonfilereader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b \u672c\u5730\u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long Double Double String String Boolean Boolean Date Date"},{"location":"reader/kafkareader/","title":"Kafka Reader","text":"<p>Kafka Reader \u63d2\u4ef6\u5b9e\u73b0\u4ece Kafka \u961f\u5217\u4e2d\u8bfb\u53d6 JSON \u683c\u5f0f\u6d88\u606f\u7684\u529f\u80fd\u3002 \u8be5\u63d2\u4ef6\u5728 <code>4.0.10</code> \u7248\u672c\u4e2d\u5f15\u5165\u3002</p>"},{"location":"reader/kafkareader/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u6f14\u793a\u4e86\u5982\u4f55\u4ece\u4ece kafka \u7684\u8bfb\u53d6\u6307\u5b9a topic \u4e2d\uff0c\u5e76\u8f93\u51fa\u5230\u7ec8\u7aef\u4e0a\u3002</p>"},{"location":"reader/kafkareader/#_2","title":"\u521b\u5efa\u4efb\u52a1\u6587\u4ef6","text":"<p>\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u4efb\u52a1\u6587\u4ef6  <code>kafka2stream.json</code> , \u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        },\n        \"reader\": {\n          \"name\": \"kafkareader\",\n          \"parameter\": {\n            \"brokerList\": \"wgzhao-laptop:9092\",\n            \"topic\": \"test-1\",\n            \"column\": [\n              \"col1\",\n              \"col3\",\n              \"col0\",\n              \"col9\"\n            ],\n            \"missingKeyValue\": \"\\\\N\",\n            \"properties\": {\n              \"auto.offset.reset\": \"earliest\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"reader/kafkareader/#_3","title":"\u8fd0\u884c","text":"<p>\u6267\u884c  <code>bin/addax.sh kafka2stream.json</code> \u547d\u4ee4\u3002</p>"},{"location":"reader/kafkareader/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 brokerList \u662f string \u65e0 \u8fde\u63a5 kafka \u670d\u52a1\u7684 broker \u914d\u7f6e\uff0c\u7c7b\u4f3c <code>localhost:9092</code> \uff0c\u591a\u4e2a broker\u4e4b\u95f4\u7528\u9017\u53f7(<code>,</code>)\u5206\u9694 topic \u662f string \u65e0 \u8981\u5199\u5165\u7684 topic column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u4ee5\u4e0b\u8be6\u8ff0 missingKeyValue \u5426 string \u65e0 \u5b57\u6bb5\u4e0d\u5b58\u5728\u65f6\u7528\u4ec0\u4e48\u503c\u586b\u5145\uff0c\u4ee5\u4e0b\u8be6\u8ff0 properties \u5426 map \u65e0 \u9700\u8981\u8bbe\u7f6e\u7684\u5176\u4ed6 kafka \u8fde\u63a5\u53c2\u6570"},{"location":"reader/kafkareader/#column","title":"column","text":"<p><code>column</code> \u7528\u6765\u6307\u5b9a\u8981\u8bfb\u53d6\u7684 JSON \u6d88\u606f\u4e2d\u7684 key\uff0c\u5982\u679c\u586b\u5199\u4e3a <code>*</code> \uff0c\u5219\u8868\u793a\u8bfb\u53d6\u6d88\u606f\u4e2d\u7684\u6240\u6709 key\u3002\u4f46\u8981\u6ce8\u610f\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e00\u662f\u8f93\u51fa\u4e0d\u4f1a\u6709\u6392\u5e8f\uff0c\u4e5f\u5c31\u662f\u8bf4\u7b2c\u6bcf\u6761\u8bb0\u5f55\u7684 key \u7684 \u8f93\u51fa\u987a\u5e8f\u4e0d\u786e\u4fdd\u4e00\u81f4\u3002</p> <p>\u4e5f\u53ef\u4ee5\u6307\u5b9a key \u6765\u8fdb\u884c\u8bfb\u53d6\uff0c\u6bd4\u5982</p> <pre><code>{\n  \"column\": [\"col1\", \"col2\", \"col3\"]\n}\n</code></pre> <p>\u8fd9\u6837\uff0c\u63d2\u4ef6\u4f1a\u5c1d\u8bd5\u6309\u7167\u7ed9\u5b9a\u7684\u987a\u5e8f\u53bb\u8bfb\u53d6\u76f8\u5e94\u7684 key\uff0c\u5982\u679c\u4e00\u6761\u6d88\u606f\u4e2d\u8981\u8bfb\u53d6\u7684 key \u4e0d\u5b58\u5728\uff0c\u63d2\u4ef6\u4f1a\u62a5\u9519\u5e76\u9000\u51fa\u3002\u5982\u679c\u5e0c\u671b\u4e0d\u9000\u51fa\uff0c\u5219\u53ef\u4ee5\u8bbe\u7f6e <code>missingKeyValue</code> \u4ed6\u8868\u793a\u5f53\u8981\u8bfb\u53d6\u7684 key \u4e0d\u5b58\u5728\u65f6\uff0c\u7528\u8be5\u914d\u7f6e\u7684\u503c\u6765\u586b\u5145\u3002</p> <p>\u53e6\u5916\uff0c\u8bfb\u53d6\u7684 key \u7684\u503c\u7684\u7c7b\u578b\uff0c\u63d2\u4ef6\u4f1a\u81ea\u52a8\u53bb\u731c\u6d4b\uff0c\u5982\u679c\u7c7b\u578b\u65e0\u6cd5\u731c\u6d4b\uff0c\u5219\u4f1a\u5f53\u4f5c String \u7c7b\u578b\u3002</p>"},{"location":"reader/kafkareader/#_5","title":"\u9650\u5236","text":"<ol> <li>\u4ec5\u652f\u6301 Kafka <code>1.0</code> \u53ca\u4ee5\u4e0a\u7248\u672c\uff0c\u4f4e\u4e8e\u8be5\u7248\u672c\u7684\u65e0\u6cd5\u786e\u5b9a\u662f\u5426\u80fd\u5199\u5165</li> <li>\u5f53\u524d\u4e0d\u652f\u6301\u542f\u7528\u4e86 <code>kerberos</code> \u8ba4\u8bc1\u7684 kafka \u670d\u52a1</li> </ol>"},{"location":"reader/kudureader/","title":"Kudu Reader","text":"<p>Kudu Reader \u63d2\u4ef6\u5229\u7528 Kudu \u7684 java\u5ba2\u6237\u7aef KuduClient \u8fdb\u884c Kudu \u7684\u8bfb\u64cd\u4f5c\u3002</p>"},{"location":"reader/kudureader/#_1","title":"\u914d\u7f6e\u793a\u4f8b","text":"<p>\u6211\u4eec\u901a\u8fc7 Trino  \u7684 <code>kudu connector</code> \u8fde\u63a5 kudu \u670d\u52a1\uff0c\u7136\u540e\u8fdb\u884c\u8868\u521b\u5efa\u4ee5\u53ca\u6570\u636e\u63d2\u5165</p>"},{"location":"reader/kudureader/#_2","title":"\u5efa\u8868\u8bed\u53e5\u4ee5\u53ca\u6570\u636e\u63d2\u5165\u8bed\u53e5","text":"<pre><code>CREATE TABLE kudu.default.users (\n  user_id int WITH (primary_key = true),\n  user_name varchar with (nullable=true),\n  age int with (nullable=true),\n  salary double with (nullable=true),\n  longtitue decimal(18,6) with (nullable=true),\n  latitude decimal(18,6) with (nullable=true),\n  p decimal(21,20) with (nullable=true),\n  mtime timestamp with (nullable=true)\n) WITH (\n  partition_by_hash_columns = ARRAY['user_id'],\n  partition_by_hash_buckets = 2\n);\n\ninsert into kudu.default.users \nvalues \n(1, cast('wgzhao' as varchar), 18, cast(18888.88 as double), \n cast(123.282424 as decimal(18,6)), cast(23.123456 as decimal(18,6)),\n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 14:40:41'),\n(2, cast('anglina' as varchar), 16, cast(23456.12 as double), \n cast(33.192123 as decimal(18,6)), cast(56.654321 as decimal(18,6)), \n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 03:40:41');\n-- ONLY insert primary key value\n insert into kudu.default.users(user_id) values  (3);\n</code></pre>"},{"location":"reader/kudureader/#_3","title":"\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u662f\u8bfb\u53d6kudu\u8868\u5e76\u8f93\u51fa\u5230\u7ec8\u7aef\u7684\u914d\u7f6e</p> job/kudu2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"kudureader\",\n        \"parameter\": {\n          \"masterAddress\": \"localhost:7051,localhost:7151,localhost:7251\",\n          \"table\": \"users\",\n          \"splitPk\": \"user_id\",\n          \"lowerBound\": 1,\n          \"upperBound\": 100,\n          \"readTimeout\": 5,\n          \"scanTimeout\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u628a\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/kudu2stream.json</code></p>"},{"location":"reader/kudureader/#_4","title":"\u6267\u884c","text":"<p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u91c7\u96c6</p> <pre><code>bin/addax.sh job/kudu2stream.json\n</code></pre>"},{"location":"reader/kudureader/#_5","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 masterAddress \u662f string \u65e0 Kudu Master \u96c6\u7fa4RPC\u5730\u5740,\u591a\u4e2a\u5730\u5740\u7528\u9017\u53f7(,)\u5206\u9694 table \u662f string \u65e0 kudu \u8868\u540d splitPk \u5426 string \u65e0 \u5e76\u884c\u8bfb\u53d6\u6570\u636e\u5206\u7247\u5b57\u6bb5 lowerBound \u5426 string \u65e0 \u5e76\u884c\u8bfb\u53d6\u6570\u636e\u5206\u7247\u8303\u56f4\u4e0b\u754c upperBound \u5426 string \u65e0 \u5e76\u884c\u8bfb\u53d6\u6570\u636e\u5206\u7247\u8303\u56f4\u4e0a\u754c readTimeout \u5426 int 10 \u8bfb\u53d6\u6570\u636e\u8d85\u65f6(\u79d2) scanTimeout \u5426 int 20 \u6570\u636e\u626b\u63cf\u8bf7\u6c42\u8d85\u65f6(\u79d2) column \u5426 list \u65e0 \u6307\u5b9a\u8981\u83b7\u53d6\u7684\u5b57\u6bb5 where \u5426 list \u65e0 \u6307\u5b9a\u5176\u4ed6\u8fc7\u6ee4\u6761\u4ef6\uff0c\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 haveKerberos \u5426 boolean false \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u9700\u8981\u540c\u65f6\u914d\u7f6e\u4ee5\u4e0b\u4e24\u9879 kerberosKeytabFilePath \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u6587\u4ef6\u8def\u5f84, \u6bd4\u5982 <code>/your/path/addax.service.keytab</code> kerberosPrincipal \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u4e3b\u4f53, \u6bd4\u5982 <code>addax/node1@WGZHAO.COM</code>"},{"location":"reader/kudureader/#where","title":"where","text":"<p><code>where</code> \u7528\u6765\u5b9a\u5236\u66f4\u591a\u7684\u8fc7\u6ee4\u6761\u4ef6\uff0c\u4ed6\u662f\u4e00\u4e2a\u6570\u7ec4\u7c7b\u578b\uff0c\u6570\u7ec4\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u4e00\u4e2a\u8fc7\u6ee4\u6761\u4ef6\uff0c\u6bd4\u5982</p> <pre><code>{\n  \"where\": [\"age &gt; 1\", \"user_name = 'wgzhao'\"] \n}\n</code></pre> <p>\u4e0a\u8ff0\u5b9a\u4e49\u4e86\u4e24\u4e2a\u8fc7\u6ee4\u6761\u4ef6\uff0c\u6bcf\u4e2a\u8fc7\u6ee4\u6761\u4ef6\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff0c\u683c\u5f0f\u4e3a  <code>column operator value</code></p> <ul> <li><code>column</code>: \u8981\u8fc7\u6ee4\u7684\u5b57\u6bb5</li> <li><code>operator</code>: \u6bd4\u8f83\u7b26\u53f7\uff0c\u5f53\u524d\u4ec5\u652f\u6301 <code>=</code>,  <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code> , <code>!=</code> \u5176\u4ed6\u64cd\u4f5c\u7b26\u53f7\u5f53\u524d\u8fd8\u4e0d\u652f\u6301</li> <li><code>value</code>: \u6bd4\u8f83\u503c</li> </ul> <p>\u8fd9\u91cc\u8fd8\u6709\u5176\u4ed6\u4e00\u4e9b\u9650\u5b9a\uff0c\u5728\u4f7f\u7528\u65f6\uff0c\u8981\u7279\u522b\u6ce8\u610f\uff1a</p> <ol> <li>\u591a\u4e2a\u8fc7\u6ee4\u6761\u4ef6\u4e4b\u95f4\u7684\u903b\u8f91\u4e0e\u5173\u7cfb(<code>AND</code>)\uff0c\u6682\u4e0d\u652f\u6301\u903b\u8f91\u6216(<code>OR</code>)\u5173\u7cfb</li> </ol>"},{"location":"reader/kudureader/#_6","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b Kudu \u6570\u636e\u7c7b\u578b Long byte, short, int, long Double float, double, decimal String string Date timestamp Boolean boolean Bytes binary"},{"location":"reader/mongodbreader/","title":"MongoDB Reader","text":"<p>MongoDBReader \u63d2\u4ef6\u5229\u7528 MongoDB \u7684java\u5ba2\u6237\u7aefMongoClient\u8fdb\u884cMongoDB\u7684\u8bfb\u64cd\u4f5c\u3002</p>"},{"location":"reader/mongodbreader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u8be5\u793a\u4f8b\u4eceMongoDB\u4e2d\u8bfb\u4e00\u5f20\u8868\u5e76\u6253\u5370\u5230\u7ec8\u7aef</p> job/mongo2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mongodbreader\",\n        \"parameter\": {\n          \"username\": \"\",\n          \"password\": \"\",\n          \"column\": [\n            \"unique_id\",\n            \"sid\",\n            \"user_id\",\n            \"auction_id\",\n            \"content_type\",\n            \"pool_type\",\n            \"frontcat_id\",\n            \"catagoryid\",\n            \"gmt_create\",\n            \"taglist\",\n            \"property\",\n            \"scorea\",\n            \"scoreb\",\n            \"scorec\"\n          ],\n          \"connection\": {\n            \"address\": [\n              \"127.0.0.1:27017\"\n            ],\n            \"database\": \"tag_per_data\",\n            \"collection\": \"tag_data\",\n            \"authDb\": \"admin\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/mongodbreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 address \u662f list \u65e0 MongoDB \u7684\u6570\u636e\u5730\u5740\u4fe1\u606f, \u53ef\u5199\u591a\u4e2a username \u5426 string \u65e0 MongoDB \u7528\u6237\u540d password \u5426 string \u65e0 MongoDB \u5bc6\u7801 database \u662f string \u65e0 MongoDB \u6570\u636e\u5e93 collection \u662f string \u65e0 MongoDB \u7684\u96c6\u5408\u540d column \u662f list \u65e0 MongoDB \u7684\u6587\u6863\u5217\u540d\uff0c\u4e0d\u652f\u6301 <code>[\"*\"]</code> \u83b7\u53d6\u6240\u6709\u5217\u65b9\u5f0f query \u5426 string \u65e0 \u81ea\u5b9a\u4e49\u67e5\u8be2\u6761\u4ef6 fetchSize \u5426 int 2048 \u6279\u91cf\u83b7\u53d6\u7684\u8bb0\u5f55\u6570"},{"location":"reader/mongodbreader/#collection","title":"collection","text":"<p>\u8fd9\u91cc\u7684 <code>collection</code>  \u76ee\u524d\u53ea\u652f\u6301\u5355\u4e00 collection\uff0c\u56e0\u6b64\u8bbe\u7f6e\u7c7b\u578b\u4e3a\u5b57\u7b26\u4e32\uff0c\u800c\u4e0d\u662f\u5176\u4ed6\u63d2\u4ef6\u5e38\u89c1\u7684\u6570\u7ec4\u7c7b\u578b\uff0c\u8fd9\u4e00\u70b9\u5c24\u4e3a\u6ce8\u610f\u3002</p>"},{"location":"reader/mongodbreader/#column","title":"column","text":"<p><code>column</code> \u7528\u6765\u6307\u5b9a\u9700\u8981\u8bfb\u53d6\u7684\u5b57\u6bb5\u540d\u79f0\uff0c\u8fd9\u91cc\u6211\u4eec\u505a\u4e86\u5b57\u6bb5\u540d\u79f0\u7684\u7ec4\u6210\u4e24\u4e2a\u5047\u5b9a\uff1a</p> <ul> <li>\u4e0d\u53ef\u80fd\u7528\u5355\u5f15\u53f7\u5f00\u5934(<code>'</code>)</li> <li>\u4e0d\u53ef\u80fd\u5168\u90e8\u7531\u6570\u5b57\u548c\u70b9(<code>.</code>)  \u7ec4\u6210</li> </ul> <p>\u57fa\u4e8e\u4ee5\u4e0a\u5047\u5b9a\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u7b80\u5316 <code>column</code> \u914d\u7f6e\u7684\u540c\u65f6\uff0c\u8fd8\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e9b\u5e38\u91cf\u4f5c\u4e3a\u8865\u5145\u5b57\u6bb5\uff0c\u6bd4\u5982\u4e00\u822c\u91c7\u96c6\u4e00\u5f20\u8868\uff0c\u6211\u4eec\u9700\u8981\u589e\u52a0\u91c7\u96c6\u65f6\u95f4\uff0c\u91c7\u96c6\u6e90\u7b49\u5e38\u91cf\uff0c\u90a3\u4e48\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e</p> <pre><code>{\n  \"column\": [\n    \"col1\",\n    \"col2\",\n    \"col3\",\n    \"'source_mongodb'\",\n    \"20211026\",\n    \"123.12\"\n  ]\n}\n</code></pre> <p>\u4e0a\u8ff0\u914d\u7f6e\u7684\u540e\u4e09\u4e2a\u5b57\u6bb5\u5c31\u662f\u5e38\u91cf\uff0c\u5206\u522b\u5f53\u4f5c\u5b57\u7b26\u7c7b\u578b\uff0c\u6574\u578b\u548c\u6d6e\u70b9\u578b\u5904\u7406\u3002</p>"},{"location":"reader/mongodbreader/#query","title":"query","text":"<p><code>query</code> \u662f\u53ea\u7b26\u5408 MongoDB \u67e5\u8be2\u683c\u5f0f\u7684 BSON \u5b57\u7b26\u4e32\uff0c\u6bd4\u5982\uff1a</p> <pre><code>{\n  \"query\": \"{amount: {$gt: 140900}, oc_date: {$gt: 20190110}}\"\n}\n</code></pre> <p>\u4e0a\u8ff0\u67e5\u8be2\u7c7b\u4f3c SQL \u4e2d\u7684 <code>where amount &gt; 140900 and oc_date &gt; 20190110</code></p>"},{"location":"reader/mongodbreader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b MongoDB \u6570\u636e\u7c7b\u578b Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes"},{"location":"reader/mysqlreader/","title":"MySQL Reader","text":"<p>MysqlReader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece MySQL \u8bfb\u53d6\u6570\u636e\u7684\u80fd\u529b</p>"},{"location":"reader/mysqlreader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u5728 MySQL \u7684 test \u5e93\u4e0a\u521b\u5efa\u5982\u4e0b\u8868\uff0c\u5e76\u63d2\u5165\u4e00\u6761\u8bb0\u5f55</p> <pre><code>CREATE TABLE addax_reader\n(\n    c_bigint     bigint,\n    c_varchar    varchar(100),\n    c_timestamp  timestamp,\n    c_text       text,\n    c_decimal    decimal(8, 3),\n    c_mediumtext mediumtext,\n    c_longtext   longtext,\n    c_int        int,\n    c_time       time,\n    c_datetime   datetime,\n    c_enum       enum('one', 'two', 'three'),\n    c_float      float,\n    c_smallint   smallint,\n    c_bit        bit,\n    c_double     double,\n    c_blob       blob,\n    c_char       char(5),\n    c_varbinary  varbinary(100),\n    c_tinyint    tinyint,\n    c_json       json,\n    c_set SET ('a', 'b', 'c', 'd'),\n    c_binary     binary,\n    c_longblob   longblob,\n    c_mediumblob mediumblob\n);\nINSERT INTO addax_reader\nVALUES (2E18,\n        'a varchar data',\n        '2021-12-12 12:12:12',\n        'a long text',\n        12345.122,\n        'a medium text',\n        'a long text',\n         2 ^ 32 - 1,\n        '12:13:14',\n        '2021-12-12 12:13:14',\n        'one',\n        17.191,\n        126,\n        0,\n        1114.1114,\n        'blob',\n        'a123b',\n        'a var binary content',\n        126,\n        '{\"k1\":\"val1\",\"k2\":\"val2\"}',\n        'b',\n        binary(1),\n        x'89504E470D0A1A0A0000000D494844520000001000000010080200000090916836000000017352474200',\n        x'89504E470D0A1A0A0000000D');\n</code></pre> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/mysql2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mysqlreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_reader\"\n            ],\n            \"jdbcUrl\": \"jdbc:mysql://127.0.0.1:3306/test\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/mysql2stream.json</code></p>"},{"location":"reader/mysqlreader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/mysql2stream.json\n</code></pre>"},{"location":"reader/mysqlreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/mysqlreader/#driver","title":"driver","text":"<p>\u5f53\u524d Addax \u91c7\u7528\u7684 MySQL JDBC \u9a71\u52a8\u4e3a 8.0 \u4ee5\u4e0a\u7248\u672c\uff0c\u9a71\u52a8\u7c7b\u540d\u4f7f\u7528\u7684 <code>com.mysql.cj.jdbc.Driver</code>\uff0c\u800c\u4e0d\u662f <code>com.mysql.jdbc.Driver</code>\u3002 \u5982\u679c\u4f60\u9700\u8981\u91c7\u96c6\u7684 MySQL \u670d\u52a1\u4f4e\u4e8e <code>5.6</code>\uff0c\u9700\u8981\u4f7f\u7528\u5230 <code>Connector/J 5.1</code> \u9a71\u52a8\uff0c\u5219\u53ef\u4ee5\u91c7\u53d6\u4e0b\u9762\u7684\u6b65\u9aa4\uff1a</p> <p>\u66ff\u6362\u63d2\u4ef6\u5185\u7f6e\u7684\u9a71\u52a8</p> <p><code>rm -f plugin/reader/mysqlreader/libs/mysql-connector-java-*.jar</code></p> <p>\u62f7\u8d1d\u8001\u7684\u9a71\u52a8\u5230\u63d2\u4ef6\u76ee\u5f55</p> <p><code>cp mysql-connector-java-5.1.48.jar plugin/reader/mysqlreader/libs/</code></p> <p>\u6307\u5b9a\u9a71\u52a8\u7c7b\u540d\u79f0</p> <p>\u5728\u4f60\u7684 json \u6587\u4ef6\u7c7b\uff0c\u914d\u7f6e <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p>"},{"location":"reader/mysqlreader/#_4","title":"\u7c7b\u578b\u8f6c\u6362\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li><code>tinyint(1)</code> \u4f1a\u89c6\u4e3a\u6574\u5f62</li> <li><code>year</code> \u88ab\u89c6\u4e3a\u6574\u5f62</li> <li><code>bit</code> \u5982\u679c\u662f <code>bit(1)</code> \u88ab\u89c6\u4e3a\u5e03\u5c14\u7c7b\u578b\uff0c\u5426\u5219\u5f53\u4f5c\u4e8c\u8fdb\u5236\u7c7b\u578b</li> </ul>"},{"location":"reader/mysqlreader/#_5","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u5728\u4f7f\u7528 <code>autoPk</code> \u6216\u8005 <code>splitPk</code> \u65f6\uff0c\u5982\u679c\u9009\u62e9\u7684\u5b57\u6bb5\u7c7b\u578b\u662f\u5b57\u7b26\u7c7b\u578b\uff0c\u4e14\u8be5\u5b57\u6bb5\u8bbe\u7f6e\u7684 <code>COLLATE</code> \u4e3a\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u5b57\u7b26\u96c6\uff0c\u6bd4\u5982 <code>utf8_general_ci</code> \u548c <code>utf8mb4_general_ci</code> \uff0c\u5219\u4f1a\u51fa\u73b0\u6570\u636e\u91cd\u590d\u7684\u60c5\u51b5\u3002\u56e0\u6b64\u9700\u8981\u907f\u514d\u4f7f\u7528\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u5b57\u7b26\u96c6\u3002</li> </ol>"},{"location":"reader/oraclereader/","title":"Oracle Reader","text":"<p>Oracle Reader \u63d2\u4ef6\u7528\u4e8e\u4ece Oracle \u8bfb\u53d6\u6570\u636e</p>"},{"location":"reader/oraclereader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4eceOracle\u6570\u636e\u5e93\u540c\u6b65\u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a:</p> job/oracle2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"oraclereader\",\n        \"parameter\": {\n          \"username\": \"oracle\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:oracle:thin:@127.0.0.1:5432/orcl\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/oraclereader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/oraclereader/#geometry","title":"\u5bf9 GEOMETRY \u7c7b\u578b\u7684\u652f\u6301","text":"<p>\u4ece Addax <code>4.0.13</code> \u5f00\u59cb\uff0c\u5b9e\u9a8c\u6027\u7684\u652f\u6301 Oracle GEOMETRY \u7c7b\u578b\uff0c\u8be5\u63d2\u4ef6\u4f1a\u628a\u8be5\u7c7b\u578b\u7684\u6570\u636e\u8f6c\u4e3a JSON \u6570\u7ec4\u5b57\u7b26\u4e32\u3002</p> <p>\u5047\u5b9a\u4f60\u6709\u8fd9\u6837\u7684\u7684\u8868\u548c\u6570\u636e</p> <pre><code>--8&lt;-- \"assets/sql/oracle_geom.sql\n</code></pre> <p>\u8bfb\u53d6\u8868\u8be5\u7684\u6570\u636e\u7684\u6700\u540e\u8f93\u51fa\u7ed3\u679c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>--8&lt;-- \"assets/output/oracle_geom_reader.txt\n</code></pre> <p>\u6ce8\u610f\uff1a\u8be5\u6570\u636e\u7c7b\u578b\u76ee\u524d\u8fd8\u5904\u4e8e\u5b9e\u9a8c\u652f\u6301\u9636\u6bb5\uff0c\u4f5c\u8005\u5bf9\u6b64\u6570\u636e\u7c7b\u578b\u7684\u7406\u89e3\u5e76\u4e0d\u6df1\u523b\uff0c\u4e5f\u672a\u7ecf\u8fc7\u5168\u9762\u7684\u6d4b\u8bd5\uff0c\u8bf7\u52ff\u76f4\u63a5\u5728\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u3002</p>"},{"location":"reader/postgresqlreader/","title":"PostgreSQL Reader","text":"<p>PostgreSQL Reader \u63d2\u4ef6\u7528\u4e8e\u4ece PostgreSQL \u8bfb\u53d6\u6570\u636e</p>"},{"location":"reader/postgresqlreader/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u5efa\u8868\u8bed\u53e5\u4ee5\u53ca\u8f93\u5165\u63d2\u5165\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table if not exists addax_tbl\n(\n    c_bigint bigint,\n    c_bit bit(3),\n    c_bool boolean,\n    c_byte bytea,\n    c_char char(10),\n    c_varchar varchar(20),\n    c_date date,\n    c_double float8,\n    c_int integer,\n    c_json json,\n    c_number decimal(8, 3),\n    c_real real,\n    c_small smallint,\n    c_text text,\n    c_ts timestamp,\n    c_uuid uuid,\n    c_xml xml,\n    c_money money,\n    c_inet inet,\n    c_cidr cidr,\n    c_macaddr macaddr\n    );\n\ninsert into addax_tbl\nvalues (999988887777,\n        b'101',\n        TRUE,\n        '\\xDEADBEEF',\n        'hello',\n        'hello, world',\n        '2021-01-04',\n        999888.9972,\n        9876542,\n        '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'::json,\n        12345.123,\n        123.123,\n        126,\n        'this is a long text ',\n        '2020-01-04 12:13:14',\n        'A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'::uuid,\n        '&lt;foo&gt;bar&lt;/foo&gt;'::xml,\n        '52093.89'::money,\n        '192.168.1.1'::inet,\n        '192.168.1/24'::cidr,\n        '08002b:010203'::macaddr);\n</code></pre> <p>\u914d\u7f6e\u4e00\u4e2a\u4ecePostgreSQL\u6570\u636e\u5e93\u540c\u6b65\u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a:</p> job/postgres2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/postgres2stream.json</code></p>"},{"location":"reader/postgresqlreader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/postgres2stream.json\n</code></pre>"},{"location":"reader/postgresqlreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/rdbmsreader/","title":"RDBMS Reader","text":"<p>RDBMS Reader \u63d2\u4ef6\u652f\u6301\u4ece\u4f20\u7edf RDBMS \u8bfb\u53d6\u6570\u636e\u3002\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u5173\u7cfb\u6570\u636e\u5e93\u8bfb\u53d6\u63d2\u4ef6\uff0c\u53ef\u4ee5\u901a\u8fc7\u6ce8\u518c\u6570\u636e\u5e93\u9a71\u52a8\u7b49\u65b9\u5f0f\u652f\u6301\u66f4\u591a\u5173\u7cfb\u6570\u636e\u5e93\u8bfb\u53d6\u3002</p> <p>\u540c\u65f6 RDBMS Reader \u53c8\u662f\u5176\u4ed6\u5173\u7cfb\u578b\u6570\u636e\u5e93\u8bfb\u53d6\u63d2\u4ef6\u7684\u7684\u57fa\u7840\u7c7b\u3002\u4ee5\u4e0b\u8bfb\u53d6\u63d2\u4ef6\u5747\u4f9d\u8d56\u8be5\u63d2\u4ef6</p> <ul> <li>Oracle Reader</li> <li>MySQL Reader</li> <li>PostgreSQL Reader</li> <li>ClickHouse Reader</li> <li>SQLServer Reader</li> <li>Access Reader</li> <li>Databend Reader</li> </ul> <p>\u6ce8\u610f\uff0c \u5982\u679c\u5df2\u7ecf\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u6570\u636e\u5e93\u8bfb\u53d6\u63d2\u4ef6\u7684\uff0c\u63a8\u8350\u4f7f\u7528\u4e13\u7528\u63d2\u4ef6\uff0c\u5982\u679c\u4f60\u9700\u8981\u8bfb\u53d6\u7684\u6570\u636e\u5e93\u6ca1\u6709\u4e13\u95e8\u63d2\u4ef6\uff0c\u5219\u8003\u8651\u4f7f\u7528\u8be5\u901a\u7528\u63d2\u4ef6\u3002 \u5728\u4f7f\u7528\u4e4b\u524d\uff0c\u8fd8\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u624d\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0c\u5426\u5219\u8fd0\u884c\u4f1a\u51fa\u73b0\u5f02\u5e38\u3002</p>"},{"location":"reader/rdbmsreader/#_1","title":"\u914d\u7f6e\u9a71\u52a8","text":"<p>\u5047\u5b9a\u4f60\u9700\u8981\u8bfb\u53d6 IBM DB2 \u7684\u6570\u636e\uff0c\u56e0\u4e3a\u6ca1\u6709\u63d0\u4f9b\u4e13\u95e8\u7684\u8bfb\u53d6\u63d2\u4ef6\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8be5\u63d2\u4ef6\u6765\u5b9e\u73b0\uff0c\u5728\u4f7f\u7528\u4e4b\u524d\uff0c\u9700\u8981\u4e0b\u8f7d\u5bf9\u5e94\u7684 JDBC \u9a71\u52a8\uff0c\u5e76\u62f7\u8d1d\u5230 <code>plugin/reader/rdbmsreader/libs</code> \u76ee\u5f55\u3002 \u5982\u679c\u4f60\u7684\u9a71\u52a8\u7c7b\u540d\u6bd4\u8f83\u7279\u6b8a\uff0c\u5219\u9700\u8981\u5728\u4efb\u52a1\u914d\u7f6e\u6587\u4ef6\u4e2d\u627e\u5230 <code>driver</code> \u4e00\u9879\uff0c\u586b\u5199\u6b63\u786e\u7684 JDBC \u9a71\u52a8\u540d\uff0c\u6bd4\u5982 DB2 \u7684\u9a71\u52a8\u540d\u4e3a <code>com.ibm.db2.jcc.DB2Driver</code>\u3002\u5982\u679c\u4e0d\u586b\u5199\uff0c\u5219\u63d2\u4ef6\u4f1a\u81ea\u52a8\u731c\u6d4b\u9a71\u52a8\u540d\u3002</p> <p>\u4ee5\u4e0b\u5217\u51fa\u5e38\u89c1\u7684\u6570\u636e\u5e93\u4ee5\u53ca\u5bf9\u5e94\u7684\u9a71\u52a8\u540d\u79f0</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"reader/rdbmsreader/#_2","title":"\u914d\u7f6e\u8bf4\u660e","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u5c55\u793a\u4e86\u5982\u4f55\u4ece Presto \u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5230\u7ec8\u7aef</p> job/rdbms2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"rdbmsreader\",\n        \"parameter\": {\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"driver\": \"io.prestosql.jdbc.PrestoDriver\",\n          \"connection\": {\n            \"table\": [\n              \"default.table\"\n            ],\n            \"jdbcUrl\": \"jdbc:presto://127.0.0.1:8080/hive\"\n          },\n          \"fetchSize\": 1024,\n          \"where\": \"1 = 1\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/rdbmsreader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 jdbcUrl \u662f list \u65e0 \u5bf9\u7aef\u6570\u636e\u5e93\u7684JDBC\u8fde\u63a5\u4fe1\u606f\uff0cjdbcUrl\u6309\u7167RDBMS\u5b98\u65b9\u89c4\u8303\uff0c\u5e76\u53ef\u4ee5\u586b\u5199\u8fde\u63a5\u9644\u4ef6\u63a7\u5236\u4fe1\u606f driver \u5426 string \u65e0 \u81ea\u5b9a\u4e49\u9a71\u52a8\u7c7b\u540d\uff0c\u89e3\u51b3\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 table \u662f list \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d,\u4f7f\u7528JSON\u6570\u636e\u683c\u5f0f\uff0c\u5f53\u914d\u7f6e\u4e3a\u591a\u5f20\u8868\u65f6\uff0c\u7528\u6237\u81ea\u5df1\u9700\u4fdd\u8bc1\u591a\u5f20\u8868\u662f\u540c\u4e00\u8868\u7ed3\u6784 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1\u540e splitPk \u5426 string \u65e0 \u4f7f\u7528splitPk\u4ee3\u8868\u7684\u5b57\u6bb5\u8fdb\u884c\u6570\u636e\u5206\u7247\uff0c\u8fd9\u6837\u53ef\u4ee5\u5927\u5927\u63d0\u4f9b\u6570\u636e\u540c\u6b65\u7684\u6548\u80fd\uff0c\u6ce8\u610f\u4e8b\u9879\u89c1\u540e autoPk \u5426 boolean false \u662f\u5426\u81ea\u52a8\u731c\u6d4b\u5206\u7247\u4e3b\u952e\uff0c<code>3.2.6</code> \u7248\u672c\u5f15\u5165\uff0c\u8be6\u89c1\u540e\u9762\u63cf\u8ff0 where \u5426 string \u65e0 \u9488\u5bf9\u8868\u7684\u7b5b\u9009\u6761\u4ef6 session \u662f\u5426 list \u65e0 \u9488\u5bf9\u672c\u5730\u8fde\u63a5,\u4fee\u6539\u4f1a\u8bdd\u914d\u7f6e,\u8be6\u89c1\u4e0b\u6587 querySql \u5426 string \u65e0 \u4f7f\u7528\u81ea\u5b9a\u4e49\u7684SQL\u800c\u4e0d\u662f\u6307\u5b9a\u8868\u6765\u83b7\u53d6\u6570\u636e\uff0c\u4e0e <code>table</code> \u914d\u7f6e\u9879\u4e92\u65a5\u3002\u5f53\u914d\u7f6e\u4e86\u8fd9\u4e00\u9879\u4e4b\u540e\uff0c\u5ffd\u7565 <code>column</code> \u914d\u7f6e\u9879\u5ffd\u7565 fetchSize \u5426 int 1024 \u5b9a\u4e49\u4e86\u63d2\u4ef6\u548c\u6570\u636e\u5e93\u670d\u52a1\u5668\u7aef\u6bcf\u6b21\u6279\u91cf\u6570\u636e\u83b7\u53d6\u6761\u6570\uff0c\u8c03\u9ad8\u8be5\u503c\u53ef\u80fd\u5bfc\u81f4 Addax \u51fa\u73b0OOM excludeColumn \u5426 list \u65e0 \u9700\u8981\u6392\u9664\u7684\u5217\u540d\u5b57\u6bb5\uff0c\u4ec5\u5728 <code>column</code> \u914d\u7f6e\u4e3a <code>*</code> \u65f6\u6709\u6548"},{"location":"reader/rdbmsreader/#jdbcurl","title":"jdbcUrl","text":"<p><code>jdbcUrl</code> \u914d\u7f6e\u9664\u4e86\u914d\u7f6e\u5fc5\u8981\u7684\u4fe1\u606f\u5916\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u5728\u589e\u52a0\u6bcf\u79cd\u7279\u5b9a\u9a71\u52a8\u7684\u7279\u5b9a\u914d\u7f6e\u5c5e\u6027\uff0c\u8fd9\u91cc\u7279\u522b\u63d0\u5230\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u914d\u7f6e\u5c5e\u6027\u5bf9\u4ee3\u7406\u7684\u652f\u6301\u4ece\u800c\u5b9e\u73b0\u901a\u8fc7\u4ee3\u7406\u8bbf\u95ee\u6570\u636e\u5e93\u7684\u529f\u80fd\u3002 \u6bd4\u5982\u5bf9\u4e8e PrestoSQL \u6570\u636e\u5e93\u7684 JDBC \u9a71\u52a8\u800c\u8a00\uff0c\u652f\u6301 <code>socksProxy</code> \u53c2\u6570\uff0c\u4e8e\u662f\u4e0a\u8ff0\u914d\u7f6e\u7684 <code>jdbcUrl</code> \u53ef\u4ee5\u4fee\u6539\u4e3a</p> <p><code>jdbc:presto://127.0.0.1:8080/hive?socksProxy=192.168.1.101:1081</code></p> <p>\u5927\u90e8\u5206\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7684 JDBC \u9a71\u52a8\u652f\u6301 <code>socksProxyHost,socksProxyPort</code> \u53c2\u6570\u6765\u652f\u6301\u4ee3\u7406\u8bbf\u95ee\u3002\u4e5f\u6709\u4e00\u4e9b\u7279\u522b\u7684\u60c5\u51b5\u3002</p> <p>\u4ee5\u4e0b\u662f\u5404\u7c7b\u6570\u636e\u5e93 JDBC \u9a71\u52a8\u6240\u652f\u6301\u7684\u4ee3\u7406\u7c7b\u578b\u4ee5\u53ca\u914d\u7f6e\u65b9\u5f0f</p> \u6570\u636e\u5e93 \u4ee3\u7406\u7c7b\u578b \u4ee3\u7406\u914d\u7f6e \u4f8b\u5b50 MySQL socks socksProxyHost,socksProxyPort <code>socksProxyHost=192.168.1.101&amp;socksProxyPort=1081</code> Presto socks socksProxy <code>socksProxy=192.168.1.101:1081</code> Presto http httpProxy <code>httpProxy=192.168.1.101:3128</code>"},{"location":"reader/rdbmsreader/#driver","title":"driver","text":"<p>\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e00\u4e2a\u6570\u636e\u5e93\u7684JDBC\u9a71\u52a8\u662f\u56fa\u5b9a\u7684\uff0c\u4f46\u6709\u4e9b\u56e0\u4e3a\u7248\u672c\u7684\u4e0d\u540c\uff0c\u6240\u5efa\u8bae\u7684\u9a71\u52a8\u7c7b\u540d\u4e0d\u540c\uff0c\u6bd4\u5982 MySQL\u3002 \u65b0\u7684 MySQL JDBC \u9a71\u52a8\u7c7b\u578b\u63a8\u8350\u4f7f\u7528 <code>com.mysql.cj.jdbc.Driver</code> \u800c\u4e0d\u662f\u4ee5\u524d\u7684 <code>com.mysql.jdbc.Drver</code> \u3002\u5982\u679c\u60f3\u8981\u4f7f\u7528\u5c31\u7684\u9a71\u52a8\u540d\u79f0\uff0c\u5219\u53ef\u4ee5\u914d\u7f6e <code>driver</code> \u914d\u7f6e\u9879\u3002\u5426\u5219\u63d2\u4ef6\u4f1a\u81ea\u52a8\u4f9d\u636e <code>jdbcUrl</code> \u4e2d\u7684\u5b57\u7b26\u4e32\u6765\u731c\u6d4b\u9a71\u52a8\u540d\u79f0.</p>"},{"location":"reader/rdbmsreader/#column","title":"column","text":"<p>\u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u4f7f\u7528JSON\u7684\u6570\u7ec4\u63cf\u8ff0\u5b57\u6bb5\u4fe1\u606f\u3002\u7528\u6237\u4f7f\u7528 <code>*</code> \u4ee3\u8868\u9ed8\u8ba4\u4f7f\u7528\u6240\u6709\u5217\u914d\u7f6e\uff0c\u4f8b\u5982 <code>[\"*\"]</code>\u3002</p> <p>\u652f\u6301\u5217\u88c1\u526a\uff0c\u5373\u5217\u53ef\u4ee5\u6311\u9009\u90e8\u5206\u5217\u8fdb\u884c\u5bfc\u51fa\u3002</p> <p>\u652f\u6301\u5217\u6362\u5e8f\uff0c\u5373\u5217\u53ef\u4ee5\u4e0d\u6309\u7167\u8868schema\u4fe1\u606f\u8fdb\u884c\u5bfc\u51fa\u3002</p> <p>\u652f\u6301\u5e38\u91cf\u914d\u7f6e\uff0c\u7528\u6237\u9700\u8981\u6309\u7167JSON\u683c\u5f0f:</p> <p><code>[\"id\", \"`table`\", \"1\", \"'bazhen.csy'\", \"null\", \"to_char(a + 1)\", \"2.3\" , \"true\"]</code></p> <ul> <li><code>id</code> \u4e3a\u666e\u901a\u5217\u540d</li> <li><code>`table`</code> \u4e3a\u5305\u542b\u4fdd\u7559\u5728\u7684\u5217\u540d\uff0c</li> <li><code>1</code> \u4e3a\u6574\u5f62\u6570\u5b57\u5e38\u91cf\uff0c</li> <li><code>'bazhen.csy'</code>\u4e3a\u5b57\u7b26\u4e32\u5e38\u91cf</li> <li><code>null</code> \u4e3a\u7a7a\u6307\u9488\uff0c\u6ce8\u610f\uff0c\u8fd9\u91cc\u7684 <code>null</code> \u5fc5\u987b\u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u51fa\u73b0\uff0c\u5373\u7528\u53cc\u5f15\u53f7\u5f15\u7528</li> <li><code>to_char(a + 1)</code>\u4e3a\u8868\u8fbe\u5f0f\uff0c</li> <li><code>2.3</code> \u4e3a\u6d6e\u70b9\u6570\uff0c</li> <li><code>true</code> \u4e3a\u5e03\u5c14\u503c\uff0c\u540c\u6837\u7684\uff0c\u8fd9\u91cc\u7684\u5e03\u5c14\u503c\u4e5f\u5fc5\u987b\u7528\u53cc\u5f15\u53f7\u5f15\u7528</li> </ul> <p>Column\u5fc5\u987b\u663e\u793a\u586b\u5199\uff0c\u4e0d\u5141\u8bb8\u4e3a\u7a7a\uff01</p>"},{"location":"reader/rdbmsreader/#excludecolumn","title":"excludeColumn","text":"<p>\u5b58\u5728\u8fd9\u6837\u7684\u4e00\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u9700\u8981\u8bfb\u53d6\u7edd\u5927\u90e8\u5206\u8868\u7684\u5b57\u6bb5\uff0c\u5982\u679c\u8868\u5b57\u6bb5\u7279\u522b\u591a\u7684\u60c5\u51b5\u4e0b\uff0c\u914d\u7f6e <code>column</code> \u663e\u7136\u662f\u4e00\u4ef6\u8017\u65f6\u7684\u4e8b\u60c5\u3002 \u7279\u522b\u7684\uff0c\u4e00\u822c\u6211\u4eec\u628a\u4e1a\u52a1\u6570\u636e\u91c7\u96c6\u5230\u5927\u6570\u636e\u5e73\u53f0\u65f6\uff0c\u4f1a\u589e\u52a0\u4e00\u4e9b\u5305\u62ec\u5206\u533a\u5b57\u6bb5\uff0c\u91c7\u96c6\u4fe1\u606f\u7684\u989d\u5916\u5b57\u6bb5\uff0c\u5f53\u6211\u4eec\u9700\u8981\u56de\u5199\u4e1a\u52a1\u6570\u636e\u8868\u65f6\uff0c\u8fd9\u4e9b\u5b57\u6bb5\u6211\u4eec\u9700\u8981\u6392\u9664\u3002 \u5728\u8fd9\u79cd\u8003\u8651\u4e0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86 <code>excludeColumn</code> \u914d\u7f6e\u9879\uff0c\u5f53 <code>column</code> \u914d\u7f6e\u4e3a <code>*</code> \u65f6\uff0c<code>excludeColumn</code> \u914d\u7f6e\u9879\u751f\u6548\uff0c\u7528\u4e8e\u6392\u9664\u90e8\u5206\u5b57\u6bb5\u3002</p> <p>\u6bd4\u5982:</p> <pre><code>{\n \"column\": [\"*\"],\n  \"excludeColumn\": [\"etl_time\", \"etl_source\", \"dt\"]\n}\n</code></pre>"},{"location":"reader/rdbmsreader/#splitpk","title":"splitPk","text":"<p>\u5982\u679c\u6307\u5b9a <code>splitPk</code>\uff0c\u8868\u793a\u7528\u6237\u5e0c\u671b\u4f7f\u7528 <code>splitPk</code> \u4ee3\u8868\u7684\u5b57\u6bb5\u8fdb\u884c\u6570\u636e\u5206\u7247\uff0c\u56e0\u6b64\u4f1a\u542f\u52a8\u5e76\u53d1\u4efb\u52a1\u8fdb\u884c\u6570\u636e\u540c\u6b65\uff0c\u8fd9\u6837\u53ef\u4ee5\u5927\u5927\u63d0\u4f9b\u6570\u636e\u540c\u6b65\u7684\u6548\u80fd\u3002</p> <p>\u63a8\u8350 <code>splitPk</code> \u7528\u6237\u4f7f\u7528\u8868\u4e3b\u952e\uff0c\u56e0\u4e3a\u8868\u4e3b\u952e\u901a\u5e38\u60c5\u51b5\u4e0b\u6bd4\u8f83\u5747\u5300\uff0c\u56e0\u6b64\u5207\u5206\u51fa\u6765\u7684\u5206\u7247\u4e5f\u4e0d\u5bb9\u6613\u51fa\u73b0\u6570\u636e\u70ed\u70b9\u3002</p> <p>\u76ee\u524d <code>splitPk</code> \u4ec5\u652f\u6301\u6574\u5f62\u3001\u5b57\u7b26\u4e32\u578b\u6570\u636e(ASCII\u7c7b\u578b) \u5207\u5206\uff0c\u4e0d\u652f\u6301\u6d6e\u70b9\u3001\u65e5\u671f\u7b49\u5176\u4ed6\u7c7b\u578b\u3002 \u5982\u679c\u7528\u6237\u6307\u5b9a\u5176\u4ed6\u975e\u652f\u6301\u7c7b\u578b\uff0cRDBMSReader \u5c06\u62a5\u9519\uff01</p> <p><code>splitPk</code> \u5982\u679c\u4e0d\u586b\u5199\uff0c\u5c06\u89c6\u4f5c\u7528\u6237\u4e0d\u5bf9\u5355\u8868\u8fdb\u884c\u5207\u5206\uff0c\u800c\u4f7f\u7528\u5355\u901a\u9053\u540c\u6b65\u5168\u91cf\u6570\u636e\u3002</p>"},{"location":"reader/rdbmsreader/#autopk","title":"autoPk","text":"<p>\u4ece <code>3.2.6</code> \u7248\u672c\u5f00\u59cb\uff0c\u652f\u6301\u81ea\u52a8\u83b7\u53d6\u8868\u4e3b\u952e\u6216\u552f\u4e00\u7d22\u5f15\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3a <code>true</code> \uff0c\u7a0b\u5e8f\u5c06\u731c\u6d4b\u53ef\u7528\u4e8e\u62c6\u5206\u8868\u7684\u5b57\u6bb5\uff0c\u4ed6\u901a\u8fc7\u67e5\u8be2\u6570\u636e\u5e93\u7684\u5143\u6570\u636e\u4fe1\u606f\u83b7\u53d6\u6307\u5b9a\u8868\u5177\u6709\u4e3b\u952e\u3001\u5355\u5b57\u6bb5\u552f\u4e00\u7d22\u5f15\u7d22\u5f15\u7684\u5b57\u6bb5\uff0c \u5982\u679c\u6709\u591a\u4e2a\u5b57\u6bb5\u7b26\u5408\u8981\u6c42\uff0c\u5219\u4f18\u5148\u4f7f\u7528\u6570\u5b57\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u5176\u6b21\u4f7f\u7528\u5b57\u7b26\u7c7b\u578b\u7684\u5b57\u6bb5\u3002\u5982\u679c\u6ca1\u6709\u7b26\u5408\u8981\u6c42\u7684\u5b57\u6bb5\uff0c\u5219\u4e0d\u5207\u5206\u8868\u3002 \u5982\u679c\u914d\u7f6e\u4e86 <code>autoPk</code>\uff0c\u5219\u4efb\u52a1\u6267\u884c\u65f6\uff0c\u6709\u7c7b\u4f3c\u5982\u4e0b\u7684\u65e5\u5fd7\u8f93\u51fa:</p> <pre><code>2025-04-13 23:17:11.036 [       job-0] INFO  CommonRdbmsReader$Job - The split key is not configured, try to guess the split key.\n2025-04-13 23:17:11.059 [       job-0] INFO  CommonRdbmsReader$Job - Take the field id as split key\n</code></pre> <p>\u8be5\u7279\u6027\u76ee\u524d\u652f\u6301\u7684\u6570\u636e\u5e93\u6709\uff1a</p> <ul> <li>ClickHouse</li> <li>MySQL</li> <li>Oracle</li> <li>PostgreSQL</li> <li>SQL Server</li> <li>SQLite</li> <li>Sybase ASE</li> </ul> <p>\u26a0\ufe0f \u6ce8\u610f\uff0c\u5982\u679c\u540c\u65f6\u914d\u7f6e\u4e86 <code>splitPk</code> \u548c <code>autoPk</code> \uff0c\u5219 <code>splitPk</code> \u4f18\u5148\u7ea7\u66f4\u9ad8\uff0c<code>autoPk</code> \u5c06\u88ab\u5ffd\u7565\u3002</p>"},{"location":"reader/rdbmsreader/#session","title":"session","text":"<p>\u63a7\u5236\u5199\u5165\u6570\u636e\u7684\u65f6\u95f4\u683c\u5f0f\uff0c\u65f6\u533a\u7b49\u7684\u914d\u7f6e\uff0c\u76ee\u524d\u4ec5\u5bf9 <code>MySQL</code>, <code>Oracle</code>, <code>SQLServer</code> \u6709\u6548\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u9488\u5bf9 Oracle \u6570\u636e\u5e93\u914d\u7f6e <code>session</code> \u7684\u4f8b\u5b50\u3002</p> <pre><code>{\n  \"session\": [\n    \"alter session set NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'\",\n    \"alter session set NLS_TIMESTAMP_FORMAT='yyyy-mm-dd hh24:mi:ss'\",\n    \"alter session set NLS_TIMESTAMP_TZ_FORMAT='yyyy-mm-dd hh24:mi:ss'\",\n    \"alter session set TIME_ZONE='Asia/Chongqing'\"\n  ]\n}\n</code></pre> <p>\u6ce8\u610f <code>&amp;quot;</code>\u662f <code>\"</code> \u7684\u8f6c\u4e49\u5b57\u7b26\u4e32</p>"},{"location":"reader/rdbmsreader/#querysql","title":"querySql","text":"<p>\u5982\u679c\u914d\u7f6e\u4e86 <code>querySql</code>\uff0c\u5219\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684 SQL \u800c\u4e0d\u662f\u6307\u5b9a\u8868\u6765\u83b7\u53d6\u6570\u636e\u3002\u540c\u65f6\uff0c<code>querySql</code> \u8fd8\u652f\u6301\u4ece\u5916\u90e8\u8bfb\u53d6 SQL \u6587\u4ef6\uff0c\u8fd9\u4e5f\u53ef\u4ee5\u51cf\u5c11\u590d\u6742 SQL \u5728\u5199\u5165 <code>json</code> \u6587\u4ef6\u4e2d\u7684\u96be\u5ea6\u3002 \u5047\u5b9a <code>/tmp/t.sql</code> \u6587\u4ef6\u6709\u5982\u4e0b\u5185\u5bb9:</p> <pre><code>select \n*\nfrom t\nwhere id &lt; 10\n</code></pre> <p>\u90a3\u4e48\u53ef\u4ee5\u914d\u7f6e\u5982\u4e0b:</p> <pre><code>{\n  \"querySql\": [\"@/tmp/t.sql\"]\n}\n</code></pre>"},{"location":"reader/rdbmsreader/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b RDBMS \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext, year,xml Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary"},{"location":"reader/redisreader/","title":"Redis Reader","text":"<p>Redis Reader \u63d2\u4ef6\u7528\u4e8e\u8bfb\u53d6 Redis RDB \u6570\u636e</p>"},{"location":"reader/redisreader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": [\"tcp://127.0.0.1:6379\", \"file:///data/dump.rdb\", \"http://localhost/dump.rdb\"],\n            \"auth\": \"password\"\n          },\n          \"include\": [\n            \"^user\"\n          ],\n          \"exclude\": [\n            \"^password\"\n          ],\n          \"db\": [\n            0,\n            1\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"timeout\": 60000\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/redisreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u9ed8\u8ba4\u503c \u63cf\u8ff0 uri \u662f \u5426 redis\u94fe\u63a5,\u652f\u6301\u591a\u4e2a\u672c\u5730rdb\u6587\u4ef6/\u7f51\u7edcrdb\u6587\u4ef6,\u5982\u679c\u662f\u96c6\u7fa4,\u586b\u5199\u6240\u6709master\u8282\u70b9\u5730\u5740 db \u5426 \u65e0 \u9700\u8981\u8bfb\u53d6\u7684db\u7d22\u5f15,\u82e5\u4e0d\u586b\u5199,\u5219\u8bfb\u53d6\u6240\u6709db include \u5426 \u65e0 \u8981\u5305\u542b\u7684 key, \u652f\u6301\u6b63\u5219\u8868\u8fbe\u5f0f exclude \u5426 \u65e0 \u8981\u6392\u9664\u7684 key,\u652f\u6301\u6b63\u5219\u8868\u8fbe\u5f0f"},{"location":"reader/redisreader/#_3","title":"\u7ea6\u675f\u9650\u5236","text":"<ol> <li>\u4e0d\u652f\u6301\u76f4\u63a5\u8bfb\u53d6\u4efb\u4f55\u4e0d\u652f\u6301 <code>sync</code> \u547d\u4ee4\u7684 redis server\uff0c\u5982\u679c\u9700\u8981\u8bf7\u5907\u4efd\u7684rdb\u6587\u4ef6\u8fdb\u884c\u8bfb\u53d6\u3002</li> <li>\u5982\u679c\u662f\u539f\u751fredis cluster\u96c6\u7fa4\uff0c\u8bf7\u586b\u5199\u6240\u6709master\u8282\u70b9\u7684tcp\u5730\u5740\uff0c<code>redisreader</code> \u63d2\u4ef6\u4f1a\u81ea\u52a8dump \u6240\u6709\u8282\u70b9\u7684rdb\u6587\u4ef6\u3002</li> <li>\u4ec5\u89e3\u6790 <code>String</code> \u6570\u636e\u7c7b\u578b\uff0c\u5176\u4ed6\u590d\u5408\u7c7b\u578b(<code>Sets</code>, <code>List</code> \u7b49\u4f1a\u5ffd\u7565)</li> </ol>"},{"location":"reader/s3reader/","title":"S3 Reader","text":"<p>S3 Reader \u63d2\u4ef6\u7528\u4e8e\u8bfb\u53d6 Amazon AWS S3 \u5b58\u50a8\u4e0a\u7684\u6570\u636e\u3002\u5728\u5b9e\u73b0\u4e0a\uff0c\u672c\u63d2\u4ef6\u57fa\u4e8e S3 \u5b98\u65b9\u7684 SDK 2.0 \u7f16\u5199\u3002</p> <p>\u540c\u65f6\u672c\u63d2\u4ef6\u4e5f\u652f\u6301\u8bfb\u53d6\u517c\u5bb9 S3 \u534f\u8bae\u7684\u5b58\u50a8\u670d\u52a1\uff0c\u6bd4\u5982 MinIO</p>"},{"location":"reader/s3reader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u4ee5\u4e0b\u6837\u4f8b\u914d\u7f6e\u7528\u4e8e\u4ece S3 \u5b58\u50a8\u4e0a\u8bfb\u53d6\u4e24\u4e2a\u6587\u4ef6\uff0c\u5e76\u6253\u5370\u51fa\u6765</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"s3reader\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": [\n            \"1.csv\",\n            \"aa.csv\",\n            \"upload_*.csv\",\n            \"bb_??.csv\"\n          ],\n          \"column\": [\n            \"*\"\n          ],\n          \"region\": \"ap-northeast-1\",\n          \"fileFormat\": \"csv\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/s3reader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 S3 Server\u7684 EndPoint\u5730\u5740\uff0c\u4f8b\u5982 <code>s3.xx.amazonaws.com</code> region \u662f string \u65e0 S3 Server\u7684 Region \u5730\u5740\uff0c\u4f8b\u5982 <code>ap-southeast-1</code> accessId \u662f string \u65e0 \u8bbf\u95ee ID accessKey \u662f string \u65e0 \u8bbf\u95ee Key bucket \u662f string \u65e0 \u8981\u8bfb\u53d6\u7684 bucket object \u662f list \u65e0 \u8981\u8bfb\u53d6\u7684 object\uff0c\u53ef\u4ee5\u586b\u5199\u591a\u4e2a\u4ee5\u53ca\u901a\u914d\u7b26\u65b9\u5f0f\uff0c\u8be6\u89c1\u4e0b\u9762\u8bf4\u660e column \u662f list \u65e0 \u8bfb\u53d6\u7684 object \u7684\u5217\u4fe1\u606f\uff0c\u586b\u5199\u65b9\u5f0f\u89c1RDBMS Reader \u4e2d <code>column</code> \u63cf\u8ff0 fieldDelimiter \u5426 string <code>,</code> \u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26\uff0c\u4ec5\u652f\u6301\u5355\u5b57\u7b26 compress \u5426 string \u65e0 \u6587\u4ef6\u538b\u7f29\u683c\u5f0f\uff0c\u9ed8\u8ba4\u4e0d\u538b\u7f29 encoding \u5426 string <code>utf8</code> \u6587\u4ef6\u7f16\u7801\u683c\u5f0f writeMode \u5426 string <code>nonConflict</code> pathStyleAccessEnabled \u5426 boolean false \u662f\u5426\u542f\u7528\u8def\u5f84\u8bbf\u95ee\u6a21\u5f0f"},{"location":"reader/s3reader/#object","title":"object","text":"<p>\u5f53\u6307\u5b9a\u5355\u4e2a object\uff0c\u63d2\u4ef6\u6682\u65f6\u53ea\u80fd\u4f7f\u7528\u5355\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002</p> <p>\u5f53\u6307\u5b9a\u591a\u4e2a object\uff0c\u63d2\u4ef6\u652f\u6301\u4f7f\u7528\u591a\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u7ebf\u7a0b\u5e76\u53d1\u6570\u901a\u8fc7\u901a\u9053\u6570\u6307\u5b9a\u3002</p> <p>\u5f53\u6307\u5b9a\u901a\u914d\u7b26\uff0c\u63d2\u4ef6\u5c1d\u8bd5\u904d\u5386\u51fa\u591a\u4e2a object \u4fe1\u606f\u3002</p> <p>\u4f8b\u5982: \u6307\u5b9a <code>/*</code> \u4ee3\u8868\u8bfb\u53d6 bucket \u4e0b\u6240\u6709\u7684 object\uff0c\u6307\u5b9a <code>/foo/*</code> \u4ee3\u8868\u8bfb\u53d6 <code>foo</code> \u76ee\u5f55\u4e0b\u6240\u6709\u7684 object\u3002</p>"},{"location":"reader/s3reader/#pathstyleaccessenabled","title":"pathStyleAccessEnabled","text":"<p>\u662f\u5426\u542f\u7528\u8def\u5f84\u8bbf\u95ee\u6a21\u5f0f,\u5982\u679c\u542f\u7528\uff0c\u5219\u8bbf\u95ee bucket \u7684\u8def\u5f84\u4e3a <code>example.com/bucket-name</code>,\u5426\u5219\u4e3a <code>bucket-name.example.com</code> \uff0c\u8be6\u7ec6\u60c5\u51b5\u53ef\u4ee5\u53c2\u89c2 path vs virtual access </p>"},{"location":"reader/s3reader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b S3 \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext, year,xml Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary"},{"location":"reader/s3reader/#_4","title":"\u9650\u5236\u8bf4\u660e","text":"<ol> <li>\u4ec5\u652f\u6301\u8bfb\u53d6\u6587\u672c\u6587\u4ef6</li> </ol>"},{"location":"reader/sqlitereader/","title":"SQLite Reader","text":"<p>SQLite Reader \u63d2\u4ef6\u7528\u4e8e\u8bfb\u53d6\u6307\u5b9a\u76ee\u5f55\u4e0b\u7684 sqlite \u6587\u4ef6\uff0c \u4ed6\u7ee7\u627f\u4e8e RDBMS Reader</p>"},{"location":"reader/sqlitereader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u521b\u5efa\u793a\u4f8b\u6587\u4ef6\uff1a</p> <pre><code>$ sqlite3  /tmp/test.sqlite3\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter \".help\" for instructions\nEnter SQL statements terminated with a \";\"\nsqlite&gt; create table test(id int, name varchar(10), salary double);\nsqlite&gt; insert into test values(1,'foo', 12.13),(2,'bar',202.22);\nsqlite&gt; .q\n</code></pre> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/sqlite2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlitereader\",\n        \"parameter\": {\n          \"username\": \"fakeuser\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite:/tmp/test.sqlite3\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/sqlite2stream.json</code></p>"},{"location":"reader/sqlitereader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/sqlite2stream.json\n</code></pre>"},{"location":"reader/sqlitereader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/sqlserverreader/","title":"SQLServer Reader","text":"<p>SqlServerReader \u63d2\u4ef6\u7528\u4e8e\u4ece\u4ece SQLServer \u8bfb\u53d6\u6570\u636e\u3002</p>"},{"location":"reader/sqlserverreader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ece SQLServer \u6570\u636e\u5e93\u540c\u6b65\u62bd\u53d6\u6570\u636e\u5230\u672c\u5730\u7684\u4f5c\u4e1a:</p> job/sqlserver2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlserverreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://localhost:3433;DatabaseName=dbname\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/sqlserverreader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/streamreader/","title":"Stream Reader","text":"<p>Stream Reader \u662f\u4e00\u4e2a\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\u7684\u63d2\u4ef6\uff0c \u4ed6\u4e3b\u8981\u7528\u6765\u5feb\u901f\u751f\u6210\u671f\u671b\u7684\u6570\u636e\u5e76\u5bf9\u5199\u5165\u63d2\u4ef6\u8fdb\u884c\u6d4b\u8bd5</p> <p>\u4e00\u4e2a\u5b8c\u6574\u7684 StreamReader \u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"reader\": {\n    \"name\": \"streamreader\",\n    \"parameter\": {\n      \"column\": [\n        {\n          \"value\": \"unique_id\",\n          \"type\": \"string\"\n        },\n        {\n          \"value\": \"1989-06-04 08:12:13\",\n          \"type\": \"date\",\n          \"dateFormat\": \"yyyy-MM-dd HH:mm:ss\"\n        },\n        {\n          \"value\": 1984,\n          \"type\": \"long\"\n        },\n        {\n          \"value\": 1989.64,\n          \"type\": \"double\"\n        },\n        {\n          \"value\": true,\n          \"type\": \"bool\"\n        },\n        {\n          \"value\": \"a long text\",\n          \"type\": \"bytes\"\n        }\n      ],\n      \"sliceRecordCount\": 10\n    }\n  }\n}\n</code></pre> <p>\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u5c06\u4f1a\u751f\u6210 10\u6761\u8bb0\u5f55\uff08\u5047\u5b9achannel\u4e3a1\uff09\uff0c\u6bcf\u6761\u8bb0\u5f55\u7684\u5185\u5bb9\u5982\u4e0b\uff1a</p> <p><code>unique_id,'1989-06-04 08:12:13',1984,1989.64,true,'a long text'</code></p> <p>\u76ee\u524d StreamReader \u652f\u6301\u7684\u8f93\u51fa\u6570\u636e\u7c7b\u578b\u5168\u90e8\u5217\u5728\u4e0a\u9762\uff0c\u5206\u522b\u662f\uff1a</p> <ul> <li><code>string</code> \u5b57\u7b26\u7c7b\u578b</li> <li><code>date</code> \u65e5\u671f\u7c7b\u578b</li> <li><code>long</code> \u6240\u6709\u6574\u578b\u7c7b\u578b</li> <li><code>double</code> \u6240\u6709\u6d6e\u70b9\u6570</li> <li><code>bool</code> \u5e03\u5c14\u7c7b\u578b</li> <li><code>bytes</code> \u5b57\u8282\u7c7b\u578b</li> </ul> <p>\u5176\u4e2d <code>date</code> \u7c7b\u578b\u8fd8\u652f\u6301 <code>dateFormat</code> \u914d\u7f6e\uff0c\u7528\u6765\u6307\u5b9a\u8f93\u5165\u7684\u65e5\u671f\u7684\u683c\u5f0f\uff0c\u9ed8\u8ba4\u4e3a <code>yyyy-MM-dd HH:mm:ss</code>\u3002\u6bd4\u5982\u4f60\u7684\u8f93\u5165\u53ef\u4ee5\u8fd9\u6837\uff1a</p> <pre><code>{\n  \"value\": \"1989/06/04 12:13:14\",\n  \"type\": \"date\",\n  \"dateFormat\": \"yyyy/MM/dd HH:mm:ss\"\n}\n</code></pre> <p>\u6ce8\u610f\uff0c\u65e5\u671f\u7c7b\u578b\u4e0d\u7ba1\u8f93\u5165\u662f\u4f55\u79cd\u683c\u5f0f\uff0c\u5185\u90e8\u90fd\u8f6c\u4e3a <code>yyyy-MM-dd HH:mm:ss</code> \u683c\u5f0f\u3002</p> <p>StreamReader \u8fd8\u652f\u6301\u968f\u673a\u8f93\u5165\u529f\u80fd\uff0c\u6bd4\u5982\u6211\u4eec\u8981\u968f\u673a\u5f97\u52300-10\u4e4b\u95f4\u7684\u4efb\u610f\u4e00\u4e2a\u6574\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\u5217\uff1a</p> <pre><code>{\n  \"random\": \"0,10\",\n  \"type\": \"long\"\n}\n</code></pre> <p>\u83b7\u5f97\u4e00\u4e2a 0 \u81f3 100 \u4e4b\u95f4\u7684\u968f\u673a\u6d6e\u70b9\u6570\uff0c\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"random\": \"0,100\",\n  \"type\": \"double\"\n}\n</code></pre> <p>\u5982\u679c\u8981\u6307\u5b9a\u6d6e\u70b9\u6570\u7684\u5c0f\u6570\u4f4d\u6570\uff0c\u6bd4\u5982\u6307\u5b9a\u5c0f\u6570\u4f4d\u4e3a2\u4f4d\uff0c\u5219\u53ef\u4ee5\u8fd9\u6837\u8bbe\u5b9a</p> <pre><code>{\n  \"random\": \"0,100,2\",\n  \"type\": \"double\"\n}\n</code></pre> <p>\u6ce8\u610f\uff1a \u5e76\u4e0d\u80fd\u4fdd\u8bc1\u6bcf\u6b21\u751f\u6210\u7684\u5c0f\u6570\u6070\u597d\u662f2\u4f4d\uff0c\u5982\u679c\u5c0f\u6570\u4e3a\u6570\u4e3a0 \uff0c\u5219\u5c0f\u6570\u4f4d\u6570\u4f1a\u5c11\u4e8e\u6307\u5b9a\u7684\u4f4d\u6570\u3002</p> <p>\u8fd9\u91cc\u4f7f\u7528 <code>random</code> \u8fd9\u4e2a\u5173\u952e\u5b57\u6765\u8868\u793a\u5176\u503c\u4e3a\u968f\u673a\u503c\uff0c\u5176\u503c\u7684\u8303\u56f4\u4e3a\u5de6\u53f3\u95ed\u533a\u95f4\u3002</p> <p>\u5176\u4ed6\u7c7b\u578b\u7684\u968f\u673a\u7c7b\u578b\u914d\u7f6e\u5982\u4e0b\uff1a</p> <ul> <li><code>long</code>: random 0, 10 0\u523010\u4e4b\u95f4\u7684\u968f\u673a\u6570\u5b57</li> <li><code>string</code>: random 0, 10 0\u5230 10 \u957f\u5ea6\u4e4b\u95f4\u7684\u968f\u673a\u5b57\u7b26\u4e32</li> <li><code>bool</code>: random 0, 10 false \u548c true\u51fa\u73b0\u7684\u6bd4\u7387</li> <li><code>double</code>: random 0, 10 0\u523010\u4e4b\u95f4\u7684\u968f\u673a\u6d6e\u70b9\u6570</li> <li><code>double</code>: random 0, 10, 2 0\u523010\u4e4b\u95f4\u7684\u968f\u673a\u6d6e\u70b9\u6570\uff0c\u5c0f\u6570\u4f4d\u4e3a2\u4f4d  </li> <li><code>date</code>: random '2014-07-07 00:00:00', '2016-07-07 00:00:00' \u5f00\u59cb\u65f6\u95f4-&gt;\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u7684\u968f\u673a\u65f6\u95f4\uff0c\u65e5\u671f\u683c\u5f0f\u9ed8\u8ba4(\u4e0d\u652f\u6301\u9017\u53f7)yyyy-MM-dd HH:mm:ss</li> <li><code>BYTES</code>: random 0, 10 0\u523010\u957f\u5ea6\u4e4b\u95f4\u7684\u968f\u673a\u5b57\u7b26\u4e32\u83b7\u53d6\u5176UTF-8\u7f16\u7801\u7684\u4e8c\u8fdb\u5236\u4e32</li> </ul> <p>StreamReader \u8fd8\u652f\u6301\u9012\u589e\u51fd\u6570\uff0c\u6bd4\u5982\u6211\u4eec\u8981\u5f97\u5230\u4e00\u4e2a\u4ece1\u5f00\u59cb\uff0c\u6bcf\u6b21\u52a05\u7684\u7b49\u5dee\u6570\u5217\uff0c\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"incr\": \"1,5\",\n  \"type\": \"long\"\n}\n</code></pre> <p>\u5982\u679c\u9700\u8981\u83b7\u5f97\u4e00\u4e2a\u9012\u51cf\u7684\u6570\u5217\uff0c\u5219\u628a\u7b2c\u4e8c\u4e2a\u53c2\u6570\u7684\u6b65\u957f\uff08\u4e0a\u4f8b\u4e2d\u76845\uff09\u6539\u4e3a\u8d1f\u6570\u5373\u53ef\u3002\u6b65\u957f\u9ed8\u8ba4\u503c\u4e3a1\u3002</p> <p>\u9012\u589e\u8fd8\u652f\u6301\u65e5\u671f\u7c7b\u578b( <code>4.0.1</code> \u7248\u672c\u5f15\u5165)\uff0c\u6bd4\u5982\u4e0b\u9762\u7684\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"incr\": \"1989-06-04 09:01:02,2,d\",\n  \"type\": \"date\"\n}\n</code></pre> <p><code>incr</code> \u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662f\u5f00\u59cb\u65e5\u671f\uff0c\u6b65\u957f\u4ee5\u53ca\u6b65\u957f\u5355\u4f4d\uff0c\u4e2d\u95f4\u7528\u82f1\u6587\u9017\u53f7(,)\u5206\u9694\u3002</p> <ul> <li>\u5f00\u59cb\u65e5\u671f\uff1a\u6b63\u786e\u7684\u65e5\u671f\u5b57\u7b26\u4e32\uff0c\u9ed8\u8ba4\u683c\u5f0f\u4e3a <code>yyyy-MM-dd hh:mm:ss</code>\uff0c\u5982\u679c\u65f6\u95f4\u683c\u5f0f\u4e0d\u540c\uff0c\u5219\u9700\u8981\u914d\u7f6e <code>dateFormat</code> \u6765\u6307\u5b9a\u65e5\u671f\u683c\u5f0f\uff0c\u8fd9\u662f\u5fc5\u586b\u9879</li> <li>\u6b65\u957f\uff1a\u6bcf\u6b21\u9700\u8981\u589e\u52a0\u7684\u957f\u5ea6\uff0c\u9ed8\u8ba4\u4e3a1\uff0c\u5982\u679c\u5e0c\u671b\u662f\u9012\u51cf\uff0c\u5219\u586b\u5199\u8d1f\u6570\uff0c\u8fd9\u662f\u53ef\u9009\u9879</li> <li>\u6b65\u957f\u5355\u4f4d\uff1a\u6309\u4ec0\u4e48\u65f6\u95f4\u5355\u4f4d\u8fdb\u884c\u9012\u589e/\u9012\u51cf\uff0c\u9ed8\u8ba4\u4e3a\u6309\u5929\uff08day\uff09\uff0c\u8fd9\u662f\u53ef\u9009\u9879\uff0c\u53ef\u9009\u7684\u5355\u4f4d\u6709</li> <li>d/day</li> <li>M/month</li> <li>y/year</li> <li>h/hour</li> <li>m/minute</li> <li>s/second</li> <li>w/week</li> </ul> <p>\u914d\u7f6e\u9879 <code>sliceRecordCount</code> \u7528\u6765\u6307\u5b9a\u8981\u751f\u6210\u7684\u6570\u636e\u6761\u6570\uff0c\u5982\u679c\u6307\u5b9a\u7684 <code>channel</code>\uff0c\u5219\u5b9e\u9645\u751f\u6210\u7684\u8bb0\u5f55\u6570\u4e3a <code>sliceRecordCount * channel</code></p>"},{"location":"reader/sybasereader/","title":"Sybase Reader","text":"<p>SybaseReader \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece Sybase \u8bfb\u53d6\u6570\u636e</p>"},{"location":"reader/sybasereader/#_1","title":"\u793a\u4f8b","text":"<p>\u6211\u4eec\u53ef\u4ee5\u7528 Docker \u5bb9\u5668\u6765\u542f\u52a8\u4e00\u4e2a Sybase \u6570\u636e\u5e93</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>\u4e0b\u9762\u7684\u914d\u7f6e\u662f\u8bfb\u53d6\u8be5\u8868\u5230\u7ec8\u7aef\u7684\u4f5c\u4e1a:</p> job/sybasereader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sybasereader\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.ijdbc_function_escapes\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/sybase2stream.json</code></p>"},{"location":"reader/sybasereader/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/sybase2stream.json\n</code></pre>"},{"location":"reader/sybasereader/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Reader \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Reader \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"reader/tdenginereader/","title":"TDengine Reader","text":"<p>TDengine Reader \u63d2\u4ef6\u7528\u4e8e\u4ece\u6d9b\u601d\u516c\u53f8\u7684 TDengine \u8bfb\u53d6\u6570\u636e\u3002</p>"},{"location":"reader/tdenginereader/#_1","title":"\u524d\u7f6e\u6761\u4ef6","text":"<p>\u8003\u8651\u5230\u6027\u80fd\u95ee\u9898\uff0c\u8be5\u63d2\u4ef6\u4f7f\u7528\u4e86 TDengine \u7684 JDBC-JNI \u9a71\u52a8\uff0c \u8be5\u9a71\u52a8\u76f4\u63a5\u8c03\u7528\u5ba2\u6237\u7aef API\uff08<code>libtaos.so</code> \u6216 <code>taos.dll</code>\uff09\u5c06\u5199\u5165\u548c\u67e5\u8be2\u8bf7\u6c42\u53d1\u9001\u5230 taosd \u5b9e\u4f8b\u3002\u56e0\u6b64\u5728\u4f7f\u7528\u4e4b\u524d\u9700\u8981\u914d\u7f6e\u597d\u52a8\u6001\u5e93\u94fe\u63a5\u6587\u4ef6\u3002</p> <p>\u9996\u5148\u5c06 <code>plugin/reader/tdenginereader/libs/libtaos.so.2.0.16.0</code> \u62f7\u8d1d\u5230 <code>/usr/lib64</code> \u76ee\u5f55\uff0c\u7136\u540e\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u8f6f\u94fe\u63a5</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"reader/tdenginereader/#_2","title":"\u793a\u4f8b","text":"<p>TDengine \u6570\u636e\u81ea\u5e26\u4e86\u4e00\u4e2a\u6f14\u793a\u6570\u636e\u5e93 taosdemo , \u6211\u4eec\u4ece\u6f14\u793a\u6570\u636e\u5e93\u8bfb\u53d6\u90e8\u5206\u6570\u636e\u5e76\u6253\u5370\u5230\u7ec8\u7aef</p> <p>\u4ee5\u4e0b\u662f\u914d\u7f6e\u6587\u4ef6</p> job/tdengine2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"tdenginereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"taosdata\",\n          \"beginDateTime\": \"2017-07-14 10:40:00\",\n          \"endDateTime\": \"2017-08-14 10:40:00\",\n          \"splitInterval\": \"1d\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:TAOS://127.0.0.1:6030/test\",\n            \"querySql\": [\n              \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 10\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/tdengine2stream.json</code></p>"},{"location":"reader/tdenginereader/#_3","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/tdengine2stream.json\n</code></pre> <p>\u547d\u4ee4\u8f93\u51fa\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>2021-02-20 15:32:23.161 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-02-20 15:32:23.229 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"password\":\"*****\",\n                    \"connection\":[\n                        {\n                            \"querySql\":[\n                                \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n                            ],\n                            \"jdbcUrl\":[\n                                \"jdbc:TAOS://127.0.0.1:6030/test\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"root\"\n                },\n                \"name\":\"tdenginereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n2021-02-20 15:32:23.277 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-20 15:32:23.278 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-20 15:32:23.281 [main] INFO  JobContainer - Set jobId = 0\njava.library.path:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n....\n2021-02-20 15:32:23.687 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Begin to read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n2021-02-20 15:32:23.692 [0-0-0-reader] WARN  DBUtil - current database does not supoort TYPE_FORWARD_ONLY/CONCUR_READ_ONLY\n2021-02-20 15:32:23.740 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Finished read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n\n1500000001000   5   5   0   1   beijing\n1500000001000   0   6   2   1   beijing\n1500000001000   7   0   0   1   beijing\n1500000001000   8   9   6   1   beijing\n1500000001000   9   9   1   1   beijing\n1500000001000   8   2   0   1   beijing\n1500000001000   4   5   5   3   beijing\n1500000001000   3   3   3   3   beijing\n1500000001000   5   4   8   3   beijing\n1500000001000   9   4   6   3   beijing\n\n2021-02-20 15:32:26.689 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-20 15:32:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-20 15:32:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              800B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/tdenginereader/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 jdbcUrl \u662f list \u65e0 \u5bf9\u7aef\u6570\u636e\u5e93\u7684JDBC\u8fde\u63a5\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684 <code>TAOS</code> \u5fc5\u987b\u5927\u5199 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 table \u662f list \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d,\u4f7f\u7528JSON\u6570\u636e\u683c\u5f0f\uff0c\u5f53\u914d\u7f6e\u4e3a\u591a\u5f20\u8868\u65f6\uff0c\u7528\u6237\u81ea\u5df1\u9700\u4fdd\u8bc1\u591a\u5f20\u8868\u662f\u540c\u4e00\u8868\u7ed3\u6784 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0rdbmreader where \u5426 string \u65e0 \u9488\u5bf9\u8868\u7684\u7b5b\u9009\u6761\u4ef6 querySql \u5426 list \u65e0 \u4f7f\u7528\u81ea\u5b9a\u4e49\u7684SQL\u800c\u4e0d\u662f\u6307\u5b9a\u8868\u6765\u83b7\u53d6\u6570\u636e\uff0c\u5f53\u914d\u7f6e\u4e86\u8fd9\u4e00\u9879\u4e4b\u540e\uff0cAddax\u7cfb\u7edf\u5c31\u4f1a\u5ffd\u7565 <code>table</code>\uff0c<code>column</code>\u8fd9\u4e9b\u914d\u7f6e\u9879 beginDateTime \u662f string \u65e0 \u6570\u636e\u7684\u5f00\u59cb\u65f6\u95f4\uff0cJob\u8fc1\u79fb\u4ece <code>begineDateTime</code> \u5230 <code>endDateTime</code> \u7684\u6570\u636e\uff0c\u683c\u5f0f\u4e3a <code>yyyy-MM-dd HH:mm:ss</code> endDateTime \u662f string \u65e0 \u6570\u636e\u7684\u7ed3\u675f\u65f6\u95f4\uff0cJob\u8fc1\u79fb\u4ece <code>begineDateTime</code> \u5230 <code>endDateTime</code> \u7684\u6570\u636e\uff0c\u683c\u5f0f\u4e3a <code>yyyy-MM-dd HH:mm:ss</code> splitInterval \u662f string \u65e0 \u6309\u7167 <code>splitInterval</code> \u6765\u5212\u5206 <code>task</code>, \u6bcf <code>splitInterval</code> \u521b\u5efa\u4e00\u4e2a <code>task</code>"},{"location":"reader/tdenginereader/#splitinterval","title":"splitInterval","text":"<p>\u7528\u6765\u5212\u5206 <code>task</code>\u3002 \u4f8b\u5982\uff0c<code>20d</code> \u4ee3\u8868\u6309\u7167\u6bcf 20 \u5929\u7684\u6570\u636e\u5212\u5206\u4e3a 1 \u4e2a <code>task</code>\u3002 \u53ef\u4ee5\u914d\u7f6e\u7684\u65f6\u95f4</p> <ul> <li><code>d</code>\uff08\u5929\uff09</li> <li><code>h</code>\uff08\u5c0f\u65f6\uff09</li> <li><code>m</code>\uff08\u5206\u949f\uff09</li> <li><code>s</code>\uff08\u79d2\uff09</li> </ul>"},{"location":"reader/tdenginereader/#jdbc-restful","title":"\u4f7f\u7528 JDBC-RESTful \u63a5\u53e3","text":"<p>\u5982\u679c\u4e0d\u60f3\u4f9d\u8d56\u672c\u5730\u5e93\uff0c\u6216\u8005\u6ca1\u6709\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 <code>JDBC-RESTful</code> \u63a5\u53e3\u6765\u5199\u5165\u8868\uff0c\u76f8\u6bd4 JDBC-JNI \u800c\u8a00\uff0c\u914d\u7f6e\u533a\u522b\u662f\uff1a</p> <ul> <li>driverClass \u6307\u5b9a\u4e3a <code>com.taosdata.jdbc.rs.RestfulDriver</code></li> <li>jdbcUrl \u4ee5 <code>jdbc:TAOS-RS://</code> \u5f00\u5934\uff1b</li> <li>\u4f7f\u7528 <code>6041</code> \u4f5c\u4e3a\u8fde\u63a5\u7aef\u53e3</li> </ul> <p>\u6240\u4ee5\u4e0a\u8ff0\u914d\u7f6e\u4e2d\u7684 <code>connection</code> \u5e94\u8be5\u4fee\u6539\u4e3a\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"connection\": [\n    {\n      \"querySql\": [\n        \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n      ],\n      \"jdbcUrl\": [\n        \"jdbc:TAOS-RS://127.0.0.1:6041/test\"\n      ],\n      \"driver\": \"com.taosdata.jdbc.rs.RestfulDriver\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reader/tdenginereader/#_5","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b TDengine \u6570\u636e\u7c7b\u578b Long SMALLINT, TINYINT, INT, BIGINT, TIMESTAMP Double FLOAT, DOUBLE String BINARY, NCHAR Boolean BOOL"},{"location":"reader/tdenginereader/#_6","title":"\u5f53\u524d\u652f\u6301\u7248\u672c","text":"<p>TDengine 2.0.16</p>"},{"location":"reader/tdenginereader/#_7","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>TDengine JDBC-JNI \u9a71\u52a8\u548c\u52a8\u6001\u5e93\u7248\u672c\u8981\u6c42\u4e00\u4e00\u5339\u914d\uff0c\u56e0\u6b64\u5982\u679c\u4f60\u7684\u6570\u636e\u7248\u672c\u5e76\u4e0d\u662f <code>2.0.16</code>\uff0c\u5219\u9700\u8981\u540c\u65f6\u66ff\u6362\u52a8\u6001\u5e93\u548c\u63d2\u4ef6\u76ee\u5f55\u4e2d\u7684JDBC\u9a71\u52a8</li> </ul>"},{"location":"reader/txtfilereader/","title":"TxtFile Reader","text":"<p>TxtFile Reader \u63d0\u4f9b\u4e86\u8bfb\u53d6\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u6570\u636e\u5b58\u50a8\u7684\u80fd\u529b\u3002</p>"},{"location":"reader/txtfilereader/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"job/txtfile2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/data\"\n          ],\n          \"encoding\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/result\",\n          \"fileName\": \"txt_\",\n          \"writeMode\": \"truncate\",\n          \"format\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/txtfilereader/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f list \u65e0 \u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84,\u8be6\u7ec6\u63cf\u8ff0\u89c1\u4e0b\u6587 column \u662f <code>list&lt;map&gt;\\|*</code> \u65e0 \u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype\u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0c\u8be6\u89c1\u4e0b\u6587 fieldDelimiter \u662f string <code>,</code> \u63cf\u8ff0\uff1a\u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26 encoding \u5426 string utf-8 \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e skipHeader \u5426 boolean false \u7c7bCSV\u683c\u5f0f\u6587\u4ef6\u53ef\u80fd\u5b58\u5728\u8868\u5934\u4e3a\u6807\u9898\u60c5\u51b5\uff0c\u9700\u8981\u8df3\u8fc7\u3002\u9ed8\u8ba4\u4e0d\u8df3\u8fc7 csvReaderConfig \u5426 string \u65e0 \u8bfb\u53d6CSV\u7c7b\u578b\u6587\u4ef6\u53c2\u6570\u914d\u7f6e\uff0cMap\u7c7b\u578b\u3002\u4e0d\u914d\u7f6e\u5219\u4f7f\u7528\u9ed8\u8ba4\u503c,\u8be6\u89c1\u4e0b\u6587"},{"location":"reader/txtfilereader/#path","title":"path","text":"<p>\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u6ce8\u610f\u8fd9\u91cc\u53ef\u4ee5\u652f\u6301\u586b\u5199\u591a\u4e2a\u8def\u5f84\u3002</p> <ul> <li>\u5f53\u6307\u5b9a\u5355\u4e2a\u672c\u5730\u6587\u4ef6\uff0cTxtFileReader\u6682\u65f6\u53ea\u80fd\u4f7f\u7528\u5355\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u4e8c\u671f\u8003\u8651\u5728\u975e\u538b\u7f29\u6587\u4ef6\u60c5\u51b5\u4e0b\u9488\u5bf9\u5355\u4e2aFile\u53ef\u4ee5\u8fdb\u884c\u591a\u7ebf\u7a0b\u5e76\u53d1\u8bfb\u53d6</li> <li>\u5f53\u6307\u5b9a\u591a\u4e2a\u672c\u5730\u6587\u4ef6\uff0cTxtFileReader\u652f\u6301\u4f7f\u7528\u591a\u7ebf\u7a0b\u8fdb\u884c\u6570\u636e\u62bd\u53d6\u3002\u7ebf\u7a0b\u5e76\u53d1\u6570\u901a\u8fc7\u901a\u9053\u6570\u6307\u5b9a</li> <li>\u5f53\u6307\u5b9a\u901a\u914d\u7b26\uff0cTxtFileReader\u5c1d\u8bd5\u904d\u5386\u51fa\u591a\u4e2a\u6587\u4ef6\u4fe1\u606f\u3002\u4f8b\u5982: \u6307\u5b9a <code>/*</code>\u4ee3\u8868\u8bfb\u53d6 <code>/</code> \u76ee\u5f55\u4e0b\u6240\u6709\u7684\u6587\u4ef6\uff0c\u6307\u5b9a <code>/bazhen/*</code> \u4ee3\u8868\u8bfb\u53d6 <code>bazhen</code> \u76ee\u5f55\u4e0b\u6e38\u6240\u6709\u7684\u6587\u4ef6\u3002\u76ee\u524d\u53ea\u652f\u6301 <code>*</code> \u4f5c\u4e3a\u6587\u4ef6\u901a\u914d\u7b26\u3002</li> </ul> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cAddax\u4f1a\u5c06\u4e00\u4e2a\u4f5c\u4e1a\u4e0b\u540c\u6b65\u7684\u6240\u6709Text File\u89c6\u4f5c\u540c\u4e00\u5f20\u6570\u636e\u8868\u3002\u7528\u6237\u5fc5\u987b\u81ea\u5df1\u4fdd\u8bc1\u6240\u6709\u7684File\u80fd\u591f\u9002\u914d\u540c\u4e00\u5957schema\u4fe1\u606f\u3002\u8bfb\u53d6\u6587\u4ef6\u7528\u6237\u5fc5\u987b\u4fdd\u8bc1\u4e3a\u7c7bCSV\u683c\u5f0f\uff0c\u5e76\u4e14\u63d0\u4f9b\u7ed9Addax\u6743\u9650\u53ef\u8bfb\u3002</p> <p>\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5982\u679cPath\u6307\u5b9a\u7684\u8def\u5f84\u4e0b\u6ca1\u6709\u7b26\u5408\u5339\u914d\u7684\u6587\u4ef6\u62bd\u53d6\uff0cAddax\u5c06\u62a5\u9519\u3002</p> <p>\u4ece 3.2.3 \u7248\u672c\u8d77\uff0c <code>path</code> \u4e0b\u5141\u8bb8\u6df7\u5408\u4e0d\u540c\u538b\u7f29\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u63d2\u4ef6\u4f1a\u5c1d\u8bd5\u81ea\u52a8\u731c\u6d4b\u538b\u7f29\u683c\u5f0f\u5e76\u81ea\u52a8\u89e3\u538b\uff0c\u76ee\u524d\u652f\u6301\u7684\u538b\u7f29\u683c\u5f0f\u6709\uff1a</p> <ul> <li>zip</li> <li>bzip2</li> <li>gzip</li> <li>LZ4</li> <li>PACK200</li> <li>XZ</li> <li>Compress</li> </ul>"},{"location":"reader/txtfilereader/#column","title":"column","text":"<p>\u8bfb\u53d6\u5b57\u6bb5\u5217\u8868\uff0ctype\u6307\u5b9a\u6e90\u6570\u636e\u7684\u7c7b\u578b\uff0cindex\u6307\u5b9a\u5f53\u524d\u5217\u6765\u81ea\u4e8e\u6587\u672c\u7b2c\u51e0\u5217(\u4ee50\u5f00\u59cb)\uff0cvalue\u6307\u5b9a\u5f53\u524d\u7c7b\u578b\u4e3a\u5e38\u91cf\uff0c\u4e0d\u4ece\u6e90\u5934\u6587\u4ef6\u8bfb\u53d6\u6570\u636e\uff0c\u800c\u662f\u6839\u636evalue\u503c\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u7684\u5217\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7528\u6237\u53ef\u4ee5\u5168\u90e8\u6309\u7167String\u7c7b\u578b\u8bfb\u53d6\u6570\u636e\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    \"*\"\n  ]\n}\n</code></pre> <p>\u7528\u6237\u53ef\u4ee5\u6307\u5b9aColumn\u5b57\u6bb5\u4fe1\u606f\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"alibaba\"\n  }\n]\n</code></pre> <p>\u5bf9\u4e8e\u7528\u6237\u6307\u5b9aColumn\u4fe1\u606f\uff0ctype\u5fc5\u987b\u586b\u5199\uff0cindex/value\u5fc5\u987b\u9009\u62e9\u5176\u4e00\u3002</p> <p>\u4ece <code>4.0.1</code> \u5f00\u59cb\uff0c\u8868\u793a\u5b57\u6bb5\u9664\u4e86\u4f7f\u7528 <code>index</code> \u6765\u6307\u5b9a\u5b57\u6bb5\u7684\u987a\u5e8f\u5916\uff0c\u8fd8\u652f\u6301 <code>name</code> \u65b9\u5f0f\uff0c\u8fd9\u9700\u8981\u6240\u8bfb\u53d6\u7684\u6587\u4ef6\u7684\u90fd\u5305\u542b\u4e86\u6587\u4ef6\u5934\uff0c\u63d2\u4ef6\u4f1a\u5c1d\u8bd5\u5c06\u6307\u5b9a\u7684 <code>name</code> \u53bb\u5339\u914d\u4ece\u6587\u4ef6\u8bfb\u53d6\u7684\u6587\u4ef6\u5934\uff0c \u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684 <code>index</code> \u503c\uff0c\u5e76\u56de\u5199\u5230 \u914d\u7f6e\u6587\u4ef6\u4e2d\u3002\u540c\u65f6\uff0c<code>index</code> \u548c <code>name</code> \u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u5217\u4e0a\u8fdb\u884c\u6df7\u5408\u4f7f\u7528\uff0c\u6bd4\u5982\u4e0b\u9762\u8fd9\u6837\uff1a</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0\n  },\n  {\n    \"name\": \"region\",\n    \"type\": \"string\"\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"alibaba\"\n  }\n]\n</code></pre> <p>\u6ce8\uff1a \u8fd9\u79cd\u65b9\u5f0f\u4ee5\u4e3a\u8fd9\u5728\u51c6\u5907\u9636\u6bb5\u5c31\u8981\u5c1d\u8bd5\u8bfb\u53d6\u6587\u4ef6\uff0c\u56e0\u4e3a\u4f1a\u6709\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\uff0c\u5982\u975e\u5fc5\u8981\uff0c\u4e0d\u5efa\u8bae\u914d\u7f6e <code>name</code> \u65b9\u5f0f\u3002</p>"},{"location":"reader/txtfilereader/#csvreaderconfig","title":"csvReaderConfig","text":"<p>\u8bfb\u53d6CSV\u7c7b\u578b\u6587\u4ef6\u53c2\u6570\u914d\u7f6e\uff0cMap\u7c7b\u578b\u3002\u8bfb\u53d6CSV\u7c7b\u578b\u6587\u4ef6\u4f7f\u7528\u7684CsvReader\u8fdb\u884c\u8bfb\u53d6\uff0c\u4f1a\u6709\u5f88\u591a\u914d\u7f6e\uff0c\u4e0d\u914d\u7f6e\u5219\u4f7f\u7528\u9ed8\u8ba4\u503c\u3002</p> <p>\u5e38\u89c1\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"csvReaderConfig\": {\n    \"safetySwitch\": false,\n    \"skipEmptyRecords\": false,\n    \"useTextQualifier\": false\n  }\n}\n</code></pre> <p>\u6240\u6709\u914d\u7f6e\u9879\u53ca\u9ed8\u8ba4\u503c,\u914d\u7f6e\u65f6 csvReaderConfig \u7684map\u4e2d\u8bf7**\u4e25\u683c\u6309\u7167\u4ee5\u4e0b\u5b57\u6bb5\u540d\u5b57\u8fdb\u884c\u914d\u7f6e**\uff1a</p> <pre><code>boolean caseSensitive = true;\nchar textQualifier = 34;\nboolean trimWhitespace = true;\nboolean useTextQualifier = true;//\u662f\u5426\u4f7f\u7528csv\u8f6c\u4e49\u5b57\u7b26\nchar delimiter = 44;//\u5206\u9694\u7b26\nchar recordDelimiter = 0;\nchar comment = 35;\nboolean useComments = false;\nint escapeMode = 1;\nboolean safetySwitch = true;//\u5355\u5217\u957f\u5ea6\u662f\u5426\u9650\u5236100000\u5b57\u7b26\nboolean skipEmptyRecords = true;//\u662f\u5426\u8df3\u8fc7\u7a7a\u884c\nboolean captureRawRecord = true;\n</code></pre>"},{"location":"reader/txtfilereader/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b \u672c\u5730\u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long Double Double String String Boolean Boolean Date Date"},{"location":"writer/accesswriter/","title":"Access Writer","text":"<p>Access Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 Access \u76ee\u7684\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/accesswriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684 Access \u8868\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\n</code></pre> <p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 Access \u5bfc\u5165\u7684\u6570\u636e\u3002</p> job/stream2access.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"accesswriter\",\n        \"parameter\": {\n          \"username\": \"wgzhao\",\n          \"password\": \"\",\n          \"column\": [\n            \"name\",\n            \"file_size\",\n            \"file_date\",\n            \"file_open\",\n            \"memo\"\n          ],\n          \"ddl\": \"create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:ucanaccess:////Users/wgzhao/Downloads/AccessThemeDemo.mdb\",\n            \"table\": [\n              \"tbl_test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2access.json</code></p>"},{"location":"writer/accesswriter/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2access.json\n</code></pre>"},{"location":"writer/accesswriter/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/accesswriter/#_4","title":"\u53d8\u66f4\u8bb0\u5f55","text":"<ol> <li>\u4ece <code>5.0.1</code> \u7248\u672c\u5176\uff0c\u5f53\u8981\u5199\u5165\u7684 Access \u6570\u636e\u5e93\u6587\u4ef6\u4e0d\u5b58\u5728\u65f6\uff0c\u4f1a\u81ea\u52a8\u521b\u5efa\uff0c\u5e76\u8bbe\u7f6e\u6570\u636e\u5e93\u683c\u5f0f\u4e3a <code>Access 2016</code></li> </ol>"},{"location":"writer/cassandrawriter/","title":"Cassandra Writer","text":"<p>Cassandra Writer \u63d2\u4ef6\u7528\u4e8e\u5411 Cassandra \u5199\u5165\u6570\u636e\u3002</p>"},{"location":"writer/cassandrawriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ece\u5185\u5b58\u4ea7\u751f\u5230 Cassandra \u5bfc\u5165\u7684\u4f5c\u4e1a:</p> jobs/stream2cassandra.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"name\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"false\",\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": \"addr\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": 1.234,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 12345678,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 2.345,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 3456789,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"value\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10000000\n        }\n      },\n      \"writer\": {\n        \"name\": \"cassandrawriter\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"stresscql\",\n          \"table\": \"dst\",\n          \"batchSize\": 10,\n          \"column\": [\n            \"name\",\n            \"choice\",\n            \"date\",\n            \"address\",\n            \"dbl\",\n            \"lval\",\n            \"fval\",\n            \"ival\",\n            \"uid\",\n            \"value\",\n            \"listval\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/cassandrawriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 host \u662f string \u65e0 \u8fde\u63a5\u70b9\u7684\u57df\u540d\u6216 ip\uff0c\u591a\u4e2a node \u4e4b\u95f4\u7528\u9017\u53f7\u5206\u9694 port \u662f int 9042 Cassandra \u7aef\u53e3 username \u5426 string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 useSSL \u5426 boolean false \u662f\u5426\u4f7f\u7528 SSL \u8fde\u63a5 connectionsPerHost \u5426 int 8 \u5ba2\u6237\u7aef\u8fde\u63a5\u6c60\u914d\u7f6e\uff1a\u4e0e\u670d\u52a1\u5668\u6bcf\u4e2a\u8282\u70b9\u5efa\u591a\u5c11\u4e2a\u8fde\u63a5 maxPendingPerConnection \u5426 int 128 \u5ba2\u6237\u7aef\u8fde\u63a5\u6c60\u914d\u7f6e\uff1a\u6bcf\u4e2a\u8fde\u63a5\u6700\u5927\u8bf7\u6c42\u6570 keyspace \u662f string \u65e0 \u9700\u8981\u540c\u6b65\u7684\u8868\u6240\u5728\u7684 keyspace table \u662f string \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u96c6\u5408 consistancyLevel \u5426 string <code>LOCAL_QUORUM</code> \u6570\u636e\u4e00\u81f4\u6027\u7ea7\u522b, batchSize \u5426 int 1 \u4e00\u6b21\u6279\u91cf\u63d0\u4ea4(UNLOGGED BATCH)\u7684\u8bb0\u5f55\u6570\u5927\u5c0f\uff08\u6761\u6570\uff09"},{"location":"writer/cassandrawriter/#column","title":"column","text":"<p>\u5185\u5bb9\u53ef\u4ee5\u662f\u5217\u7684\u540d\u79f0\u6216 <code>writetime()</code>\u3002\u5982\u679c\u5c06\u5217\u540d\u914d\u7f6e\u4e3a <code>writetime()</code>\uff0c\u4f1a\u5c06\u8fd9\u4e00\u5217\u7684\u5185\u5bb9\u4f5c\u4e3a\u65f6\u95f4\u6233</p>"},{"location":"writer/cassandrawriter/#consistancylevel","title":"consistancyLevel","text":"<p>\u53ef\u9009 <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code></p>"},{"location":"writer/cassandrawriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b Cassandra \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint,varint,bigint,time Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob <p>\u8bf7\u6ce8\u610f:</p> <p>\u76ee\u524d\u4e0d\u652f\u6301 <code>counter</code> \u7c7b\u578b\u548c <code>custom</code> \u7c7b\u578b\u3002</p>"},{"location":"writer/cassandrawriter/#_4","title":"\u7ea6\u675f\u9650\u5236","text":""},{"location":"writer/cassandrawriter/#batchsize","title":"batchSize","text":"<ol> <li>\u4e0d\u80fd\u8d85\u8fc7 65535</li> <li>batch \u4e2d\u7684\u5185\u5bb9\u5927\u5c0f\u53d7\u5230\u670d\u52a1\u5668\u7aef <code>batch_size_fail_threshold_in_kb</code> \u7684\u9650\u5236\u3002</li> <li>\u5982\u679c batch \u4e2d\u7684\u5185\u5bb9\u8d85\u8fc7\u4e86 <code>batch_size_warn_threshold_in_kb</code> \u7684\u9650\u5236\uff0c\u4f1a\u6253\u51fa warn \u65e5\u5fd7\uff0c\u4f46\u5e76\u4e0d\u5f71\u54cd\u5199\u5165\uff0c\u5ffd\u7565\u5373\u53ef\u3002</li> <li>\u5982\u679c\u6279\u91cf\u63d0\u4ea4\u5931\u8d25\uff0c\u4f1a\u628a\u8fd9\u4e2a\u6279\u91cf\u7684\u6240\u6709\u5185\u5bb9\u91cd\u65b0\u9010\u6761\u5199\u5165\u4e00\u904d\u3002</li> </ol>"},{"location":"writer/clickhousewriter/","title":"ClickHouse Writer","text":"<p>ClickHouse Writer \u63d2\u4ef6\u7528\u4e8e\u5411 ClickHouse \u5199\u5165\u6570\u636e\u3002 </p>"},{"location":"writer/clickhousewriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u6211\u4eec\u6f14\u793a\u4ece clickhouse \u4e2d\u8bfb\u53d6\u4e00\u5f20\u8868\u7684\u5185\u5bb9\uff0c\u5e76\u5199\u5165\u5230\u76f8\u540c\u8868\u7ed3\u6784\u7684\u53e6\u5916\u4e00\u5f20\u8868\u4e2d\uff0c\u7528\u6765\u6d4b\u8bd5\u63d2\u4ef6\u6240\u652f\u6301\u7684\u6570\u636e\u7ed3\u6784</p>"},{"location":"writer/clickhousewriter/#_2","title":"\u8868\u7ed3\u6784\u4ee5\u6570\u636e","text":"<p>\u5047\u5b9a\u8981\u8bfb\u53d6\u7684\u8868\u7ed3\u6784\u53ca\u6570\u636e\u5982\u4e0b\uff1a</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre> <p>\u8981\u5199\u5165\u7684\u8868\u91c7\u53d6\u548c\u8bfb\u53d6\u8868\u7ed3\u6784\u76f8\u540c\uff0c\u5176\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table ck_addax_writer as ck_addax;\n</code></pre>"},{"location":"writer/clickhousewriter/#_3","title":"\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u4e3a\u914d\u7f6e\u6587\u4ef6</p> job/clickhouse2clickhouse.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"clickhousewriter\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          },\n          \"preSql\": [\n            \"alter table @table delete where 1=1\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/\",\n            \"table\": [\n              \"ck_addax\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/clickhouse2clickhouse.json</code></p>"},{"location":"writer/clickhousewriter/#_4","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/clickhouse2clickhouse.json\n</code></pre>"},{"location":"writer/clickhousewriter/#_5","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/databendwriter/","title":"DatabendWriter","text":"<p>Databend \u63d2\u4ef6\u7528\u4e8e\u5411 Databend \u6570\u636e\u5e93\u4ee5 JDBC \u65b9\u5f0f\u5199\u5165\u6570\u636e\u3002 </p> <p>Databend \u662f\u4e00\u4e2a\u517c\u5bb9 MySQL \u534f\u8bae\u7684\u6570\u636e\u5e93\u540e\u7aef\uff0c\u56e0\u6b64 Databend \u5199\u5165\u53ef\u4ee5\u4f7f\u7528 MySQLWriter \u8fdb\u884c\u8bbf\u95ee\u3002</p>"},{"location":"writer/databendwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684\u8868\u7684\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>CREATE\nDATABASE example_db;\nCREATE TABLE `example_db`.`table1`\n(\n    `siteid`   INT DEFAULT CAST(10 AS INT),\n    `citycode` INT,\n    `username` VARCHAR,\n    `pv`       BIGINT\n);\n</code></pre> <p>\u4e0b\u9762\u914d\u7f6e\u4e00\u4e2a\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\uff0c\u7136\u540e\u5199\u5165\u5230 databend \u8868\u7684\u914d\u7f6e\u6587\u4ef6</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"databendwriter\",\n        \"parameter\": {\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"postSql\": [],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://localhost:8000/addax\",\n            \"table\": [\n              \"table1\"\n            ]\n          },\n          \"username\": \"u1\",\n          \"password\": \"123\",\n          \"column\": [\n            \"*\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2databend.json</code></p> <p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4</p> <pre><code>bin/addax.sh job/stream2Databend.json\n</code></pre>"},{"location":"writer/databendwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879, \u5e76\u589e\u52a0\u4e86\u5982\u4e0b\u914d\u7f6e\u9879\uff1a</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 writeMode \u5426 string <code>insert</code> \u5199\u5165\u6a21\u5f0f\uff0c\u652f\u6301 <code>insert</code> \u548c <code>replace</code> \u4e24\u79cd\u6a21\u5f0f onConflictColumn \u5426 string \u65e0 \u51b2\u7a81\u5217\uff0c\u5f53 writeMode \u4e3a <code>replace</code> \u65f6\uff0c\u5fc5\u987b\u6307\u5b9a\u51b2\u7a81\u5217\uff0c\u5426\u5219\u4f1a\u5bfc\u81f4\u5199\u5165\u5931\u8d25\u3002"},{"location":"writer/databendwriter/#writemode","title":"writeMode","text":"<p>\u7528\u6765\u652f\u6301 Databend \u7684 <code>replace into</code> \u8bed\u6cd5\uff0c\u5f53\u8be5\u53c2\u6570\u8bbe\u5b9a\u4e3a <code>replace</code> \u65f6\uff0c\u5fc5\u987b\u540c\u65f6\u6307\u5b9a <code>onConflictColumn</code> \u53c2\u6570\uff0c\u7528\u6765\u5224\u65ad\u6570\u636e\u662f\u63d2\u5165\u8fd8\u662f\u66f4\u65b0\u7684\u4f9d\u636e\u3002</p> <p>\u4e24\u4e2a\u53c2\u6570\u7684\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"writeMode\": \"replace\",\n  \"onConflictColumn\": [\n    \"id\"\n  ]\n}\n</code></pre>"},{"location":"writer/dbfwriter/","title":"DBF Writer","text":"<p>Dbf Writer \u63d0\u4f9b\u4e86\u5411\u672c\u5730\u6587\u4ef6\u5199\u5165\u7c7bdbf\u683c\u5f0f\u7684\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u8868\u6587\u4ef6\u3002</p>"},{"location":"writer/dbfwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"\u4e2d\u6587\u6d4b\u8bd5\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"dbfwriter\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"name\": \"col1\",\n              \"type\": \"char\",\n              \"length\": 100\n            },\n            {\n              \"name\": \"col2\",\n              \"type\": \"numeric\",\n              \"length\": 18,\n              \"scale\": 0\n            },\n            {\n              \"name\": \"col3\",\n              \"type\": \"date\"\n            },\n            {\n              \"name\": \"col4\",\n              \"type\": \"logical\"\n            },\n            {\n              \"name\": \"col5\",\n              \"type\": \"char\",\n              \"length\": 100\n            }\n          ],\n          \"fileName\": \"test.dbf\",\n          \"path\": \"/tmp/out\",\n          \"writeMode\": \"truncate\",\n          \"encoding\": \"GBK\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/dbfwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f string \u65e0 \u6587\u4ef6\u76ee\u5f55\uff0c\u6ce8\u610f\u8fd9\u91cc\u662f\u6587\u4ef6\u5939\uff0c\u4e0d\u662f\u6587\u4ef6 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u96c6\u5408\uff0c\u8be6\u89c1\u793a\u4f8b\u914d\u7f6e fileName \u662f string \u65e0 \u5199\u5165\u7684\u6587\u4ef6\u540d writeMode \u662f string \u65e0 \u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff0c\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 encoding \u5426 string UTF-8 \u6587\u4ef6\u7f16\u7801\uff0c\u6bd4\u5982 <code>GBK</code>, <code>UTF-8</code> nullFormat \u5426 string <code>\\N</code> \u5b9a\u4e49\u54ea\u4e2a\u5b57\u7b26\u4e32\u53ef\u4ee5\u8868\u793a\u4e3anull, dateFormat \u5426 string \u65e0 \u65e5\u671f\u7c7b\u578b\u7684\u6570\u636e\u5e8f\u5217\u5316\u5230\u6587\u4ef6\u4e2d\u65f6\u7684\u683c\u5f0f\uff0c\u4f8b\u5982 <code>\"yyyy-MM-dd\"</code>"},{"location":"writer/dbfwriter/#writemode","title":"writeMode","text":"<p>\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ul> <li>truncate: \u5199\u5165\u524d\u6e05\u7406\u76ee\u5f55\u4e0b <code>fileName</code> \u524d\u7f00\u7684\u6240\u6709\u6587\u4ef6\u3002</li> <li>append: \u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u4f7f\u7528 <code>filename</code> \u5199\u5165\uff0c\u5e76\u4fdd\u8bc1\u6587\u4ef6\u540d\u4e0d\u51b2\u7a81\u3002</li> <li>nonConflict: \u5982\u679c\u76ee\u5f55\u4e0b\u6709 <code>fileName</code> \u524d\u7f00\u7684\u6587\u4ef6\uff0c\u76f4\u63a5\u62a5\u9519\u3002</li> </ul>"},{"location":"writer/dbfwriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u5f53\u524d\u8be5\u63d2\u4ef6\u652f\u6301\u5199\u5165\u7684\u7c7b\u578b\u4ee5\u53ca\u5bf9\u5e94\u5173\u7cfb\u5982\u4e0b\uff1a</p> XBase Type XBase Symbol Java Type used in JavaDBF Character C java.lang.String Numeric N java.math.BigDecimal Floating Point F java.math.BigDecimal Logical L java.lang.Boolean Date D java.util.Date <p>\u5176\u4e2d\uff1a</p> <ul> <li>numeric \u662f\u6307\u672c\u5730\u6587\u4ef6\u4e2d\u4f7f\u7528\u6570\u5b57\u7c7b\u578b\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>19901219</code> ,\u6574\u5f62\u5c0f\u6570\u4f4d\u6570\u4e3a <code>0</code>\u3002</li> <li>logical \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528Boolean\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>true</code>\u3001<code>false</code>\u3002</li> <li>Date \u662f\u6307\u672c\u5730\u6587\u4ef6\u6587\u672c\u4e2d\u4f7f\u7528Date\u8868\u793a\u5f62\u5f0f\uff0c\u4f8b\u5982 <code>2014-12-31</code>\uff0cDate \u662fJAVA\u8bed\u8a00\u7684 Date \u7c7b\u578b\u3002</li> </ul>"},{"location":"writer/doriswriter/","title":"Doris Writer","text":"<p>DorisWriter \u63d2\u4ef6\u7528\u4e8e\u5411 Doris \u6570\u636e\u5e93\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u5199\u5165\u6570\u636e\u3002 \u5176\u5b9e\u73b0\u4e0a\u662f\u901a\u8fc7\u8bbf\u95ee Doris http \u8fde\u63a5(8030)\uff0c\u7136\u540e\u901a\u8fc7 stream load \u52a0\u8f7d\u6570\u636e\u5230\u6570\u636e\u4e2d\uff0c\u76f8\u6bd4 <code>insert into</code> \u65b9\u5f0f\u6548\u7387\u8981\u9ad8\u4e0d\u5c11\uff0c\u4e5f\u662f\u5b98\u65b9\u63a8\u8350\u7684\u751f\u4ea7\u73af\u5883\u4e0b\u7684\u6570\u636e\u52a0\u8f7d\u65b9\u5f0f\u3002</p> <p>Doris \u662f\u4e00\u4e2a\u517c\u5bb9 MySQL \u534f\u8bae\u7684\u6570\u636e\u5e93\u540e\u7aef\uff0c\u56e0\u6b64 Doris \u8bfb\u53d6\u53ef\u4ee5\u4f7f\u7528 MySQL Reader \u8fdb\u884c\u8bbf\u95ee\u3002</p>"},{"location":"writer/doriswriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684\u8868\u7684\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>CREATE\nDATABASE example_db;\nCREATE TABLE example_db.table1\n(\n    siteid   INT         DEFAULT '10',\n    citycode SMALLINT,\n    username VARCHAR(32) DEFAULT '',\n    pv       BIGINT SUM DEFAULT '0'\n) AGGREGATE KEY(siteid, citycode, username)\nDISTRIBUTED BY HASH(siteid) BUCKETS 10\nPROPERTIES(\"replication_num\" = \"1\");\n</code></pre> <p>\u4e0b\u9762\u914d\u7f6e\u4e00\u4e2a\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\uff0c\u7136\u540e\u5199\u5165\u5230 doris \u8868\u7684\u914d\u7f6e\u6587\u4ef6</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"doriswriter\",\n        \"parameter\": {\n          \"loadUrl\": [\n            \"127.0.0.1:8030\"\n          ],\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"batchSize\": 1024,\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"connection\": {\n            \"table\": \"table1\",\n            \"database\": \"example_db\",\n            \"jdbcUrl\": \"jdbc:mysql://localhost:9030/example_db\"\n          },\n          \"loadProps\": {\n            \"format\": \"json\",\n            \"strip_outer_array\": true\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2doris.json</code></p> <p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4</p> <pre><code>bin/addax.sh job/stream2doris.json\n</code></pre> <p>\u8f93\u51fa\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>2021-02-23 15:22:57.851 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-02-23 15:22:57.871 [main] INFO  Engine -\n{\n\"content\":{\n\"reader\":{\n    \"parameter\":{\n            \"column\":[\n                    {\n                            \"random\":\"1,500\",\n                            \"type\":\"long\"\n                    },\n                    {\n                            \"random\":\"1,127\",\n                            \"type\":\"long\"\n                    },\n                    {\n                            \"type\":\"string\",\n                            \"value\":\"username\"\n                    }\n            ],\n            \"sliceRecordCount\":100\n    },\n    \"name\":\"streamreader\"\n},\n\"writer\":{\n    \"parameter\":{\n            \"password\":\"*****\",\n            \"batchSize\":1024,\n            \"connection\":[\n                    {\n                            \"database\":\"example_db\",\n                            \"endpoint\":\"http://127.0.0.1:8030/\",\n                            \"table\":\"table1\"\n                    }\n            ],\n            \"username\":\"test\"\n    },\n    \"name\":\"doriswriter\"\n }\n},\n\"setting\":{\n\"speed\":{\n\"channel\":2\n}\n}\n}\n\n2021-02-23 15:22:57.886 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-23 15:22:57.886 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-23 15:22:57.920 [job-0] INFO  JobContainer - Scheduler starts [1] taskGroups.\n2021-02-23 15:22:57.928 [taskGroup-0] INFO  TaskGroupContainer - taskGroupId=[0] start [2] channels for [2] tasks.\n2021-02-23 15:22:57.935 [taskGroup-0] INFO  Channel - Channel set byte_speed_limit to -1, No bps activated.\n2021-02-23 15:22:57.936 [taskGroup-0] INFO  Channel - Channel set record_speed_limit to -1, No tps activated.\n2021-02-23 15:22:57.970 [0-0-1-writer] INFO  DorisWriterTask - connect DorisDB with http://127.0.0.1:8030//api/example_db/table1/_stream_load\n2021-02-23 15:22:57.970 [0-0-0-writer] INFO  DorisWriterTask - connect DorisDB with http://127.0.0.1:8030//api/example_db/table1/_stream_load\n\n2021-02-23 15:23:00.941 [job-0] INFO  JobContainer - PerfTrace not enable!\n2021-02-23 15:23:00.946 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-23 15:22:57\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-23 15:23:00\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            1.56KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             66rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 200\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"writer/doriswriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 loadUrl \u662f string \u65e0 Stream Load \u7684\u8fde\u63a5\u76ee\u6807 \uff5c username \u662f string \u65e0 \u8bbf\u95eeDoris\u6570\u636e\u5e93\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u8bbf\u95eeDoris\u6570\u636e\u5e93\u7684\u5bc6\u7801 flushInterval \u5426 int 3000 \u6570\u636e\u5199\u5165\u5230\u76ee\u6807\u8868\u7684\u95f4\u9694\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u6beb\u79d2\uff0c\u5373\u6bcf\u9694\u591a\u5c11\u6beb\u79d2\u5199\u5165\u4e00\u6b21\u6570\u636e flushQueueLength \u5426 int 1 \u4e0a\u4f20\u6570\u636e\u7684\u961f\u5217\u957f\u5ea6 table \u662f string \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1 RBDMS Writer batchSize \u5426 int 2048 \u6bcf\u6279\u6b21\u5bfc\u5165\u6570\u636e\u7684\u6700\u5927\u884c\u6570 loadProps \u5426 map <code>csv</code> streamLoad \u7684\u8bf7\u6c42\u53c2\u6570\uff0c\u8be6\u60c5\u53c2\u7167StreamLoad\u4ecb\u7ecd\u9875\u9762 preSql \u5426 list \u5199\u5165\u6570\u636e\u5230\u76ee\u6807\u8868\u524d\u8981\u6267\u884c\u7684 SQL \u8bed\u53e5 postSql \u5426 list \u6570\u636e\u5199\u5b8c\u540e\u8981\u6267\u884c\u7684 SQL \u8bed\u53e5"},{"location":"writer/doriswriter/#loadurl","title":"loadUrl","text":"<p>\u4f5c\u4e3a Stream Load \u7684\u8fde\u63a5\u76ee\u6807\u3002\u683c\u5f0f\u4e3a \"ip:port\"\u3002\u5176\u4e2d IP \u662f FE \u8282\u70b9 IP\uff0cport \u662f FE \u8282\u70b9\u7684 http_port\u3002\u53ef\u4ee5\u586b\u5199\u591a\u4e2a\uff0c\u5f53\u586b\u5199\u591a\u4e2a\u65f6\uff0c\u63d2\u4ef6\u4f1a\u6bcf\u4e2a\u6279\u6b21\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6709\u6548 FE \u8282\u70b9\u8fdb\u884c\u8fde\u63a5\u3002</p>"},{"location":"writer/doriswriter/#column","title":"column","text":"<p>\u5141\u8bb8\u914d\u7f6e\u4e3a <code>[\"*\"]</code> \uff0c \u5982\u679c\u662f \"*\" , \u5219\u5c1d\u8bd5\u4ece Doris \u6570\u636e\u5e93\u4e2d\u76f4\u63a5\u8bfb\u53d6\u8868\u5b57\u6bb5\uff0c\u7136\u540e\u8fdb\u884c\u62fc\u88c5\u3002</p>"},{"location":"writer/doriswriter/#loadprops","title":"loadProps","text":"<p>StreamLoad \u7684\u8bf7\u6c42\u53c2\u6570\uff0c\u8be6\u60c5\u53c2\u7167StreamLoad\u4ecb\u7ecd\u9875\u9762\u3002Stream load - Apache Doris</p> <p>\u8fd9\u91cc\u5305\u62ec\u5bfc\u5165\u7684\u6570\u636e\u683c\u5f0f\uff1aformat\u7b49\uff0c\u5bfc\u5165\u6570\u636e\u683c\u5f0f\u9ed8\u8ba4\u6211\u4eec\u4f7f\u7528csv\uff0c\u652f\u6301JSON\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u7167\u4e0b\u9762\u7c7b\u578b\u8f6c\u6362\u90e8\u5206\uff0c\u4e5f\u53ef\u4ee5\u53c2\u7167\u4e0a\u9762Stream load \u5b98\u65b9\u4fe1\u606f</p>"},{"location":"writer/doriswriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u9ed8\u8ba4\u4f20\u5165\u7684\u6570\u636e\u5747\u4f1a\u88ab\u8f6c\u4e3a\u5b57\u7b26\u4e32\uff0c\u5e76\u4ee5\\t\u4f5c\u4e3a\u5217\u5206\u9694\u7b26\uff0c\\n\u4f5c\u4e3a\u884c\u5206\u9694\u7b26\uff0c\u7ec4\u6210csv\u6587\u4ef6\u8fdb\u884cStreamLoad\u5bfc\u5165\u64cd\u4f5c\u3002</p> <p>\u9ed8\u8ba4\u662fcsv\u683c\u5f0f\u5bfc\u5165\uff0c\u5982\u9700\u66f4\u6539\u5217\u5206\u9694\u7b26\uff0c \u5219\u6b63\u786e\u914d\u7f6e loadProps \u5373\u53ef</p> <pre><code>{\n  \"loadProps\": {\n    \"column_separator\": \"\\\\x01\",\n    \"line_delimiter\": \"\\\\x02\"\n  }\n}\n</code></pre> <p>\u5982\u9700\u66f4\u6539\u5bfc\u5165\u683c\u5f0f\u4e3ajson\uff0c \u5219\u6b63\u786e\u914d\u7f6e loadProps \u5373\u53ef\uff1a</p> <pre><code>{\n  \"loadProps\": {\n    \"format\": \"json\",\n    \"strip_outer_array\": true\n  }\n}\n</code></pre>"},{"location":"writer/elasticsearchwriter/","title":"ElasticSearch Writer","text":"<p>ElasticSearch Writer \u63d2\u4ef6\u7528\u4e8e\u5411 ElasticSearch \u5199\u5165\u6570\u636e\u3002 \u5176\u5b9e\u73b0\u662f\u901a\u8fc7 elasticsearch \u7684 rest api \u63a5\u53e3\uff0c \u6279\u91cf\u628a\u636e\u5199\u5165 elasticsearch</p>"},{"location":"writer/elasticsearchwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"job/stream2es.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1.1.1.1\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"hello world\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"long text\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"41.12,-71.34\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"2017-05-25 11:22:33\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"elasticsearchwriter\",\n        \"parameter\": {\n          \"endpoint\": \"http://localhost:9200\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"cleanup\": true,\n          \"settings\": {\n            \"index\": {\n              \"number_of_shards\": 1,\n              \"number_of_replicas\": 0\n            }\n          },\n          \"discovery\": false,\n          \"batchSize\": 1000,\n          \"splitter\": \",\",\n          \"column\": [\n            {\n              \"name\": \"pk\",\n              \"type\": \"id\"\n            },\n            {\n              \"name\": \"col_ip\",\n              \"type\": \"ip\"\n            },\n            {\n              \"name\": \"col_double\",\n              \"type\": \"double\"\n            },\n            {\n              \"name\": \"col_long\",\n              \"type\": \"long\"\n            },\n            {\n              \"name\": \"col_integer\",\n              \"type\": \"integer\"\n            },\n            {\n              \"name\": \"col_keyword\",\n              \"type\": \"keyword\"\n            },\n            {\n              \"name\": \"col_text\",\n              \"type\": \"text\",\n              \"analyzer\": \"ik_max_word\"\n            },\n            {\n              \"name\": \"col_geo_point\",\n              \"type\": \"geo_point\"\n            },\n            {\n              \"name\": \"col_date\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd HH:mm:ss\"\n            },\n            {\n              \"name\": \"col_nested1\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_nested2\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_object1\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_object2\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_integer_array\",\n              \"type\": \"integer\",\n              \"array\": true\n            },\n            {\n              \"name\": \"col_geo_shape\",\n              \"type\": \"geo_shape\",\n              \"tree\": \"quadtree\",\n              \"precision\": \"10m\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/elasticsearchwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 ElasticSearch \u7684\u8fde\u63a5\u5730\u5740,\u5982\u679c\u662f\u96c6\u7fa4\uff0c\u5219\u591a\u4e2a\u5730\u5740\u7528\u9017\u53f7(,)\u5206\u5272 accessId \u5426 string \u7a7a http auth \u4e2d\u7684 user, \u9ed8\u8ba4\u4e3a\u7a7a accessKey \u5426 string \u7a7a http auth \u4e2d\u7684 password index \u662f string \u65e0 index \u540d type \u5426 string <code>default</code> index \u7c7b\u578b cleanup \u5426 boolean false \u662f\u5426\u5220\u9664\u539f\u8868 batchSize \u5426 int 1000 \u6bcf\u6b21\u6279\u91cf\u6570\u636e\u7684\u6761\u6570 trySize \u5426 int 30 \u5931\u8d25\u540e\u91cd\u8bd5\u7684\u6b21\u6570 timeout \u5426 int 600000 \u5ba2\u6237\u7aef\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u6beb\u79d2(ms) discovery \u5426 boolean false \u542f\u7528\u8282\u70b9\u53d1\u73b0\u5c06(\u8f6e\u8be2)\u5e76\u5b9a\u671f\u66f4\u65b0\u5ba2\u6237\u673a\u4e2d\u7684\u670d\u52a1\u5668\u5217\u8868 compression \u5426 boolean true \u5426\u662f\u5f00\u542f http \u8bf7\u6c42\u538b\u7f29 multiThread \u5426 boolean true \u662f\u5426\u5f00\u542f\u591a\u7ebf\u7a0b http \u8bf7\u6c42 ignoreWriteError \u5426 boolean false \u5199\u5165\u9519\u8bef\u65f6\uff0c\u662f\u5426\u91cd\u8bd5\uff0c\u5982\u679c\u662f <code>true</code> \u5219\u8868\u793a\u4e00\u76f4\u91cd\u8bd5\uff0c\u5426\u5219\u5ffd\u7565\u8be5\u6761\u6570\u636e ignoreParseError \u5426 boolean true \u89e3\u6790\u6570\u636e\u683c\u5f0f\u9519\u8bef\u65f6\uff0c\u662f\u5426\u7ee7\u7eed\u5199\u5165 alias \u5426 string \u65e0 \u6570\u636e\u5bfc\u5165\u5b8c\u6210\u540e\u5199\u5165\u522b\u540d aliasMode \u5426 string append \u6570\u636e\u5bfc\u5165\u5b8c\u6210\u540e\u589e\u52a0\u522b\u540d\u7684\u6a21\u5f0f\uff0cappend(\u589e\u52a0\u6a21\u5f0f), exclusive(\u53ea\u7559\u8fd9\u4e00\u4e2a) settings \u5426 map \u65e0 \u521b\u5efa index \u65f6\u5019\u7684 settings, \u4e0e elasticsearch \u5b98\u65b9\u76f8\u540c splitter \u5426 string <code>,</code> \u5982\u679c\u63d2\u5165\u6570\u636e\u662f array\uff0c\u5c31\u4f7f\u7528\u6307\u5b9a\u5206\u9694\u7b26 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u6863\u4e2d\u7ed9\u51fa\u7684\u6837\u4f8b\u4e2d\u5305\u542b\u4e86\u5168\u90e8\u652f\u6301\u7684\u5b57\u6bb5\u7c7b\u578b dynamic \u5426 boolean false \u4e0d\u4f7f\u7528 addax \u7684 mappings\uff0c\u4f7f\u7528 es \u81ea\u5df1\u7684\u81ea\u52a8 mappings"},{"location":"writer/elasticsearchwriter/#_3","title":"\u7ea6\u675f\u9650\u5236","text":"<ul> <li>\u5982\u679c\u5bfc\u5165 id\uff0c\u8fd9\u6837\u6570\u636e\u5bfc\u5165\u5931\u8d25\u4e5f\u4f1a\u91cd\u8bd5\uff0c\u91cd\u65b0\u5bfc\u5165\u4e5f\u4ec5\u4ec5\u662f\u8986\u76d6\uff0c\u4fdd\u8bc1\u6570\u636e\u4e00\u81f4\u6027</li> <li>\u5982\u679c\u4e0d\u5bfc\u5165 id\uff0c\u5c31\u662f append_only \u6a21\u5f0f\uff0celasticsearch \u81ea\u52a8\u751f\u6210 id\uff0c\u901f\u5ea6\u4f1a\u63d0\u5347 20%\u5de6\u53f3\uff0c\u4f46\u6570\u636e\u65e0\u6cd5\u4fee\u590d\uff0c\u9002\u5408\u65e5\u5fd7\u578b\u6570\u636e(\u5bf9\u6570\u636e\u7cbe\u5ea6\u8981\u6c42\u4e0d\u9ad8\u7684)</li> </ul>"},{"location":"writer/excelwriter/","title":"Excel Writer","text":"<p>Excel Writer \u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u5199\u5165\u5230 Excel \u6587\u4ef6\u7684\u529f\u80fd</p>"},{"location":"writer/excelwriter/#_1","title":"\u914d\u7f6e\u793a\u4f8b","text":"<p>\u6211\u4eec\u5047\u5b9a\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u5199\u5165\u5230 Excel \u6587\u4ef6\u4e2d</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"excelwriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/out\",\n          \"fileName\": \"test\",\n          \"header\": [\n            \"str\",\n            \"\u957f\u5ea6\",\n            \"\u65e5\u671f\",\n            \"\u662f\u5426\u4e3a\u771f\",\n            \"\u5b57\u8282\u7c7b\u578b\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u8bb2\u4e0a\u8ff0\u5185\u5bb9\u4fdd\u5b58\u4e3a <code>job/stream2excel.json</code></p> <p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff1a</p> <pre><code>bin/addax.sh job/stream2excel.sh\n</code></pre> <p>\u5e94\u8be5\u5f97\u5230\u7c7b\u4f3c\u5982\u4e0b\u7684\u8f93\u51fa</p> \u70b9\u51fb\u5c55\u5f00 <pre><code>  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3-SNAPSHOT)\n\n2021-09-10 22:16:38.247 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-10 22:16:38.269 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"type\":\"long\",\n                            \"value\":19890604\n                        },\n                        {\n                            \"type\":\"date\",\n                            \"value\":\"1989-06-04 11:22:33\"\n                        },\n                        {\n                            \"type\":\"bool\",\n                            \"value\":true\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        }\n                    ],\n                    \"sliceRecordCount\":1000\n                },\n                \"name\":\"streamreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"path\":\"/tmp/out\",\n                    \"fileName\":\"test\",\n                    \"header\":[\n                        \"str\",\n                        \"\u957f\u5ea6\",\n                        \"\u65e5\u671f\",\n                        \"\u662f\u5426\u4e3a\u771f\",\n                        \"\u5b57\u8282\u7c7b\u578b\"\n                    ],\n                    \"writeMode\":\"truncate\"\n                },\n                \"name\":\"excelwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-09-10 22:16:38.287 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-10 22:16:38.287 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-10 22:16:38.289 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-10 22:16:38.303 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-09-10 22:16:38.305 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] splits to [1] tasks.\n2021-09-10 22:16:38.325 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-10 22:16:38.332 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-09-10 22:16:38.335 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-10 22:16:38.336 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-10 22:16:41.345 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do post work.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-09-10 22:16:41.348 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-10 22:16:41.349 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 1000 records, 26000 bytes | Speed 8.46KB/s, 333 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.528s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-09-10 22:16:41.350 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-10 22:16:38\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-10 22:16:41\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            8.46KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :            333rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                1000\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"writer/excelwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f string \u65e0 \u6307\u5b9a\u6587\u4ef6\u4fdd\u5b58\u7684\u76ee\u5f55, \u6307\u5b9a\u7684\u76ee\u5f55\u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5219\u5c1d\u8bd5\u521b\u5efa fileName \u662f string \u65e0 \u8981\u751f\u6210\u7684excel \u6587\u4ef6\u540d\uff0c\u8be6\u8ff0\u5982\u4e0b header \u5426 list \u65e0 Excel \u8868\u5934"},{"location":"writer/excelwriter/#filename","title":"fileName","text":"<p>\u5982\u679c\u914d\u7f6e\u7684 <code>fileName</code> \u6ca1\u6709\u540e\u7f00\uff0c\u5219\u81ea\u52a8\u52a0\u4e0a <code>.xlsx</code>\uff1b \u5982\u679c\u540e\u7f00\u4e3a <code>.xls</code>\uff0c\u5219\u62a5\u9519\uff0c\u56e0\u4e3a\u5f53\u524d\u4ec5\u751f\u6210 Excel 97 \u4ee5\u540e\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u5373 <code>.xlsx</code> \u540e\u7f00\u7684\u6587\u4ef6</p>"},{"location":"writer/excelwriter/#header","title":"header","text":"<p>\u5982\u679c\u4e0d\u6307\u5b9a <code>header</code> \uff0c\u5219\u751f\u6210\u7684 Excel \u6587\u4ef6\u6ca1\u6709\u8868\u5934\uff0c\u53ea\u6709\u6570\u636e\u3002 \u6ce8\u610f\uff0c\u63d2\u4ef6\u4e0d\u5173\u5fc3 header \u7684\u6570\u91cf\u662f\u5426\u5339\u914d\u6570\u636e\u4e2d\u7684\u5217\u6570\uff0c\u4e5f\u5c31\u662f\u8bf4\u8868\u5934\u7684\u5217\u6570\u5e76\u4e0d\u8981\u6c42\u548c\u63a5\u4e0b\u6765\u7684\u6570\u636e\u7684\u5217\u6570\u76f8\u7b49\u3002</p>"},{"location":"writer/excelwriter/#_3","title":"\u9650\u5236","text":"<ol> <li>\u5f53\u524d\u4ec5\u751f\u6210\u4e00\u4e2a Excel \u6587\u4ef6\uff0c\u4e14\u6ca1\u6709\u8003\u8651\u884c\u6570\u548c\u5217\u6570\u662f\u5426\u8d85\u8fc7\u4e86 Excel \u7684\u9650\u5b9a</li> <li>\u5982\u679c\u6307\u5b9a\u7684\u76ee\u5f55\u4e0b\u6709\u540c\u540d\u6587\u4ef6\uff0c\u5f53\u524d\u4f1a\u88ab\u8986\u76d6\uff0c\u540e\u7eed\u4f1a\u7edf\u4e00\u5904\u7406\u76ee\u6807\u76ee\u5f55\u7684\u95ee\u9898</li> <li>\u5f53\u524d\u65e5\u671f\u683c\u5f0f\u7684\u6570\u636e\uff0c\u8bbe\u7f6e\u5355\u5143\u683c\u6837\u5f0f\u4e3a <code>yyyy-MM-dd HH:mm:ss</code>\uff0c\u4e14\u4e0d\u80fd\u5b9a\u5236</li> <li>\u4e0d\u652f\u6301\u4e8c\u8fdb\u5236\u7c7b\u578b\u7684\u6570\u636e\u5199\u5165</li> </ol>"},{"location":"writer/ftpwriter/","title":"Ftp Writer","text":"<p>Ftp Writer \u63d0\u4f9b\u4e86\u5411\u8fdc\u7a0b FTP/SFTP \u670d\u52a1\u5199\u5165\u6587\u4ef6\u7684\u80fd\u529b\uff0c\u5f53\u524d\u4ec5\u652f\u6301\u5199\u5165\u6587\u672c\u6587\u4ef6\u3002</p>"},{"location":"writer/ftpwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"job/stream2ftp.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"ftpwriter\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"***\",\n          \"port\": 22,\n          \"username\": \"xxx\",\n          \"password\": \"xxx\",\n          \"timeout\": \"60000\",\n          \"connectPattern\": \"PASV\",\n          \"path\": \"/tmp/data/\",\n          \"fileName\": \"test\",\n          \"writeMode\": \"truncate|append|nonConflict\",\n          \"fieldDelimiter\": \",\",\n          \"encoding\": \"UTF-8\",\n          \"nullFormat\": \"null\",\n          \"dateFormat\": \"yyyy-MM-dd\",\n          \"fileFormat\": \"csv\",\n          \"useKey\": false,\n          \"keyPath\": \"\",\n          \"keyPass\": \"\",\n          \"header\": []\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/ftpwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 protocol \u662f string <code>ftp</code> \u670d\u52a1\u5668\u534f\u8bae\uff0c\u76ee\u524d\u652f\u6301\u4f20\u8f93\u534f\u8bae\u6709 ftp \u548c sftp host \u662f string \u65e0 \u670d\u52a1\u5668\u5730\u5740 port \u5426 int 22/21 ftp \u9ed8\u8ba4\u4e3a 21\uff0csftp \u9ed8\u8ba4\u4e3a 22 timeout \u5426 int <code>60000</code> \u8fde\u63a5ftp\u670d\u52a1\u5668\u8fde\u63a5\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u6beb\u79d2(ms) connectPattern \u5426 string <code>PASV</code> \u8fde\u63a5\u6a21\u5f0f\uff0c\u4ec5\u652f\u6301 <code>PORT</code>, <code>PASV</code> \u6a21\u5f0f\u3002ftp\u534f\u8bae\u65f6\u4f7f\u7528 \uff5c username \u662f string \u65e0 \u7528\u6237\u540d password \u662f string \u65e0 \u8bbf\u95ee\u5bc6\u7801 useKey \u5426 boolean false \u662f\u5426\u4f7f\u7528\u79c1\u94a5\u767b\u5f55\uff0c\u4ec5\u9488\u5bf9 sftp \u767b\u5f55\u6709\u6548 keyPath \u5426 string <code>~/.ssh/id_rsa</code> \u79c1\u94a5\u5730\u5740 keyPass \u5426 string \u65e0 \u79c1\u94a5\u5bc6\u7801\uff0c\u82e5\u6ca1\u6709\u8bbe\u7f6e\u79c1\u94a5\u5bc6\u7801\uff0c\u5219\u65e0\u9700\u914d\u7f6e\u8be5\u9879 path \u662f string \u65e0 \u8fdc\u7a0bFTP\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0cFtpWriter\u4f1a\u5199\u5165Path\u76ee\u5f55\u4e0b\u5c5e\u591a\u4e2a\u6587\u4ef6 fileName \u662f string \u65e0 \u5199\u5165\u7684\u6587\u4ef6\u540d\uff0c\u8be5\u6587\u4ef6\u540d\u4f1a\u6dfb\u52a0\u968f\u673a\u7684\u540e\u7f00\u4f5c\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u5199\u5165\u5b9e\u9645\u6587\u4ef6\u540d writeMode \u662f string \u65e0 \u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff0c\u8be6\u89c1\u4e0b\u6587 fieldDelimiter \u662f string <code>,</code> \u63cf\u8ff0\uff1a\u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26 compress \u5426 string \u65e0 \u6587\u672c\u538b\u7f29\u7c7b\u578b\uff0c\u6682\u4e0d\u652f\u6301 encoding \u5426 string <code>utf-8</code> \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e dateFormat \u5426 string \u65e0 \u65e5\u671f\u7c7b\u578b\u7684\u6570\u636e\u5e8f\u5217\u5316\u5230\u6587\u4ef6\u4e2d\u65f6\u7684\u683c\u5f0f\uff0c\u4f8b\u5982 <code>\"yyyy-MM-dd\"</code> fileFormat \u5426 string <code>text</code> \u6587\u4ef6\u5199\u51fa\u7684\u683c\u5f0f\uff0c\u5305\u62ec csv, text \u4e24\u79cd\uff0c header \u5426 list \u65e0 text\u5199\u51fa\u65f6\u7684\u8868\u5934\uff0c\u793a\u4f8b <code>['id', 'name', 'age']</code> nullFormat \u5426 string <code>\\N</code> \u5b9a\u4e49\u54ea\u4e9b\u5b57\u7b26\u4e32\u53ef\u4ee5\u8868\u793a\u4e3anull maxTraversalLevel \u5426 int 100 \u5141\u8bb8\u904d\u5386\u6587\u4ef6\u5939\u7684\u6700\u5927\u5c42\u6570 csvReaderConfig \u5426 map \u65e0 \u8bfb\u53d6CSV\u7c7b\u578b\u6587\u4ef6\u53c2\u6570\u914d\u7f6e\uff0c\u8be6\u89c1\u4e0b\u6587"},{"location":"writer/ftpwriter/#writemode","title":"writeMode","text":"<p>\u63cf\u8ff0\uff1aFtpWriter\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ol> <li><code>truncate</code>\uff0c\u5199\u5165\u524d\u6e05\u7406\u76ee\u5f55\u4e0b\u4e00fileName\u524d\u7f00\u7684\u6240\u6709\u6587\u4ef6\u3002</li> <li><code>append</code>\uff0c\u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0cAddax FtpWriter\u76f4\u63a5\u4f7f\u7528filename\u5199\u5165\uff0c\u5e76\u4fdd\u8bc1\u6587\u4ef6\u540d\u4e0d\u51b2\u7a81\u3002</li> <li><code>nonConflict</code>\uff0c\u5982\u679c\u76ee\u5f55\u4e0b\u6709fileName\u524d\u7f00\u7684\u6587\u4ef6\uff0c\u76f4\u63a5\u62a5\u9519\u3002</li> </ol>"},{"location":"writer/ftpwriter/#_3","title":"\u8ba4\u8bc1","text":"<p>\u4ece <code>4.0.2</code> \u7248\u672c\u5f00\u59cb\uff0c \u652f\u6301\u79c1\u94a5\u8ba4\u8bc1\u65b9\u5f0f\u767b\u5f55 SFTP \u670d\u52a1\u5668\uff0c\u5982\u679c\u5bc6\u7801\u548c\u79c1\u6709\u90fd\u586b\u5199\u4e86\uff0c\u5219\u4e24\u8005\u8ba4\u8bc1\u65b9\u5f0f\u90fd\u4f1a\u5c1d\u8bd5\u3002 \u6ce8\u610f\uff0c\u5982\u679c\u586b\u5199\u4e86 <code>keyPath</code>, <code>keyPass</code> \u9879\uff0c\u4f46 <code>useKey</code> \u8bbe\u7f6e\u4e3a <code>false</code> \uff0c\u63d2\u4ef6\u4f9d\u7136\u4e0d\u4f1a\u5c1d\u8bd5\u7528\u79c1\u94a5\u8fdb\u884c\u767b\u5f55\u3002</p>"},{"location":"writer/ftpwriter/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>FTP\u6587\u4ef6\u672c\u8eab\u4e0d\u63d0\u4f9b\u6570\u636e\u7c7b\u578b\uff0c\u8be5\u7c7b\u578b\u662f Addax FtpWriter \u5b9a\u4e49\uff1a</p> Addax \u5185\u90e8\u7c7b\u578b FTP\u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long -&gt; \u5b57\u7b26\u4e32\u5e8f\u5217\u5316\u8868\u793a Double Double -&gt; \u5b57\u7b26\u4e32\u5e8f\u5217\u5316\u8868\u793a String String -&gt; \u5b57\u7b26\u4e32\u5e8f\u5217\u5316\u8868\u793a Boolean Boolean -&gt; \u5b57\u7b26\u4e32\u5e8f\u5217\u5316\u8868\u793a Date Date -&gt; \u5b57\u7b26\u4e32\u5e8f\u5217\u5316\u8868\u793a"},{"location":"writer/greenplumwriter/","title":"Greenplum Writer","text":"<p>Greenplum Writer \u63d2\u4ef6\u4f7f\u7528 <code>COPY FROM</code> \u8bed\u6cd5 \u5c06\u6570\u636e\u5199\u5165 Greenplum \u6570\u636e\u5e93\u3002</p>"},{"location":"writer/greenplumwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u6f14\u793a\u4ece greenplum \u6307\u5b9a\u7684\u8868\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u63d2\u5165\u5230\u5177\u6709\u76f8\u540c\u8868\u7ed3\u6784\u7684\u53e6\u5916\u4e00\u5f20\u8868\u4e2d\uff0c\u7528\u6765\u6d4b\u8bd5\u8be5\u63d2\u4ef6\u6240\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u3002</p> <pre><code>create table if not exists addax_tbl\n(\n    c_bigint\n    bigint,\n    c_bit\n    bit(3),\n    c_bool boolean,\n    c_byte bytea,\n    c_char char(10),\n    c_varchar varchar(20),\n    c_date date,\n    c_double float8,\n    c_int integer,\n    c_json json,\n    c_number decimal(8, 3),\n    c_real real,\n    c_small smallint,\n    c_text text,\n    c_ts timestamp,\n    c_uuid uuid,\n    c_xml xml,\n    c_money money,\n    c_inet inet,\n    c_cidr cidr,\n    c_macaddr macaddr\n    );\ninsert into addax_tbl\nvalues (999988887777,\n        B'101',\n        TRUE,\n        '\\xDEADBEEF',\n        'hello',\n        'hello, world',\n        '2021-01-04',\n        999888.9972,\n        9876542,\n        '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'::json,\n        12345.123,\n        123.123,\n        126,\n        'this is a long text ',\n        '2020-01-04 12:13:14',\n        'A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'::uuid,\n        '&lt;foo&gt;bar&lt;/foo&gt;'::xml,\n        '52093.89'::money,\n        '192.168.1.1'::inet,\n        '192.168.1/24'::cidr,\n        '08002b:010203'::macaddr);\n</code></pre> <p>\u521b\u5efa\u9700\u8981\u63d2\u5165\u7684\u8868\u7684\u8bed\u53e5\u5982\u4e0b:</p> <pre><code>create table gp_test like addax_tbl;\n</code></pre>"},{"location":"writer/greenplumwriter/#_2","title":"\u4efb\u52a1\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u662f\u914d\u7f6e\u6587\u4ef6</p> job/pg2gp.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"wgzhao\",\n          \"password\": \"wgzhao\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/wgzhao\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"greenplumwriter\",\n        \"parameter\": {\n          \"username\": \"wgzhao\",\n          \"password\": \"wgzhao\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/wgzhao\",\n            \"table\": [\n              \"gp_test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/pg2gp.json</code></p>"},{"location":"writer/greenplumwriter/#_3","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/pg2gp.json\n</code></pre>"},{"location":"writer/greenplumwriter/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/hanawriter/","title":"HANA Writer","text":"<p>HANA Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 SAP HANA \u76ee\u7684\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/hanawriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684 HANA \u8868\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table system.addax_tbl\n(\ncol1 varchar(200) ,\ncol2 int(4),\ncol3 date,\ncol4 boolean,\ncol5 clob\n);\n</code></pre> <p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 HANA \u5bfc\u5165\u7684\u6570\u636e\u3002</p> job/hanawriter.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"hanawriter\",\n        \"parameter\": {\n          \"username\": \"system\",\n          \"password\": \"HXEHana1\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/hana2stream.json</code></p>"},{"location":"writer/hanawriter/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"writer/hanawriter/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/hbase11xsqlwriter/","title":"HBase11x SQL Writer","text":"<p>HBase11x SQL Writer \u63d2\u4ef6\u5229\u7528 Phoniex\uff0c \u7528\u4e8e\u5411 HBase 1.x \u7248\u672c\u7684\u6570\u636e\u5e93\u5199\u5165\u6570\u636e\u3002</p> <p>\u5982\u679c\u4f60\u5e0c\u671b\u901a\u8fc7\u8c03\u7528\u539f\u751f\u63a5\u53e3\u5199\u5165\u6570\u636e\uff0c\u5219\u9700\u8981\u4f7f\u7528HBase11xWriter \u63d2\u4ef6</p> <p>\u5982\u679c HBase \u662f 2.X \u7248\u672c\uff0c\u5219\u9700\u8981\u4f7f\u7528 HBase20xsqlwriter \u63d2\u4ef6</p>"},{"location":"writer/hbase11xsqlwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":""},{"location":"writer/hbase11xsqlwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 hbaseConfig \u662f map \u65e0 hbase \u96c6\u7fa4\u5730\u5740\uff0c\u8be6\u89c1\u793a\u4f8b\u914d\u7f6e table \u662f string \u65e0 \u8981\u5bfc\u5165\u7684\u8868\u540d\uff0c\u5927\u5c0f\u5199\u654f\u611f\uff0c\u901a\u5e38 phoenix \u8868\u90fd\u662f \u5927\u5199 \u8868\u540d column \u662f list \u65e0 \u5217\u540d\uff0c\u5927\u5c0f\u5199\u654f\u611f\uff0c\u901a\u5e38 phoenix \u7684\u5217\u540d\u90fd\u662f \u5927\u5199 batchSize \u5426 int 256 \u4e00\u6b21\u5199\u5165\u7684\u6700\u5927\u8bb0\u5f55\u6570 nullMode \u5426 string skip \u8bfb\u53d6\u5230\u7684\u5217\u503c\u4e3a null \u65f6\uff0c\u5982\u4f55\u5904\u7406\u3002 haveKerberos \u5426 bolean false \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1, true \u8868\u793a\u542f\u7528, false \u8868\u793a\u4e0d\u542f\u7528 kerberosPrincipal \u5426 string \u65e0 kerberos \u51ed\u8bc1\u4fe1\u606f\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548 kerberosKeytabFilePath \u5426 string \u65e0 kerberos \u51ed\u8bc1\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548"},{"location":"writer/hbase11xsqlwriter/#nullmode","title":"nullMode","text":"<p>\u652f\u6301 <code>skip</code>, <code>empty</code>,\u524d\u8005\u8868\u793a\u8df3\u8fc7\u8be5\u5217,\u540e\u8005\u8868\u793a\u63d2\u5165\u7a7a\u503c,\u6570\u503c\u7c7b\u578b\u4e3a 0,\u5b57\u7b26\u7c7b\u578b\u4e3a <code>null</code></p> <p>\u6ce8\u610f\uff1a\u542f\u7528 kerberos \u8ba4\u8bc1\u540e\uff0c\u7a0b\u5e8f\u9700\u8981\u77e5\u9053<code>hbase-site.xml</code> \u6240\u5728\u7684\u8def\u5f84\uff0c\u4e00\u79cd\u529e\u6cd5\u662f\u8fd0\u884c\u6267\u884c\u5728\u73af\u5883\u53d8\u91cf <code>CLASSPATH</code> \u4e2d\u589e\u52a0\u8be5\u6587\u4ef6\u7684\u6240\u5728\u8def\u5f84\u3002</p> <p>\u53e6\u5916\u4e00\u4e2a\u89e3\u51b3\u529e\u6cd5\u662f\u5c06 <code>hbase-site.xml</code> \u6587\u4ef6\u62f7\u8d1d\u5230\u63d2\u4ef6\u7684 <code>libs</code> \u76ee\u5f55\u91cc\u3002</p>"},{"location":"writer/hbase11xwriter/","title":"HBase11X Writer","text":"<p>Hbase11X Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u4ece\u5411 HBase \u5199\u6570\u636e\u7684\u80fd\u529b\u3002\u5728\u5e95\u5c42\u5b9e\u73b0\u4e0a\uff0cHBase11X Writer \u901a\u8fc7 HBase \u7684 Java \u5ba2\u6237\u7aef\u8fde\u63a5\u8fdc\u7a0b HBase \u670d\u52a1\uff0c\u5e76\u901a\u8fc7 put \u65b9\u5f0f\u5199\u5165Hbase\u3002</p> <p>\u5982\u679c HBase \u662f 2.X \u7248\u672c\uff0c\u5219\u9700\u8981\u4f7f\u7528 HBase20xsqlwriter \u63d2\u4ef6</p>"},{"location":"writer/hbase11xwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u4ece\u672c\u5730\u5199\u5165hbase1.1.x\u7684\u4f5c\u4e1a\uff1a</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase11xwriter\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"***\"\n          },\n          \"table\": \"writer\",\n          \"mode\": \"normal\",\n          \"rowkeyColumn\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": -1,\n              \"type\": \"string\",\n              \"value\": \"_\"\n            }\n          ],\n          \"column\": [\n            {\n              \"index\": 1,\n              \"name\": \"cf1:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"name\": \"cf1:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"name\": \"cf1:q3\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"name\": \"cf2:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"name\": \"cf2:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"name\": \"cf2:q3\",\n              \"type\": \"string\"\n            }\n          ],\n          \"versionColumn\": {\n            \"index\": -1,\n            \"value\": \"123456789\"\n          },\n          \"encoding\": \"utf-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 hbaseConfig \u662f map \u65e0 \u8fde\u63a5  HBase \u96c6\u7fa4\u9700\u8981\u7684\u914d\u7f6e\u4fe1\u606f,\u8be6\u89c1\u793a\u4f8b\u914d\u7f6e mode \u662f string \u65e0 \u5199\u5165 HBase \u7684\u6a21\u5f0f\uff0c\u76ee\u524d\u4ec5\u652f\u6301 <code>normal</code> \u6a21\u5f0f table \u662f string \u65e0 HBase \u8868\u540d\uff08\u5927\u5c0f\u5199\u654f\u611f\uff09 encoding \u5426 string UTF-8 \u7f16\u7801\u65b9\u5f0f\uff0c<code>UTF-8</code> \u6216\u662f <code>GBK</code>\uff0c\u7528\u4e8e\u5bf9\u4e8c\u8fdb\u5236\u5b58\u50a8\u7684 <code>HBase byte[]</code> \u8f6c\u4e3a String \u65f6\u7684\u7f16\u7801 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u8981\u5199\u5165\u7684\u5b57\u6bb5\uff0c<code>normal</code> \u6a21\u5f0f\u4e0e  <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b\u9879, \u8be6\u7ec6\u8bf4\u660e\u89c1\u4e0b\u6587 rowkeyColumn \u662f <code>list&lt;map&gt;</code> \u65e0 \u8981\u5199\u5165\u7684 <code>rowkey</code> \u5217, \u8be6\u7ec6\u8bf4\u660e\u89c1\u4e0b\u6587 versionColumn \u5426 string \u65e0 \u6307\u5b9a\u5199\u5165\u7684\u65f6\u95f4\u6233,\u8be6\u89c1\u4e0b\u6587 nullMode \u5426 string skip \u8bfb\u53d6\u7684null\u503c\u65f6\uff0c\u5982\u4f55\u5904\u7406, walFlag \u5426 boolean false \u662f\u5426\u5199 <code>WAL</code>, <code>true</code> \u8868\u793a\u5199\u5165, <code>false</code> \u8868\u793a\u4e0d\u5199 writeBufferSize \u5426 int <code>8388608</code> \u8bbe\u7f6e\u5199 <code>buffer</code> \u5927\u5c0f\uff0c\u5355\u4f4d\u5b57\u8282, \u9ed8\u8ba48M maxVersion \u662f string \u65e0 \u6307\u5b9a\u5728\u591a\u7248\u672c\u6a21\u5f0f\u4e0b\u8bfb\u53d6\u7684\u7248\u672c\u6570\uff0c<code>-1</code> \u8868\u793a\u8bfb\u53d6\u6240\u6709\u7248\u672c, <code>multiVersionFixedColumn</code> \u6a21\u5f0f\u4e0b\u5fc5\u586b range \u5426 string \u65e0 \u6307\u5b9a\u8bfb\u53d6\u7684 <code>rowkey</code> \u8303\u56f4, \u8be6\u89c1\u4e0b\u6587 scanCacheSize \u5426 string 256 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u884c\u6570 scanBatchSize \u5426 string 100 \u6bcf\u6b21\u4ece\u670d\u52a1\u5668\u7aef\u8bfb\u53d6\u7684\u5217\u6570"},{"location":"writer/hbase11xwriter/#column","title":"column","text":"<p>\u8981\u5199\u5165\u7684hbase\u5b57\u6bb5\u3002</p> <ul> <li>index\uff1a\u6307\u5b9a\u8be5\u5217\u5bf9\u5e94 reader \u7aef column \u7684\u7d22\u5f15\uff0c\u4ece 0 \u5f00\u59cb</li> <li>name\uff1a\u6307\u5b9a hbase \u8868\u4e2d\u7684\u5217\uff0c\u5fc5\u987b\u4e3a <code>\u5217\u65cf:\u5217\u540d</code> \u7684\u683c\u5f0f</li> <li>type\uff1a\u6307\u5b9a\u5199\u5165\u6570\u636e\u7c7b\u578b\uff0c\u7528\u4e8e\u8f6c\u6362 <code>HBase byte[]</code>\u3002</li> </ul> <p>\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    {\n      \"index\": 1,\n      \"name\": \"cf1:q1\",\n      \"type\": \"string\"\n    },\n    {\n      \"index\": 2,\n      \"name\": \"cf1:q2\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/#rowkey","title":"rowkey","text":"<p>\u8981\u5199\u5165\u7684 <code>rowkey</code> \u5217\u3002</p> <ul> <li>index\uff1a\u6307\u5b9a\u8be5\u5217\u5bf9\u5e94 reader \u7aef column \u7684\u7d22\u5f15\uff0c\u4ece 0 \u5f00\u59cb\uff0c\u82e5\u4e3a\u5e38\u91cf index \u4e3a \uff0d1\uff1b</li> <li>type\uff1a\u6307\u5b9a\u5199\u5165\u6570\u636e\u7c7b\u578b\uff0c\u7528\u4e8e\u8f6c\u6362 <code>HBase byte[]</code>\uff1b</li> <li>value\uff1a\u914d\u7f6e\u5e38\u91cf\uff0c\u5e38\u4f5c\u4e3a\u591a\u4e2a\u5b57\u6bb5\u7684\u62fc\u63a5\u7b26\u3002</li> </ul> <p>\u63d2\u4ef6\u4f1a\u5c06 rowkeyColumn \u4e2d\u6240\u6709\u5217\u6309\u7167\u914d\u7f6e\u987a\u5e8f\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u5199\u5165 hbase \u7684 rowkey\uff0c\u4e0d\u80fd\u5168\u4e3a\u5e38\u91cf\u3002\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"rowkeyColumn\": [\n    {\n      \"index\": 0,\n      \"type\": \"string\"\n    },\n    {\n      \"index\": -1,\n      \"type\": \"string\",\n      \"value\": \"_\"\n    }\n  ]\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/#versioncolumn","title":"versionColumn","text":"<p>\u6307\u5b9a\u5199\u5165\u7684\u65f6\u95f4\u6233\u3002\u652f\u6301\uff1a\u5f53\u524d\u65f6\u95f4\u3001\u6307\u5b9a\u65f6\u95f4\u5217\uff0c\u6307\u5b9a\u65f6\u95f4\uff0c\u4e09\u8005\u9009\u4e00\u3002\u82e5\u4e0d\u914d\u7f6e\u8868\u793a\u7528\u5f53\u524d\u65f6\u95f4\u3002</p> <p>index\uff1a\u6307\u5b9a\u5bf9\u5e94 reader \u7aef column \u7684\u7d22\u5f15\uff0c\u4ece 0 \u5f00\u59cb\uff0c\u9700\u4fdd\u8bc1\u80fd\u8f6c\u6362\u4e3a long,\u82e5\u662f Date \u7c7b\u578b\uff0c \u4f1a\u5c1d\u8bd5\u7528 <code>yyyy-MM-dd HH:mm:ss</code> \u548c <code>yyyy-MM-dd HH:mm:ss SSS</code> \u53bb\u89e3\u6790\uff1b  \u82e5\u4e3a\u6307\u5b9a\u65f6\u95f4 index \u4e3a <code>\uff0d1</code>\uff0c\u5219 value \u4e3a\u6307\u5b9a\u65f6\u95f4\u7684\u503c\u3002\u914d\u7f6e\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"versionColumn\": {\n    \"index\": 1\n  }\n}\n</code></pre> <p>\u6216\u8005</p> <pre><code>{\n  \"versionColumn\": {\n    \"index\": -1,\n    \"value\": 123456789\n  }\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/#nullmode","title":"nullMode","text":"<p><code>skip</code> \u8868\u793a\u4e0d\u5411hbase\u5199\u8fd9\u5217\uff1b<code>empty</code>\uff1a\u5199\u5165 <code>HConstants.EMPTY_BYTE_ARRAY</code>\uff0c\u5373<code>new byte [0]</code></p>"},{"location":"writer/hbase11xwriter/#_3","title":"\u652f\u6301\u7684\u5217\u7c7b\u578b","text":"<ul> <li>BOOLEAN</li> <li>SHORT</li> <li>INT</li> <li>LONG</li> <li>FLOAT</li> <li>DOUBLE</li> <li>STRING</li> </ul> <p>\u8bf7\u6ce8\u610f: \u9664\u4e0a\u8ff0\u7f57\u5217\u5b57\u6bb5\u7c7b\u578b\u5916\uff0c\u5176\u4ed6\u7c7b\u578b\u5747\u4e0d\u652f\u6301</p>"},{"location":"writer/hbase20xsqlwriter/","title":"HBase20x SQL Writer","text":"<p>HBase20x SQL Writer \u63d2\u4ef6\u5229\u7528 Phoenix \u5411 HBase 2.x \u5199\u5165\u6570\u636e\u3002</p> <p>\u5982\u679c HBase \u662f 1.X \u7248\u672c\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 HBase11xsqlWriter \u6216HBase11xWriter \u63d2\u4ef6</p>"},{"location":"writer/hbase20xsqlwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase20xsqlwriter\",\n        \"parameter\": {\n          \"batchSize\": \"100\",\n          \"column\": [\n            \"UID\",\n            \"TS\",\n            \"EVENTID\",\n            \"CONTENT\"\n          ],\n          \"queryServerAddress\": \"http://127.0.0.1:8765\",\n          \"nullMode\": \"skip\",\n          \"table\": \"TEST_TBL\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hbase20xsqlwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 jdbcUrl \u662f string \u65e0 Phoenix \u8fde\u63a5\u5730\u5740 table \u662f string \u65e0 \u6240\u8981\u8bfb\u53d6\u8868\u540d schema \u5426 string \u65e0 \u8868\u6240\u5728\u7684 schema batchSize \u5426 int 256 \u4e00\u6b21\u6279\u91cf\u5199\u5165\u7684\u6700\u5927\u884c\u6570 column \u5426 list \u65e0 \u5217\u540d\uff0c\u5927\u5c0f\u5199\u654f\u611f\uff0c\u901a\u5e38phoenix\u7684\u5217\u540d\u90fd\u662f**\u5927\u5199** nullMode \u5426 string skip \u8bfb\u53d6\u7684 null \u503c\u65f6\uff0c\u5982\u4f55\u5904\u7406, \u8be6\u8ff0\u89c1\u4e0b haveKerberos \u5426 boolean false \u662f\u5426\u542f\u7528Kerberos\u8ba4\u8bc1, true \u8868\u793a\u542f\u7528, false \u8868\u793a\u4e0d\u542f\u7528 kerberosPrincipal \u5426 string \u65e0 kerberos \u51ed\u8bc1\u4fe1\u606f\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548 kerberosKeytabFilePath \u5426 string \u65e0 kerberos \u51ed\u8bc1\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548"},{"location":"writer/hbase20xsqlwriter/#jdbcurl","title":"jdbcUrl","text":"<p><code>queryServerAddress</code> \u662f\u6ee1\u8db3 Phoenix \u94fe\u63a5\u7684\u5730\u5740\uff0c\u5177\u4f53\u683c\u5f0f\u548c\u8981\u6c42\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u6587\u6863 \uff0c\u5176 jdbc \u8fde\u63a5\u4e32\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>jdbc:phoenix [ :&lt;zookeeper quorum&gt; [ :&lt;port number&gt; [ :&lt;root node&gt; [ :&lt;principal&gt; [ :&lt;keytab file&gt; ] ] ] ] ] \n</code></pre> <ul> <li>zookeeper quorum: zookeeper \u96c6\u7fa4\u5730\u5740\uff0c\u591a\u4e2a\u5730\u5740\u7528\u9017\u53f7\u5206\u9694\uff0c\u5982\uff1a<code>node1,node2,node3</code></li> <li>port number: zookeeper \u96c6\u7fa4\u7aef\u53e3\uff0c\u9ed8\u8ba4\u4e3a 2181</li> <li>root node: zookeeper \u96c6\u7fa4\u6839\u8282\u70b9\uff0c\u9ed8\u8ba4\u4e3a <code>/hbase</code>\uff0c\u542f\u7528 kerberos \u540e\uff0c\u9ed8\u8ba4\u4e3a <code>/hbase-secure</code></li> <li>principal: kerberos \u51ed\u8bc1\u4fe1\u606f\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548</li> <li>keytab file: kerberos \u51ed\u8bc1\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4ec5\u5f53 <code>havekerberos</code> \u542f\u7528\u540e\u6709\u6548</li> </ul> <p>\u5982\u679c\u4f60\u5e0c\u671b\u901a\u8fc7\u8fde\u63a5 Phoenix Query Server (a.k.a PQS) \uff0c\u5219 JDBC \u8fde\u63a5\u4e32\u5982\u4e0b\uff1a</p> <pre><code>jdbc:phoenix:thin:url=&lt;scheme&gt;://&lt;server-hostname&gt;:&lt;port&gt;[;option=value...]\n</code></pre> <ul> <li>schema: \u4f20\u8f93\u534f\u8bae\uff0c<code>http</code> \u6216 <code>https</code>\uff0c\u9ed8\u8ba4\u4e3a <code>http</code></li> <li>server-hostname: Phoenix Query Server \u5730\u5740\uff0c\u5982\uff1a<code>node1</code></li> <li>port: Phoenix Query Server \u7aef\u53e3\uff0c\u9ed8\u8ba4\u4e3a 8765</li> <li>option: \u53ef\u9009\u53c2\u6570\uff0c\u53ef\u4ee5\u662f\u591a\u4e2a\uff0c\u7528\u9017\u53f7\u5206\u9694\uff0c\u5982\uff1a<code>option1=value1,option2=value2</code></li> </ul> <p>\u66f4\u8be6\u7ec6\u7684\u63cf\u8ff0\uff0c\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u6587\u6863</p>"},{"location":"writer/hbase20xsqlwriter/#nullmode","title":"nullMode","text":"<p><code>skip</code> \u8868\u793a\u4e0d\u5411hbase\u5199\u8fd9\u5217\uff1b<code>empty</code>\uff1a\u5199\u5165 <code>HConstants.EMPTY_BYTE_ARRAY</code>\uff0c\u5373<code>new byte [0]</code></p> <p>\u6ce8\u610f\uff1a\u542f\u7528kerberos\u8ba4\u8bc1\u540e\uff0c\u7a0b\u5e8f\u9700\u8981\u77e5\u9053<code>hbase-site.xml</code> \u6240\u5728\u7684\u8def\u5f84\uff0c\u4e00\u79cd\u529e\u6cd5\u662f\u8fd0\u884c\u6267\u884c\u5728\u73af\u5883\u53d8\u91cf <code>CLASSPATH</code> \u4e2d\u589e\u52a0\u8be5\u6587\u4ef6\u7684\u6240\u5728\u8def\u5f84\u3002</p> <p>\u53e6\u5916\u4e00\u4e2a\u89e3\u51b3\u529e\u6cd5\u662f\u5c06 <code>hbase-site.xml</code> \u6587\u4ef6\u62f7\u8d1d\u5230\u63d2\u4ef6\u7684 <code>libs</code> \u76ee\u5f55\u91cc\u3002</p>"},{"location":"writer/hdfswriter/","title":"HDFS Writer","text":"<p>HDFS Writer \u63d0\u4f9b\u5411 HDFS \u6587\u4ef6\u7cfb\u7edf\u6307\u5b9a\u8def\u5f84\u4e2d\u5199\u5165 <code>TextFile</code> \uff0c <code>ORCFile</code> , <code>Parquet</code> \u7b49\u683c\u5f0f\u6587\u4ef6\u7684\u80fd\u529b\uff0c \u6587\u4ef6\u5185\u5bb9\u53ef\u4e0e hive \u4e2d\u8868\u5173\u8054\u3002</p>"},{"location":"writer/hdfswriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"['tag1', 'tag2', 'tag3']\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"{'loc':'HZ','num':'12'}\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        },\n        \"writer\": {\n          \"name\": \"hdfswriter\",\n          \"parameter\": {\n            \"defaultFS\": \"hdfs://xxx:port\",\n            \"fileType\": \"orc\",\n            \"path\": \"/user/hive/warehouse/writerorc.db/orcfull\",\n            \"fileName\": \"xxxx\",\n            \"column\": [\n              {\n                \"name\": \"col1\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col2\",\n                \"type\": \"int\"\n              },\n              {\n                \"name\": \"col3\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col4\",\n                \"type\": \"boolean\"\n              },\n              {\n                \"name\": \"col5\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col6\",\n                \"type\": \"array&lt;string&gt;\"\n              },\n              {\n                \"name\": \"col7\",\n                \"type\": \"map&lt;string,string&gt;\"\n              }\n            ],\n            \"writeMode\": \"overwrite\",\n            \"fieldDelimiter\": \"\\u0001\",\n            \"compress\": \"SNAPPY\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hdfswriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e path \u662f string \u65e0 \u8981\u8bfb\u53d6\u7684\u6587\u4ef6\u8def\u5f84 defaultFS \u662f string \u65e0 \u8be6\u8ff0\u89c1\u4e0b fileType \u662f string \u65e0 \u6587\u4ef6\u7684\u7c7b\u578b\uff0c\u8be6\u8ff0\u89c1\u4e0b fileName \u662f string \u65e0 \u8981\u5199\u5165\u7684\u6587\u4ef6\u540d\uff0c\u7528\u4e8e\u5f53\u4f5c\u524d\u7f00 column \u662f <code>list&lt;map&gt;</code> \u65e0 \u5199\u5165\u7684\u5b57\u6bb5\u5217\u8868 writeMode \u662f string \u65e0 \u5199\u5165\u6a21\u5f0f\uff0c\u8be6\u8ff0\u89c1\u4e0b skipTrash \u5426 boolean false \u662f\u5426\u8df3\u8fc7\u5783\u573e\u56de\u6536\u7ad9\uff0c\u548c <code>writeMode</code> \u914d\u7f6e\u76f8\u5173\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 fieldDelimiter \u5426 string <code>,</code> \u6587\u672c\u6587\u4ef6\u7684\u5b57\u6bb5\u5206\u9694\u7b26\uff0c\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0d\u9700\u8981\u6307\u5b9a\u8be5\u9879 encoding \u5426 string <code>utf-8</code> \u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e\uff0c \u76ee\u524d\u4ec5\u652f\u6301 <code>utf-8</code> nullFormat \u5426 string \u65e0 \u5b9a\u4e49\u8868\u793a\u4e3a\u7a7a\u7684\u5b57\u7b26\uff0c\u4f8b\u5982\u5982\u679c\u7528\u6237\u914d\u7f6e: <code>\"\\\\N\"</code> \uff0c\u90a3\u4e48\u5982\u679c\u6e90\u5934\u6570\u636e\u662f <code>\"\\N\"</code> \uff0c\u89c6\u4f5c <code>null</code> \u5b57\u6bb5 haveKerberos \u5426 boolean false \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u9700\u8981\u540c\u65f6\u914d\u7f6e\u4ee5\u4e0b\u4e24\u9879 kerberosKeytabFilePath \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u6587\u4ef6\u8def\u5f84, \u6bd4\u5982 <code>/your/path/addax.service.keytab</code> kerberosPrincipal \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u4e3b\u4f53, \u6bd4\u5982 <code>addax/node1@WGZHAO.COM</code> compress \u5426 string \u65e0 \u6587\u4ef6\u7684\u538b\u7f29\u683c\u5f0f\uff0c\u8be6\u89c1\u4e0b\u6587 hadoopConfig \u5426 map \u65e0 \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Hadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e preShell \u5426 <code>list</code> \u65e0 \u5199\u5165\u6570\u636e\u524d\u6267\u884c\u7684shell\u547d\u4ee4\uff0c\u6bd4\u5982 <code>hive -e \"truncate table test.hello\"</code> postShell \u5426 <code>list</code> \u65e0 \u5199\u5165\u6570\u636e\u540e\u6267\u884c\u7684shell\u547d\u4ee4\uff0c\u6bd4\u5982 <code>hive -e \"select count(1) from test.hello\"</code> ignoreError \u5426 boolean false \u662f\u5426\u5ffd\u7565<code>preShell</code>, <code>postShell</code> \u547d\u4ee4\u7684\u9519\u8bef hdfsSitePath \u5426 string \u65e0 <code>hdfs-site.xml</code> \u7684\u8def\u5f84\uff0c\u8be6\u7ec6\u89e3\u91ca\u89c1\u4e0b createPath \u5426 boolean \u5426 \u9ed8\u8ba4\u4e0d\u5b58\u5728\u65f6\uff0c\u662f\u5426\u521b\u5efa\uff0c\u9ed8\u8ba4\u4e0d\u521b\u5efa\u800c\u7ed9\u51fa\u62a5\u9519\u63d0\u793a"},{"location":"writer/hdfswriter/#path","title":"path","text":"<p>\u5b58\u50a8\u5230 Hadoop hdfs\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0cHdfsWriter \u4f1a\u6839\u636e\u5e76\u53d1\u914d\u7f6e\u5728 <code>Path</code> \u76ee\u5f55\u4e0b\u5199\u5165\u591a\u4e2a\u6587\u4ef6\u3002\u4e3a\u4e0ehive\u8868\u5173\u8054\uff0c\u8bf7\u586b\u5199hive\u8868\u5728hdfs\u4e0a\u7684\u5b58\u50a8\u8def\u5f84\u3002 \u4f8b\uff1aHive\u4e0a\u8bbe\u7f6e\u7684\u6570\u636e\u4ed3\u5e93\u7684\u5b58\u50a8\u8def\u5f84\u4e3a\uff1a <code>/user/hive/warehouse/</code> \uff0c\u5df2\u5efa\u7acb\u6570\u636e\u5e93\uff1a<code>test</code>\uff0c\u8868\uff1a<code>hello</code>\uff1b \u5219\u5bf9\u5e94\u7684\u5b58\u50a8\u8def\u5f84\u4e3a\uff1a<code>/user/hive/warehouse/test.db/hello</code> (\u5982\u679c\u5efa\u8868\u65f6\u6307\u5b9a\u4e86<code>location</code> \u5c5e\u6027\uff0c\u5219\u4f9d\u636e\u8be5\u5c5e\u6027\u7684\u8def\u5f84)</p>"},{"location":"writer/hdfswriter/#defaultfs","title":"defaultFS","text":"<p>Hadoop hdfs \u6587\u4ef6\u7cfb\u7edf namenode \u8282\u70b9\u5730\u5740\u3002\u683c\u5f0f\uff1a<code>hdfs://ip:port</code> \uff1b\u4f8b\u5982\uff1a<code>hdfs://127.0.0.1:9000</code> , \u5982\u679c\u542f\u7528\u4e86HA\uff0c\u5219\u4e3a servicename \u6a21\u5f0f\uff0c\u6bd4\u5982 <code>hdfs://sandbox</code></p>"},{"location":"writer/hdfswriter/#filetype","title":"fileType","text":"<p>\u63cf\u8ff0\uff1a\u6587\u4ef6\u7684\u7c7b\u578b\uff0c\u76ee\u524d\u53ea\u652f\u6301\u7528\u6237\u914d\u7f6e\u4e3a</p> <ul> <li>text \u8868\u793a Text file\u6587\u4ef6\u683c\u5f0f</li> <li>orc \u8868\u793a OrcFile\u6587\u4ef6\u683c\u5f0f</li> <li>parquet \u8868\u793a Parquet \u6587\u4ef6\u683c\u5f0f</li> <li>rc \u8868\u793a Rcfile \u6587\u4ef6\u683c\u5f0f</li> <li>seq \u8868\u793asequence file\u6587\u4ef6\u683c\u5f0f</li> <li>csv \u8868\u793a\u666e\u901ahdfs\u6587\u4ef6\u683c\u5f0f\uff08\u903b\u8f91\u4e8c\u7ef4\u8868\uff09</li> </ul>"},{"location":"writer/hdfswriter/#column","title":"column","text":"<p>\u5199\u5165\u6570\u636e\u7684\u5b57\u6bb5\uff0c\u4e0d\u652f\u6301\u5bf9\u90e8\u5206\u5217\u5199\u5165\u3002\u4e3a\u4e0ehive\u4e2d\u8868\u5173\u8054\uff0c\u9700\u8981\u6307\u5b9a\u8868\u4e2d\u6240\u6709\u5b57\u6bb5\u540d\u548c\u5b57\u6bb5\u7c7b\u578b\uff0c \u5176\u4e2d\uff1a<code>name</code> \u6307\u5b9a\u5b57\u6bb5\u540d\uff0c<code>type</code> \u6307\u5b9a\u5b57\u6bb5\u7c7b\u578b\u3002</p> <p>\u7528\u6237\u53ef\u4ee5\u6307\u5b9a <code>column</code> \u5b57\u6bb5\u4fe1\u606f\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": [\n    {\n      \"name\": \"userName\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"salary\",\n      \"type\": \"decimal(8,2)\"\n    }\n  ]\n}\n</code></pre> <p>\u5bf9\u4e8e\u6570\u636e\u7c7b\u578b\u662f <code>decimal</code> \u7c7b\u578b\u7684\uff0c\u9700\u8981\u6ce8\u610f\uff1a</p> <ol> <li>\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u7cbe\u5ea6\u548c\u5c0f\u6570\u4f4d\uff0c\u5219\u4f7f\u7528\u9ed8\u8ba4\u7684 <code>decimal(38,10)</code> \u8868\u793a</li> <li>\u5982\u679c\u4ec5\u6307\u5b9a\u4e86\u7cbe\u5ea6\u4f46\u672a\u6307\u5b9a\u5c0f\u6570\u4f4d\uff0c\u5219\u5c0f\u6570\u4f4d\u75280\u8868\u793a\uff0c\u5373 <code>decimal(p,0)</code></li> <li>\u5982\u679c\u90fd\u6307\u5b9a\uff0c\u5219\u4f7f\u7528\u6307\u5b9a\u7684\u89c4\u683c\uff0c\u5373 <code>decimal(p,s)</code></li> </ol> <p>\u4ece <code>5.0.1</code> \u5f00\u59cb\uff0c\u5df2\u7ecf\u652f\u6301 <code>array</code>, <code>map</code> \u4e24\u79cd\u590d\u5408\u7c7b\u578b\uff0c\u4e0a\u8ff0\u793a\u4f8b\u914d\u7f6e\u6587\u4ef6\u7ed9\u51fa\u7684\u4f7f\u7528\u65b9\u5f0f</p>"},{"location":"writer/hdfswriter/#writemode","title":"writeMode","text":"<p>\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ul> <li>append\uff0c\u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u4f7f\u7528 <code>filename</code> \u5199\u5165\uff0c\u5e76\u4fdd\u8bc1\u6587\u4ef6\u540d\u4e0d\u51b2\u7a81\u3002</li> <li>overwrite \u5982\u679c\u5199\u5165\u76ee\u5f55\u5b58\u5728\u6570\u636e\uff0c\u5219\u5148\u5220\u9664\uff0c\u540e\u5199\u5165</li> <li>nonConflict\uff0c\u5982\u679c\u76ee\u5f55\u4e0b\u6709 <code>fileName</code> \u524d\u7f00\u7684\u6587\u4ef6\uff0c\u76f4\u63a5\u62a5\u9519\u3002</li> </ul>"},{"location":"writer/hdfswriter/#skiptrash","title":"skipTrash","text":"<p>\u5f53 <code>writeMode</code> \u4e3a <code>overwrite</code> \u6a21\u5f0f\u65f6\uff0c\u5f53\u524d\u8981\u5220\u9664\u7684\u6587\u4ef6\u6216\u6587\u4ef6\u5939\u662f\u5426\u8fdb\u5165\u56de\u6536\u7ad9\uff0c\u9ed8\u8ba4\u4e3a\u8fdb\u56de\u6536\u7ad9\uff0c\u4ec5\u5f53\u914d\u7f6e\u4e3a <code>true</code> \u65f6\u4e3a\u76f4\u63a5\u5220\u9664\u3002</p> <p>\u8be5\u529f\u80fd\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e3a\u83b7\u53d6 Hadoop HDFS \u7684  <code>fs.trash.interval</code> \u53c2\u6570\uff0c\u5982\u679c\u8be5\u53c2\u6570\u6ca1\u6709\u8bbe\u7f6e\uff0c\u6216\u8bbe\u7f6e\u4e3a0\u65f6\uff0c\u4f1a\u5728\u5220\u9664\u65f6\uff0c\u8bbe\u7f6e\u8be5\u53c2\u6570\u4e3a 10080 \uff0c\u8868\u793a 7 \u5929\u3002</p> <p>\u8fd9\u6837\uff0c\u8fdb\u5165\u56de\u6536\u7ad9\u7684\u6587\u4ef6\u4f1a\u4fdd\u75597\u5929\u3002</p> <p>\u4fee\u6539\u5220\u9664\u7684\u9ed8\u8ba4\u884c\u4e3a\u662f\u4e3a\u4e86\u7ed9\u56e0\u4e3a\u9519\u8bef\u7684\u91c7\u96c6\u800c\u5bfc\u81f4\u5220\u9664\u7684\u6570\u636e\u6709\u633d\u56de\u7684\u673a\u4f1a\u3002</p>"},{"location":"writer/hdfswriter/#compress","title":"compress","text":"<p>\u5f53 fileType\uff08\u6587\u4ef6\u7c7b\u578b\uff09\u4e3a csv \u4e0b\u7684\u6587\u4ef6\u538b\u7f29\u65b9\u5f0f\uff0c\u76ee\u524d\u4ec5\u652f\u6301 gzip\u3001bz2\u3001zip\u3001lzo\u3001lzo_deflate\u3001hadoop-snappy\u3001framing-snappy \u538b\u7f29\uff1b \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0clzo \u5b58\u5728\u4e24\u79cd\u538b\u7f29\u683c\u5f0f\uff1alzo \u548c lzo_deflate\uff0c\u7528\u6237\u5728\u914d\u7f6e\u7684\u65f6\u5019\u9700\u8981\u7559\u5fc3\uff0c\u4e0d\u8981\u914d\u9519\u4e86\uff1b</p> <p>\u53e6\u5916\uff0c\u7531\u4e8e snappy \u76ee\u524d\u6ca1\u6709\u7edf\u4e00\u7684 stream format\uff0caddax \u76ee\u524d\u53ea\u652f\u6301\u6700\u4e3b\u6d41\u7684\u4e24\u79cd\uff1ahadoop-snappy\uff08hadoop \u4e0a\u7684 snappy stream format\uff09 \u548c framing-snappy\uff08google \u5efa\u8bae\u7684 snappy stream format\uff09;</p>"},{"location":"writer/hdfswriter/#hadoopconfig","title":"hadoopConfig","text":"<p><code>hadoopConfig</code> \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Hadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e</p> <pre><code>{\n  \"hadoopConfig\": {\n    \"dfs.nameservices\": \"cluster\",\n    \"dfs.ha.namenodes.cluster\": \"nn1,nn2\",\n    \"dfs.namenode.rpc-address.cluster.nn1\": \"node1.example.com:8020\",\n    \"dfs.namenode.rpc-address.cluster.nn2\": \"node2.example.com:8020\",\n    \"dfs.client.failover.proxy.provider.cluster\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"\n  }\n}\n</code></pre> <p>\u8fd9\u91cc\u7684 <code>cluster</code> \u8868\u793a HDFS \u914d\u7f6e\u6210HA\u65f6\u7684\u540d\u5b57\uff0c\u4e5f\u662f <code>defaultFS</code> \u914d\u7f6e\u9879\u4e2d\u7684\u540d\u5b57 \u5982\u679c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u540d\u5b57\u4e0d\u662f <code>cluster</code> \uff0c\u5219\u4e0a\u8ff0\u914d\u7f6e\u4e2d\u6240\u6709\u5199\u6709 <code>cluster</code> \u90fd\u9700\u8981\u66ff\u6362</p>"},{"location":"writer/hdfswriter/#preshell-postshell","title":"preShell \u4e0e postShell","text":"<p>\u5f15\u5165 <code>preShell</code> \u4e0e <code>postShell</code> \u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5728\u5199\u5165\u6570\u636e\u524d\u540e\u6267\u884c\u4e00\u4e9b\u989d\u5916\u7684\u64cd\u4f5c\uff0c\u6bd4\u5982\u5728\u5199\u5165\u6570\u636e\u524d\u6e05\u7a7a\u8868\uff0c\u5199\u5165\u6570\u636e\u540e\u67e5\u8be2\u8868\u7684\u884c\u6570\u7b49\u3002\u4e00\u4e2a\u5178\u578b\u7684\u751f\u4ea7\u73af\u5883\u573a\u666f\u65f6\uff0c\u91c7\u96c6\u7684\u6570\u636e\u6309\u65e5\u5206\u533a\u4fdd\u5b58\u5728 HDFS \u4e0a\uff0c \u91c7\u96c6\u4e4b\u524d\u9700\u8981\u521b\u5efa\u5206\u533a\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e <code>preShell</code> \u6765\u5b9e\u73b0\uff0c\u6bd4\u5982 <code>hive -e \"alter table test.hello add partition(dt='${logdate}')\"</code></p>"},{"location":"writer/hdfswriter/#ignoreerror","title":"ignoreError","text":"<p>\u8be5\u914d\u7f6e\u9879\u7528\u4e8e\u63a7\u5236\u662f\u5426\u5ffd\u7565 <code>preShell</code> \u548c <code>postShell</code> \u547d\u4ee4\u7684\u9519\u8bef\uff0c\u5982\u679c\u914d\u7f6e\u4e3a <code>true</code>\uff0c\u5219\u5728\u6267\u884c <code>preShell</code> \u548c <code>postShell</code> \u547d\u4ee4\u65f6\uff0c\u5982\u679c\u547d\u4ee4\u6267\u884c\u5931\u8d25\uff0c\u4e0d\u4f1a\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\uff0c\u800c\u662f\u4f1a\u6253\u5370\u9519\u8bef\u65e5\u5fd7\uff0c\u7ee7\u7eed\u6267\u884c\u4efb\u52a1\u3002 \u5426\u5219\uff0c\u5982\u679c\u914d\u7f6e\u4e3a <code>false</code>\uff0c\u5219\u5728\u6267\u884c <code>preShell</code> \u548c <code>postShell</code> \u547d\u4ee4\u65f6\uff0c\u5982\u679c\u547d\u4ee4\u6267\u884c\u5931\u8d25\uff0c\u4f1a\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002</p>"},{"location":"writer/hdfswriter/#hdfssitepath","title":"hdfsSitePath","text":"<p>\u8fd9\u662f <code>4.2.4</code> \u5f15\u5165\u7684\u65b0\u914d\u7f6e\u60f3\uff0c\u7528\u4e8e\u6307\u5b9a <code>hdfs-site.xml</code> \u6587\u4ef6\u7684\u8def\u5f84\uff0c\u6bd4\u5982\u5bf9 HDP/CDH \u800c\u8a00\uff0c\u53ef\u4ee5\u8fd9\u6837\u914d\u7f6e\uff1a</p> <pre><code>{\n  \"hdfsSitePath\": \"/etc/hadoop/conf/hdfs-site.xml\"\n}\n</code></pre> <p>\u5982\u679c\u914d\u7f6e\u4e86 <code>hdfsSitePath</code> , \u5219\u63d2\u4ef6\u4f1a\u4ece\u8be5\u6587\u4ef6\u4e2d\u83b7\u5f97\u8bbf\u95ee HDFS \u6587\u4ef6\u7cfb\u7edf\u5fc5\u8981\u7684\u914d\u7f6e\uff0c\u4ece\u800c\u5728\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u4e0d\u5728\u9700\u8981\u914d\u7f6e <code>hadoopConfig</code>\uff0c\u51cf\u5c11\u914d\u7f6e\u91cf\u3002</p> <p>\u5bf9\u4e8e\u628a Addax \u90e8\u7f72\u5728 Hadoop \u96c6\u7fa4\u4e0a\u7684\u573a\u666f\uff0c\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002</p>"},{"location":"writer/hdfswriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b HIVE \u6570\u636e\u7c7b\u578b Long TINYINT,SMALLINT,INT,INTEGER,BIGINT Double FLOAT,DOUBLE,DECIMAL String STRING,VARCHAR,CHAR Boolean BOOLEAN Date DATE,TIMESTAMP Bytes BINARY String ARRAY, MAP"},{"location":"writer/hdfswriter/#_4","title":"\u529f\u80fd\u4e0e\u9650\u5236","text":"<ol> <li>\u76ee\u524d\u4e0d\u652f\u6301\uff1a<code>structs</code>\u3001<code>union</code> \u7c7b\u578b</li> </ol>"},{"location":"writer/icebergwriter/","title":"Iceberg Writer","text":"<p>Iceberg Writer \u63d0\u4f9b\u5411 \u5df2\u6709\u7684iceberg\u8868\u5199\u5165\u6570\u636e\u7684\u80fd\u529b\u3002</p>"},{"location":"writer/icebergwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"streamreader\",\n          \"parameter\": {\n            \"column\": [\n              {\n                \"value\": \"1\",\n                \"type\": \"long\"\n              },\n              {\n                \"value\": \"1989-06-04 00:00:00\",\n                \"type\": \"timestamp\"\n              },\n              {\n                \"value\": \"test1\",\n                \"type\": \"string\"\n              }\n            ],\n            \"sliceRecordCount\": 1000\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test.test1\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\": \"hadoop\",\n            \"warehouse\": \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/iceberg\",\n            \"hadoopConfig\": {\n              \"fs.s3a.endpoint\": \"http://localhost:9000\",\n              \"fs.s3a.access.key\": \"gy0dX5lALP176g6c9fYf\",\n              \"fs.s3a.secret.key\": \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\",\n              \"fs.s3a.connection.ssl.enabled\": \"false\",\n              \"fs.s3a.path.style.access\": \"true\",\n              \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/icebergwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e tableName \u662f string \u65e0 \u8981\u5199\u5165\u7684iceberg\u8868\u540d catalogType \u662f string \u65e0 catalog\u7c7b\u578b, \u76ee\u524d\u652f\u6301 hive,hadoop warehouse \u662f string \u65e0 \u4ed3\u5e93\u5730\u5740 writeMode \u662f string \u65e0 \u5199\u5165\u6a21\u5f0f\uff0c\u8be6\u8ff0\u89c1\u4e0b hadoopConfig \u662f json {} \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Iceberg catalog\u548cHadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e"},{"location":"writer/icebergwriter/#writemode","title":"writeMode","text":"<p>\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ul> <li>append\uff0c\u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u5199\u5165\uff0c\u4e0d\u6e05\u9664\u539f\u6765\u7684\u6570\u636e\u3002</li> <li>truncate \u5199\u5165\u524d\u5148\u6e05\u7a7a\u8868\uff0c\u518d\u5199\u5165\u3002</li> </ul>"},{"location":"writer/icebergwriter/#hadoopconfig","title":"hadoopConfig","text":"<p><code>hadoopConfig</code> \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Iceberg catalog\u548cHadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e</p> <p>\u521b\u5efa\u8868\u5b9e\u4f8b:</p> <p>\u4f9d\u8d56\u5305\u8bbe\u7f6e:</p> <p>build.gradle</p> <pre><code>plugins {\n    id 'java'\n}\n\ngroup = 'com.awol2005ex'\nversion = '1.0-SNAPSHOT'\next[\"hadoop_version\"] = \"3.2.4\"\next[\"hive_version\"] = \"3.1.3\"\next[\"woodstox_version\"] = \"7.0.0\"\next[\"iceberg_version\"] = \"1.8.0\"\nrepositories {\n    maven { url \"https://maven.aliyun.com/repository/central\" }\n    maven { url \"https://maven.aliyun.com/repository/public\" }\n    maven {\n        url 'https://repo.huaweicloud.com/repository/maven/'\n    }\n    maven {\n        url 'https://repo.spring.io/libs-milestone/'\n    }\n\n\n    maven {\n        url 'https://repo.spring.io/libs-snapshot'\n    }\n    mavenCentral()\n}\n\ndependencies {\n    testImplementation platform('org.junit:junit-bom:5.10.0')\n    testImplementation 'org.junit.jupiter:junit-jupiter'\n    implementation(\"org.apache.hadoop:hadoop-common:${hadoop_version}\") {\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-databind'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-core-asl'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-mapper-asl'\n        exclude group: 'com.fasterxml.woodstox', module: 'woodstox-core'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'commons-net', module: 'commons-net'\n        exclude group: 'io.netty', module: 'netty'\n        exclude group: 'log4j', module: 'log4j'\n        exclude group: 'net.minidev', module: 'json-smart'\n        exclude group: 'org.codehaus.jettison', module: 'jettison'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-server'\n        exclude group: 'org.xerial.snappy', module: 'snappy-java'\n        exclude group: 'org.apache.zookeeper', module: 'zookeeper'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-util'\n    }\n    implementation(\"org.apache.hadoop:hadoop-aws:${hadoop_version}\") {\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-databind'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-core-asl'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-mapper-asl'\n        exclude group: 'com.fasterxml.woodstox', module: 'woodstox-core'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'commons-net', module: 'commons-net'\n        exclude group: 'io.netty', module: 'netty'\n        exclude group: 'log4j', module: 'log4j'\n        exclude group: 'net.minidev', module: 'json-smart'\n        exclude group: 'org.codehaus.jettison', module: 'jettison'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-server'\n        exclude group: 'org.xerial.snappy', module: 'snappy-java'\n        exclude group: 'org.apache.zookeeper', module: 'zookeeper'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-util'\n    }\n    implementation(\"org.apache.hadoop:hadoop-mapreduce-client-core:${hadoop_version}\") {\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-databind'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-core-asl'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-mapper-asl'\n        exclude group: 'com.fasterxml.woodstox', module: 'woodstox-core'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'commons-net', module: 'commons-net'\n        exclude group: 'io.netty', module: 'netty'\n        exclude group: 'log4j', module: 'log4j'\n        exclude group: 'net.minidev', module: 'json-smart'\n        exclude group: 'org.codehaus.jettison', module: 'jettison'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-server'\n        exclude group: 'org.xerial.snappy', module: 'snappy-java'\n        exclude group: 'org.apache.zookeeper', module: 'zookeeper'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-util'\n    }\n    implementation(\"org.apache.hive:hive-metastore:${hive_version}\"){\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-databind'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-core-asl'\n        exclude group: 'org.codehaus.jackson', module: 'jackson-mapper-asl'\n        exclude group: 'com.fasterxml.woodstox', module: 'woodstox-core'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'commons-net', module: 'commons-net'\n        exclude group: 'io.netty', module: 'netty'\n        exclude group: 'log4j', module: 'log4j'\n        exclude group: 'net.minidev', module: 'json-smart'\n        exclude group: 'org.codehaus.jettison', module: 'jettison'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-server'\n        exclude group: 'org.xerial.snappy', module: 'snappy-java'\n        exclude group: 'org.apache.zookeeper', module: 'zookeeper'\n        exclude group: 'org.eclipse.jetty', module: 'jetty-util'\n    }\n    implementation(\"com.fasterxml.woodstox:woodstox-core:${woodstox_version}\")\n\n    implementation(\"org.apache.iceberg:iceberg-common:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-api:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-arrow:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-aws:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-core:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-parquet:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-orc:${iceberg_version}\")\n    implementation(\"org.apache.iceberg:iceberg-hive-metastore:${iceberg_version}\")\n}\n\ntest {\n    useJUnitPlatform()\n}\n</code></pre> <p>\u521b\u5efa\u5b58\u50a8\u5728minio,catalogType\u662fhadoop\u7684iceberg\u8868</p> <pre><code>package com.test;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.iceberg.hadoop.HadoopCatalog;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.catalog.TableIdentifier;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.types.Types;\n\nimport java.io.IOException;\n\npublic class CreateMinioTable {\n    public static void main(String[] args) throws IOException {\n\n        Configuration hadoopConf = new Configuration();\n        \"fs.s3a.endpoint\", \"http://localhost:9000\");\n        \"fs.s3a.access.key\", \"gy0dX5lALP176g6c9fYf\");\n        \"fs.s3a.secret.key\", \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\");\n        \"fs.s3a.connection.ssl.enabled\", \"false\");\n        \"fs.s3a.path.style.access\", \"true\");\n        \"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\");\n        String warehousePath = \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/iceberg\";\n        HadoopCatalog catalog = new HadoopCatalog(hadoopConf, warehousePath);\n\n        TableIdentifier name = TableIdentifier.of(\"test\", \"test1\");\n\n        Schema schema = new Schema(\n                Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n                Types.NestedField.required(2, \"ts1\", Types.TimestampType.withoutZone()),\n                Types.NestedField.required(3, \"name\", Types.StringType.get())\n        );\n        Table table = catalog.createTable(name, schema);\n        System.out.println(table.location());\n\n        catalog.close();\n    }\n}\n</code></pre> <p>\u521b\u5efa\u5b58\u50a8\u5728hdfs,catalogType\u662fhadoop\u7684iceberg\u8868 <pre><code>package com.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.catalog.TableIdentifier;\nimport org.apache.iceberg.hadoop.HadoopCatalog;\nimport org.apache.iceberg.types.Types;\n\nimport java.io.IOException;\n\npublic class CreateHdfsTable {\n    public static void main(String[] args) throws IOException {\n\n        System.setProperty(\"java.security.krb5.conf\",\"D:/MIT/krb5.ini\");\n\n        Configuration hadoopConf = new Configuration();\n        \"fs.defaultFS\", \"hdfs://nameservice1\");\n        \"hadoop.security.authentication\", \"kerberos\");\n        \"hadoop.kerberos.principal\", \"hive/_HOST@XXX.COM\");\n        \"hadoop.kerberos.keytab\", \"/tmp/hive@XXX.COM.keytab\");\n        \"ha.zookeeper.quorum\", \"nn1:2181,nn2:2181,nn3:2181\");\n        \"dfs.nameservices\", \"nameservice1\");\n        \"dfs.namenode.rpc-address.nameservice1.namenode371\", \"nn2:8020\");\n        \"dfs.namenode.rpc-address.nameservice1.namenode265\", \"nn1:8020\");\n        \"dfs.namenode.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.namenode.keytab.enabled\", \"true\");\n        \"dfs.namenode.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n        \"dfs.namenode.kerberos.internal.spnego.principal\", \"HTTP/_HOST@XXX.COM\");\n        \"dfs.ha.namenodes.nameservice1\", \"namenode265,namenode371\");\n        \"dfs.datanode.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.datanode.keytab.enabled\", \"true\");\n        \"dfs.datanode.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n        \"dfs.client.use.datanode.hostname\", \"false\");\n        \"dfs.client.failover.proxy.provider.nameservice1\", \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\");\n        \"dfs.balancer.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.balancer.keytab.enabled\", \"true\");\n        \"dfs.balancer.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n\n        UserGroupInformation.setConfiguration(hadoopConf);\n        try {\n            UserGroupInformation.loginUserFromKeytab(\"hive@XXX.COM\", \"/tmp/hive@XXX.COM.keytab\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n\n\n        String warehousePath = \"hdfs://nameservice1/user/hive/iceberg\";\n        HadoopCatalog catalog = new HadoopCatalog(hadoopConf, warehousePath);\n\n        TableIdentifier name = TableIdentifier.of(\"test1\", \"test20250219\");\n\n        Schema schema = new Schema(\n                Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n                Types.NestedField.required(2, \"ts1\", Types.TimestampType.withoutZone()),\n                Types.NestedField.required(3, \"dec1\", Types.DecimalType.of(12,2)),\n                Types.NestedField.required(4, \"bool1\", Types.BooleanType.get()),\n                Types.NestedField.required(5, \"map1\", Types.MapType.ofRequired(11,12,Types.StringType.get(),Types.StringType.get())),\n                Types.NestedField.required(6, \"date1\", Types.DateType.get()),\n                Types.NestedField.required(7, \"float1\", Types.FloatType.get()),\n                Types.NestedField.required(8, \"double1\", Types.DoubleType.get()),\n                Types.NestedField.required(9, \"array1\", Types.ListType.ofRequired(13,Types.StringType.get())),\n                Types.NestedField.required(10, \"name\", Types.StringType.get())\n        );\n        catalog.dropTable(name,true);\n        Table table = catalog.createTable(name, schema);\n        System.out.println(table.location());\n\n        catalog.close();\n    }\n}\n</code></pre> \u521b\u5efahive\u8868</p> <pre><code>package com.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.catalog.TableIdentifier;\nimport org.apache.iceberg.hive.HiveCatalog;\nimport org.apache.iceberg.types.Types;\n\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class CreateHiveTable {\n    public static void main(String[] args) throws IOException {\n\n        System.setProperty(\"java.security.krb5.conf\",\"D:/MIT/krb5.ini\");\n\n        Configuration hadoopConf = new Configuration();\n        \"fs.defaultFS\", \"hdfs,//nameservice1\");\n        \"hadoop.security.authentication\", \"kerberos\");\n        \"hadoop.kerberos.principal\", \"hive/_HOST@XXX.COM\");\n        \"hadoop.kerberos.keytab\", \"/tmp/hive@XXX.COM.keytab\");\n        \"ha.zookeeper.quorum\", \"nn1:2181,nn2:2181,nn3:2181\");\n        \"dfs.nameservices\", \"nameservice1\");\n        \"dfs.namenode.rpc-address.nameservice1.namenode371\", \"nn2:8020\");\n        \"dfs.namenode.rpc-address.nameservice1.namenode265\", \"nn1:8020\");\n        \"dfs.namenode.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.namenode.keytab.enabled\", \"true\");\n        \"dfs.namenode.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n        \"dfs.namenode.kerberos.internal.spnego.principal\", \"HTTP/_HOST@XXX.COM\");\n        \"dfs.ha.namenodes.nameservice1\", \"namenode265,namenode371\");\n        \"dfs.datanode.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.datanode.keytab.enabled\", \"true\");\n        \"dfs.datanode.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n        \"dfs.client.use.datanode.hostname\", \"false\");\n        \"dfs.client.failover.proxy.provider.nameservice1\", \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\");\n        \"dfs.balancer.keytab.file\", \"/tmp/hdfs@XXX.COM.keytab\");\n        \"dfs.balancer.keytab.enabled\", \"true\");\n        \"dfs.balancer.kerberos.principal\", \"hdfs/_HOST@XXX.COM\");\n\n        \"hive.metastore.uris\", \"thrift://nn1:9083,thrift://nn2:9083\");\n        \"hive.server2.authentication\",\"kerberos\");\n        \"hive.metastore.kerberos.principal\",\"hive/_HOST@XXX.COM\");\n\n        \"hive.metastore.sasl.enabled\", \"true\");\n\n        UserGroupInformation.setConfiguration(hadoopConf);\n        try {\n            UserGroupInformation.loginUserFromKeytab(\"hive@XXX.COM\", \"/tmp/hive@XXX.COM.keytab\");\n        } catch (Exception e) {\n            e.printStackTrace();;\n        }\n\n\n\n\n        HiveCatalog catalog = new HiveCatalog();\n        catalog.setConf(hadoopConf);\n        Map&lt;String, String&gt; properties = new HashMap&lt;String, String&gt;();\n        properties.put(\"warehouse\", \"/warehouse/tablespace/managed/hive\");\n        properties.put(\"uri\", \"thrift://nn1:9083,thrift://nn2:9083\");\n\n        catalog.initialize(\"hive\", properties);\n\n        TableIdentifier name = TableIdentifier.of(\"test1\", \"test20250218\");\n\n        Schema schema = new Schema(\n                Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n                Types.NestedField.required(2, \"ts1\", Types.TimestampType.withoutZone()),\n                Types.NestedField.required(3, \"name\", Types.StringType.get())\n        );\n        Table table = catalog.createTable(name, schema);\n        System.out.println(table.location());\n\n        catalog.close();\n    }\n}\n</code></pre> <p>Spark \u6216\u8005 flink \u73af\u5883\u521b\u5efa\u8868</p> <pre><code>CREATE TABLE if not exists test1.test1_iceberg1 USING ICEBERG \n  TBLPROPERTIES(\n     'format-version'='2',\n     'write.metadata.delete-after-commit.enabled'=true,\n      'write.metadata.previous-versions-max'=1,\n     'target-file-size-bytes'=268435456\n  )\n  as select * from test1.test1 limit 0;\n</code></pre> <p>s3 \u6216\u8005 minio hadoop catalog\u4f8b\u5b50 <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id  ,now() ts1,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\n                  \"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\"\n                ]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test.test1\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\":\"hadoop\",\n            \"warehouse\": \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/iceberg\",\n            \"hadoopConfig\": {\n\n              \"fs.s3a.endpoint\": \"http://localhost:9000\",\n              \"fs.s3a.access.key\": \"gy0dX5lALP176g6c9fYf\",\n              \"fs.s3a.secret.key\": \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\",\n              \"fs.s3a.connection.ssl.enabled\": \"false\",\n              \"fs.s3a.path.style.access\": \"true\",\n              \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre></p> <p>hdfs hadoop catalog\u4f8b\u5b50</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id  ,now() ts1,CAST(1.2 AS DECIMAL(12,2)) dec1,true bool1,'{\\\"a\\\":\\\"1\\\"}' map1,now() date1,1.3 float1,1.4 double1,'a,b,c' array1,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\n                  \"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\"\n                ]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test1.test20250219\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\": \"hadoop\",\n            \"warehouse\": \"hdfs://nameservice1/user/hive/iceberg\",\n            \"kerberosKeytabFilePath\":\"/tmp/hive@XXX.COM.keytab\",\n            \"kerberosPrincipal\":\"hive@XXX.COM\",\n            \"hadoopConfig\": {\n              \"fs.defaultFS\": \"hdfs://nameservice1\",\n              \"hadoop.security.authentication\": \"kerberos\",\n              \"hadoop.kerberos.principal\": \"hive/_HOST@XXX.COM\",\n              \"hadoop.kerberos.keytab\": \"/tmp/hive@XXX.COM.keytab\",\n              \"ha.zookeeper.quorum\": \"nn1:2181,nn2:2181,nn3:2181\",\n              \"dfs.nameservices\": \"nameservice1\",\n              \"dfs.namenode.rpc-address.nameservice1.namenode371\": \"nn2:8020\",\n              \"dfs.namenode.rpc-address.nameservice1.namenode265\": \"nn1:8020\",\n              \"dfs.namenode.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.namenode.keytab.enabled\": \"true\",\n              \"dfs.namenode.kerberos.principal\": \"hdfs/_HOST@XXX.COM\",\n              \"dfs.namenode.kerberos.internal.spnego.principal\": \"HTTP/_HOST@XXX.COM\",\n              \"dfs.ha.namenodes.nameservice1\": \"namenode265,namenode371\",\n              \"dfs.datanode.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.datanode.keytab.enabled\": \"true\",\n              \"dfs.datanode.kerberos.principal\": \"hdfs/_HOST@XXX.COM\",\n              \"dfs.client.use.datanode.hostname\": \"false\",\n              \"dfs.client.failover.proxy.provider.nameservice1\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n              \"dfs.balancer.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.balancer.keytab.enabled\": \"true\",\n              \"dfs.balancer.kerberos.principal\": \"hdfs/_HOST@XXX.COM\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>hive catalog\u4f8b\u5b50</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id  ,now() ts1,CAST(1.2 AS DECIMAL(12,2)) dec1,true bool1,'{\\\"a\\\":\\\"1\\\"}' map1,now() date1,1.3 float1,1.4 double1,'a,b,c' array1,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\n                  \"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\"\n                ]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test1.test20250219\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\": \"hive\",\n            \"uri\": \"thrift://nn1:9083,thrift://nn2:9083\",\n            \"warehouse\": \"/warehouse/tablespace/managed/hive\",\n            \"kerberosKeytabFilePath\":\"/tmp/hive@XXX.COM.keytab\",\n            \"kerberosPrincipal\":\"hive@XXX.COM\",\n            \"hadoopConfig\": {\n              \"fs.defaultFS\": \"hdfs://nameservice1\",\n              \"hadoop.security.authentication\": \"kerberos\",\n              \"hadoop.kerberos.principal\": \"hive/_HOST@XXX.COM\",\n              \"hadoop.kerberos.keytab\": \"/tmp/hive@XXX.COM.keytab\",\n              \"ha.zookeeper.quorum\": \"nn1:2181,nn2:2181,nn3:2181\",\n              \"dfs.nameservices\": \"nameservice1\",\n              \"dfs.namenode.rpc-address.nameservice1.namenode371\": \"nn2:8020\",\n              \"dfs.namenode.rpc-address.nameservice1.namenode265\": \"nn1:8020\",\n              \"dfs.namenode.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.namenode.keytab.enabled\": \"true\",\n              \"dfs.namenode.kerberos.principal\": \"hdfs/_HOST@XXX.COM\",\n              \"dfs.namenode.kerberos.internal.spnego.principal\": \"HTTP/_HOST@XXX.COM\",\n              \"dfs.ha.namenodes.nameservice1\": \"namenode265,namenode371\",\n              \"dfs.datanode.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.datanode.keytab.enabled\": \"true\",\n              \"dfs.datanode.kerberos.principal\": \"hdfs/_HOST@XXX.COM\",\n              \"dfs.client.use.datanode.hostname\": \"false\",\n              \"dfs.client.failover.proxy.provider.nameservice1\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n              \"dfs.balancer.keytab.file\": \"/tmp/hdfs@XXX.COM.keytab\",\n              \"dfs.balancer.keytab.enabled\": \"true\",\n              \"dfs.balancer.kerberos.principal\": \"hdfs/_HOST@XXX.COM\",\n              \"hive.metastore.uris\":\"thrift://nn1:9083,thrift://nn2:9083\",\n              \"hive.server2.authentication\":\"kerberos\",\n              \"hive.metastore.kerberos.principal\":\"hive/_HOST@XXX.COM\",\n              \"hive.metastore.sasl.enabled\":\"true\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/icebergwriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b Iceberg \u6570\u636e\u7c7b\u578b Integer INTEGER Long LONG Double DOUBLE Float FLOAT Decimal DECIMAL String STRING Boolean BOOLEAN Date DATE TIMESTAMP TIMESTAMP Bytes BINARY STRING(\u9017\u53f7\u5206\u9694\u5982'a,b,c') ARRAY STRING(json\u683c\u5f0f\u5982'{\"a\":\"1\"}') MAP"},{"location":"writer/icebergwriter/#_4","title":"\u63d2\u4ef6\u6784\u5efa","text":"<pre><code>set JAVA_HOME=E:\\jdk\\openlogic-openjdk-17.0.13+11-windows-x64\nmvn package install -Pdefault -Piceberg   -pl plugin/writer/icebergwriter\n</code></pre>"},{"location":"writer/influxdb2writer/","title":"InfluxDB2 Writer","text":"<p>InfluxDB2 Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u5199\u5165 InfluxDB 2.0 \u53ca\u4ee5\u4e0a\u7248\u672c\u7684\u6570\u636e\u5e93\u7684\u529f\u80fd\u3002</p> <p>\u6ce8\u610f\uff0c\u5982\u679c\u4f60\u7684 InfluxDB \u662f 1.8 \u53ca\u4ee5\u4e0b\u7248\u672c\uff0c\u5219\u5e94\u8be5\u4f7f\u7528 InfluxDBWriter \u63d2\u4ef6</p>"},{"location":"writer/influxdb2writer/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u7528\u6765\u6f14\u793a\u8be5\u63d2\u4ef6\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\u5e76\u5199\u5165\u5230\u6307\u5b9a\u8868</p>"},{"location":"writer/influxdb2writer/#job","title":"\u521b\u5efa job \u6587\u4ef6","text":"<p>\u521b\u5efa <code>job/stream2influx2.json</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/stream2influx2.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"incr\": \"2021-10-17 22:40:00,1,s\",\n              \"type\": \"date\"\n            },\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"influxdb2writer\",\n        \"parameter\": {\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"org\": \"com.wgzhao\",\n            \"bucket\": \"test\",\n            \"table\": \"addax_tbl\"\n          },\n          \"token\": \"YOUR_SECURE_TOKEN\",\n          \"tag\": [\n            {\n              \"location\": \"east\"\n            },\n            {\n              \"lat\": \"23.123445\"\n            }\n          ],\n          \"interval\": \"ms\",\n          \"column\": [\n            \"c_long\",\n            \"c_string\",\n            \"c_double\"\n          ],\n          \"batchSize\": 1024\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/influxdb2writer/#_2","title":"\u8fd0\u884c","text":"<p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2influx2.json\n</code></pre>"},{"location":"writer/influxdb2writer/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 InfluxDB \u8fde\u63a5\u4e32 table \u662f string \u65e0 \u8981\u5199\u5165\u7684\u8868\uff08\u6307\u6807\uff09 org \u662f string \u65e0 \u6307\u5b9a InfluxDB \u7684 org \u540d\u79f0 bucket \u662f string \u65e0 \u6307\u5b9a InfluxDB \u7684 bucket \u540d\u79f0 token \u662f string \u65e0 \u8bbf\u95ee\u6570\u636e\u5e93\u7684 token column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408 tag \u5426 <code>list&lt;map&gt;</code> \u65e0 \u8981\u6307\u5b9a\u7684 tag interval \u5426 string ms \u6307\u5b9a\u65f6\u95f4\u95f4\u9694\uff0c\u53ef\u4ee5\u6307\u5b9a <code>s</code>,<code>ms</code>,<code>us</code>, <code>ns</code> batchSize \u5426 int 1024 \u6279\u91cf\u5199\u5165\u7684\u5927\u5c0f"},{"location":"writer/influxdb2writer/#column","title":"column","text":"<p>InfluxDB \u4f5c\u4e3a\u65f6\u5e8f\u6570\u636e\u5e93\uff0c\u9700\u8981\u6bcf\u6761\u8bb0\u5f55\u90fd\u6709\u65f6\u95f4\u6233\u5b57\u6bb5\uff0c\u56e0\u6b64\u4f1a\u628a\u6bcf\u6761\u7684\u8bb0\u5f55\u7684\u7b2c\u4e00\u4e2a\u5b57\u6bb5\u5f53\u4f5c\u65f6\u95f4\u6233\u6765\u5904\u7406\u3002 <code>column</code> \u53ea\u9700\u8981\u6307\u5b9a\u9664\u4e86\u7b2c\u4e00\u4e2a\u5b57\u6bb5\u5916\u7684\u5176\u4ed6\u5b57\u6bb5\u3002 \u6bd4\u5982\u793a\u4f8b\u4e2d\uff0c<code>streamreader</code> \u8bbe\u7f6e\u4e864\u4e2a\u5b57\u6bb5\uff0c\u4f46\u5728 <code>influxdb2writer</code> \u4e2d\u7684 <code>column</code> \u53ea\u6307\u5b9a\u4e86\u4e09\u4e2a\u5b57\u6bb5\uff0c\u5c31\u662f\u56e0\u4e3a\u7b2c\u4e00\u4e2a\u5b57\u6bb5\u5df2\u7ecf\u9ed8\u8ba4\u4f5c\u4e3a\u65f6\u95f4\u6233\u4e86\u3002</p>"},{"location":"writer/influxdb2writer/#tag","title":"tag","text":"<p>\u7528\u4e8e\u6307\u5b9a\u6307\u6807\uff08\u8fd9\u91cc\u5f53\u4f5c\u8868\uff09\u7684 \u6807\u7b7e\uff0c\u6bcf\u4e2a tag \u4f7f\u7528 map \u65b9\u5f0f\u6307\u5b9a\uff0c\u6bd4\u5982\u793a\u4f8b\u4e2d\uff1a</p> <pre><code>{\n  \"tag\": [\n    {\n      \"location\": \"east\"\n    },\n    {\n      \"lat\": 23.123445\n    }\n  ]\n}\n</code></pre> <p>map\u4e2d\u7684 key \u8868\u793a\u6807\u7b7e\u7684\u540d\u79f0\uff0cvalue \u8868\u793a\u6807\u7b7e\u503c</p>"},{"location":"writer/influxdb2writer/#interval","title":"interval","text":"<p>\u8bbe\u7f6e\u65f6\u95f4\u6233\u7684\u95f4\u9694\u9891\u7387\uff0c\u8be5\u5b57\u6bb5\u7684\u5b9a\u4e49\u6765\u6e90\u4e8e influxdb-client-java \u4e2d\u7684 WritePrecision.java  \u5176\u5b57\u7b26\u4e32\u8868\u8fbe\u7684\u542b\u4e49\u5206\u522b\u4e3a\uff1a</p> <ul> <li>s : \u79d2</li> <li>ms : \u6beb\u79d2</li> <li>us : \u5fae\u79d2</li> <li>ns : \u7eb3\u79d2</li> </ul>"},{"location":"writer/influxdb2writer/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u5f53\u524d\u652f\u6301 InfluxDB 2.0 \u7684\u57fa\u672c\u7c7b\u578b</p>"},{"location":"writer/influxdbwriter/","title":"InfluxDB Writer","text":"<p>InfluxDB Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u5199\u5165 InfluxDB \u8bfb\u53d6\u6570\u636e\u7684\u529f\u80fd\u3002 \u5e95\u5c42\u5b9e\u73b0\u4e0a\uff0c\u662f\u901a\u8fc7\u8c03\u7528 InfluQL \u8bed\u8a00\u63a5\u53e3\uff0c\u6784\u5efa\u63d2\u5165\u8bed\u53e5\uff0c\u7136\u540e\u8fdb\u884c\u6570\u636e\u63d2\u5165\u3002</p>"},{"location":"writer/influxdbwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u7528\u6765\u6f14\u793a\u8be5\u63d2\u4ef6\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\u5e76\u5199\u5165\u5230\u6307\u5b9a\u8868</p>"},{"location":"writer/influxdbwriter/#_2","title":"\u521b\u5efa\u9700\u8981\u7684\u5e93","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u521b\u5efa\u9700\u8981\u5199\u5165\u7684\u5e93</p> <pre><code># create database\ninflux --execute \"CREATE DATABASE addax\"\n</code></pre>"},{"location":"writer/influxdbwriter/#job","title":"\u521b\u5efa job \u6587\u4ef6","text":"<p>\u521b\u5efa <code>job/stream2influxdb.json</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/stream2influxdb.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"2001-01-01 00:00:00, 2016-07-07 23:59:59\",\n              \"type\": \"date\"\n            },\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"influxdbwriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"addax\",\n            \"table\": \"addax_tbl\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\",\n          \"column\": [\n            {\n              \"name\": \"time\",\n              \"type\": \"timestamp\"\n            },\n            {\n              \"name\": \"user_id\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"user_name\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"salary\",\n              \"type\": \"double\"\n            }\n          ],\n          \"preSql\": [\n            \"delete from addax_tbl\"\n          ],\n          \"batchSize\": 1024,\n          \"retentionPolicy\": {\n            \"name\": \"one_day_only\",\n            \"duration\": \"1d\",\n            \"replication\": 1\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/influxdbwriter/#_3","title":"\u8fd0\u884c","text":"<p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2influxdb.json\n</code></pre>"},{"location":"writer/influxdbwriter/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 InfluxDB \u8fde\u63a5\u4e32 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 database \u662f string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7684\u6570\u636e\u5e93 table \u662f string \u65e0 \u8981\u5199\u5165\u7684\u8868\uff08\u6307\u6807\uff09 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408 connTimeout \u5426 int 15 \u8bbe\u7f6e\u8fde\u63a5\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 readTimeout \u5426 int 20 \u8bbe\u7f6e\u8bfb\u53d6\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 writeTimeout \u5426 int 20 \u8bbe\u7f6e\u5199\u5165\u8d85\u65f6\u503c\uff0c\u5355\u4f4d\u4e3a\u79d2 preSql \u5426 list \u65e0 \u63d2\u5165\u6570\u636e\u524d\u6267\u884c\u7684SQL\u8bed\u53e5 postSql \u5426 list \u65e0 \u6570\u636e\u63d2\u5165\u5b8c\u6bd5\u540e\u9700\u8981\u6267\u884c\u7684\u8bed\u53e5 retentionPolicy \u5426 map \u65e0 \u8bbe\u7f6e\u6570\u636e\u5e93\u7684 Retention Policy \u7b56\u7565"},{"location":"writer/influxdbwriter/#column","title":"column","text":"<p>InfluxDB \u4f5c\u4e3a\u65f6\u5e8f\u6570\u636e\u5e93\uff0c\u9700\u8981\u6bcf\u6761\u8bb0\u5f55\u90fd\u6709\u65f6\u95f4\u6233\u5b57\u6bb5\uff0c\u56e0\u6b64\u8fd9\u91cc\u4f1a\u628a <code>column</code> \u914d\u7f6e\u7684\u7b2c\u4e00\u4e2a\u5b57\u6bb5\u9ed8\u8ba4\u5f53\u4f5c\u65f6\u95f4\u6233</p>"},{"location":"writer/influxdbwriter/#retentionpolicy","title":"retentionPolicy","text":"<p>\u8bbe\u5b9a\u6570\u636e\u5e93\u7684 <code>Retention Policy</code> \u7b56\u7565\uff0c\u4f9d\u636e\u7ed9\u5b9a\u7684\u914d\u7f6e\uff0c\u5728\u6307\u5b9a\u6570\u636e\u5e93\u4e0a\u521b\u5efa\u4e00\u6761 <code>Retention Policy</code> \u4fe1\u606f\u3002 \u6709\u5173 <code>Retention Policy</code> \u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u53c2\u8003 [\u5b98\u65b9\u6587\u6863][2]</p>"},{"location":"writer/influxdbwriter/#_5","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u5f53\u524d\u652f\u6301 InfluxDB \u7684\u57fa\u672c\u7c7b\u578b</p>"},{"location":"writer/influxdbwriter/#_6","title":"\u9650\u5236","text":"<ol> <li>\u5f53\u524d\u63d2\u4ef6\u4ec5\u652f\u6301 1.x \u7248\u672c\uff0c2.0 \u53ca\u4ee5\u4e0a\u5e76\u4e0d\u652f\u6301</li> </ol>"},{"location":"writer/kafkawriter/","title":"Kafka Writer","text":"<p>Kafka Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u4ee5 json \u683c\u5f0f\u5199\u5165 Kafka \u7684\u529f\u80fd\u3002</p>"},{"location":"writer/kafkawriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u6f14\u793a\u4e86\u5982\u4f55\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\u5e76\u5199\u5165\u5230 kafka \u7684\u6307\u5b9a topic \u4e2d\u3002</p>"},{"location":"writer/kafkawriter/#_2","title":"\u521b\u5efa\u4efb\u52a1\u6587\u4ef6","text":"<p>\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u4efb\u52a1\u6587\u4ef6  <code>stream2kafka.json</code> , \u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n        \"speed\": {\n            \"channel\": 1\n        }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n            \"name\": \"streamreader\",\n            \"parameter\": {\n              \"column\": [\n                    {\"random\": \"10,1000\", \"type\": \"long\"},\n                    {\"value\": \"1.1.1.1\", \"type\": \"string\"},\n                    {\"value\": 19890604.0, \"type\": \"double\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": \"hello world\", \"type\": \"string\"},\n                    {\"value\": \"long text\", \"type\": \"string\"},\n                    {\"value\": \"41.12,-71.34\", \"type\": \"string\"},\n                    {\"value\": \"2017-05-25 11:22:33\", \"type\": \"string\"}\n                    ],\n            \"sliceRecordCount\": 100\n            }\n        },\n        \"writer\": {\n          \"name\": \"kafkawriter\",\n          \"parameter\": {\n            \"brokerList\": \"localhost:9092\",\n            \"topic\": \"test-1\",\n            \"partitions\": 0,\n            \"batchSize\": 1000,\n            \"column\": [\"col1\", \"col2\",\"col3\",\"col4\",\"col5\", \"col6\", \"col7\", \"col8\", \"col9\"]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/kafkawriter/#_3","title":"\u8fd0\u884c","text":"<p>\u6267\u884c  <code>bin/addax.sh stream2kafka.json</code> \u547d\u4ee4\uff0c\u83b7\u5f97\u7c7b\u4f3c\u4e0b\u9762\u7684\u8f93\u51fa\uff1a</p> <pre><code>2022-02-26 21:59:22.975 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2022-02-26 21:59:22.985 [        main] INFO  Engine               - \n{\n    \"content\":{\n        \"reader\":{\n            \"parameter\":{\n                \"column\":[\n                    {\n                        \"random\":\"10,1000\",\n                        \"type\":\"long\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"1.1.1.1\"\n                    },\n                    {\n                        \"type\":\"double\",\n                        \"value\":19890604.0\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"hello world\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"long text\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"41.12,-71.34\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"2017-05-25 11:22:33\"\n                    }\n                ],\n                \"sliceRecordCount\":100\n            },\n            \"name\":\"streamreader\"\n        },\n        \"writer\":{\n            \"parameter\":{\n                \"partitions\":0,\n                \"column\":[\n                    \"col1\",\n                    \"col2\",\n                    \"col3\",\n                    \"col4\",\n                    \"col5\",\n                    \"col6\",\n                    \"col7\",\n                    \"col8\",\n                    \"col9\"\n                ],\n                \"topic\":\"test-1\",\n                \"batchSize\":1000,\n                \"brokerList\":\"localhost:9092\"\n            },\n            \"name\":\"kafkawriter\"\n        }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"channel\":1\n        }\n    }\n}\n\n2022-02-26 21:59:23.002 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2022-02-26 21:59:23.003 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2022-02-26 21:59:23.004 [        main] INFO  JobContainer         - Set jobId = 0\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channel(s).\n2022-02-26 21:59:23.018 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2022-02-26 21:59:23.019 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] splits to [1] tasks.\n2022-02-26 21:59:23.039 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2022-02-26 21:59:23.047 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2022-02-26 21:59:23.082 [0-0-0-writer] INFO  ProducerConfig       - ProducerConfig values: \n    acks = 1\n    batch.size = 1000\n    bootstrap.servers = [localhost:9092]\n    buffer.memory = 33554432\n    client.id = addax-kafka-writer\n    compression.type = none\n    connections.max.idle.ms = 540000\n    enable.idempotence = false\n    interceptor.classes = []\n    key.serializer = class org.apache.kafka.common.serialization.StringSerializer\n    linger.ms = 0\n    max.block.ms = 60000\n    max.in.flight.requests.per.connection = 5\n    max.request.size = 1048576\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n    partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\n    receive.buffer.bytes = 32768\n    reconnect.backoff.max.ms = 1000\n    reconnect.backoff.ms = 50\n    request.timeout.ms = 30000\n    retries = 0\n    retry.backoff.ms = 100\n    sasl.client.callback.handler.class = null\n    sasl.jaas.config = null\n    sasl.kerberos.kinit.cmd = /usr/bin/kinit\n    sasl.kerberos.min.time.before.relogin = 60000\n    sasl.kerberos.service.name = null\n    sasl.kerberos.ticket.renew.jitter = 0.05\n    sasl.kerberos.ticket.renew.window.factor = 0.8\n    sasl.login.callback.handler.class = null\n    sasl.login.class = null\n    sasl.login.refresh.buffer.seconds = 300\n    sasl.login.refresh.min.period.seconds = 60\n    sasl.login.refresh.window.factor = 0.8\n    sasl.login.refresh.window.jitter = 0.05\n    sasl.mechanism = GSSAPI\n    security.protocol = PLAINTEXT\n    send.buffer.bytes = 131072\n    ssl.cipher.suites = null\n    ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n    ssl.endpoint.identification.algorithm = https\n    ssl.key.password = null\n    ssl.keymanager.algorithm = SunX509\n    ssl.keystore.location = null\n    ssl.keystore.password = null\n    ssl.keystore.type = JKS\n    ssl.protocol = TLS\n    ssl.provider = null\n    ssl.secure.random.implementation = null\n    ssl.trustmanager.algorithm = PKIX\n    ssl.truststore.location = null\n    ssl.truststore.password = null\n    ssl.truststore.type = JKS\n    transaction.timeout.ms = 60000\n    transactional.id = null\n    value.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\n2022-02-26 21:59:23.412 [0-0-0-writer] INFO  AppInfoParser        - Kafka version : 2.0.0\n2022-02-26 21:59:23.413 [0-0-0-writer] INFO  AppInfoParser        - Kafka commitId : 3402a8361b734732\n2022-02-26 21:59:23.534 [kafka-producer-network-thread | addax-kafka-writer] INFO  Metadata             - Cluster ID: xPAQZFNDTp6y63nZO4LACA\n2022-02-26 21:59:26.061 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do post work.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2022-02-26 21:59:26.063 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2022-02-26 21:59:26.064 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 100 records, 9200 bytes | Speed 2.99KB/s, 33 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2022-02-26 21:59:26.065 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2022-02-26 21:59:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2022-02-26 21:59:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.99KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre> <p>\u6211\u4eec\u4f7f\u7528 kafka \u81ea\u5e26\u7684 <code>kafka-console-consumer.sh</code> \u5c1d\u8bd5\u8bfb\u53d6\u6570\u636e\uff0c\u8f93\u51fa\u5982\u4e0b\uff1a</p> <pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-1 --from-beginning\n\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":916}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":572}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":88}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":33}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":697}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":381}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":304}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":103}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":967}\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":147}\n</code></pre>"},{"location":"writer/kafkawriter/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 brokerList \u662f string \u65e0 \u8fde\u63a5 kafka \u670d\u52a1\u7684 broker \u914d\u7f6e\uff0c\u591a\u4e2a broker\u4e4b\u95f4\u7528\u9017\u53f7(<code>,</code>)\u5206\u9694 topic \u662f string \u65e0 \u8981\u5199\u5165\u7684 topic batchSize \u5426 int 1204 \u8bbe\u7f6e Kafka \u7684 <code>batch.size</code> \u53c2\u6570 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u4e0d\u5141\u8bb8\u4e3a <code>*</code> properties \u5426 map \u65e0 \u9700\u8981\u8bbe\u7f6e\u7684\u5176\u4ed6 kafka \u8fde\u63a5\u53c2\u6570"},{"location":"writer/kafkawriter/#_5","title":"\u9650\u5236","text":"<ol> <li>\u4ec5\u652f\u6301 Kafka <code>1.0</code> \u53ca\u4ee5\u4e0a\u7248\u672c\uff0c\u4f4e\u4e8e\u8be5\u7248\u672c\u7684\u65e0\u6cd5\u786e\u5b9a\u662f\u5426\u80fd\u5199\u5165</li> <li>\u5f53\u524d\u4e0d\u652f\u6301\u542f\u7528\u4e86 <code>kerberos</code> \u8ba4\u8bc1\u7684 kafka \u670d\u52a1</li> </ol>"},{"location":"writer/kuduwriter/","title":"Kudu Writer","text":"<p>Kudu Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u5199\u5165\u5230 kudu \u7684\u80fd\u529b\uff0c\u5f53\u524d\u662f\u901a\u8fc7\u8c03\u7528\u539f\u751fRPC\u63a5\u53e3\u6765\u5b9e\u73b0\u7684\u3002 \u540e\u671f\u5e0c\u671b\u901a\u8fc7 impala \u63a5\u53e3\u5b9e\u73b0\uff0c\u4ece\u800c\u589e\u52a0\u66f4\u591a\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/kuduwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\u4ece\u5185\u5b58\u8bfb\u53d6\u6837\u4f8b\u6570\u636e\u5e76\u5199\u5165\u5230 kudu \u8868\u4e2d\u7684\u3002</p>"},{"location":"writer/kuduwriter/#_2","title":"\u8868\u7ed3\u6784","text":"<p>\u6211\u4eec\u7528 trino \u5de5\u5177\u8fde\u63a5\u5230 kudu \u670d\u52a1\uff0c\u7136\u540e\u901a\u8fc7\u4e0b\u9762\u7684 SQL \u8bed\u53e5\u521b\u5efa\u8868</p> <pre><code>CREATE TABLE kudu.default.users (\n  user_id int WITH (primary_key = true),\n  user_name varchar,\n  salary double\n) WITH (\n  partition_by_hash_columns = ARRAY['user_id'],\n  partition_by_hash_buckets = 2\n);\n</code></pre>"},{"location":"writer/kuduwriter/#job","title":"job \u914d\u7f6e\u6587\u4ef6","text":"<p>\u521b\u5efa <code>job/stream2kudu.json</code> \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> job/stream2kudu.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"kuduwriter\",\n        \"parameter\": {\n          \"masterAddress\": \"127.0.0.1:7051,127.0.0.1:7151,127.0.0.1:7251\",\n          \"timeout\": 60,\n          \"table\": \"users\",\n          \"writeMode\": \"upsert\",\n          \"column\": [ \"user_id\", \"user_name\", \"salary\"],\n          \"batchSize\": 1024,\n          \"bufferSize\": 2048,\n          \"skipFail\": false,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/kuduwriter/#_3","title":"\u8fd0\u884c","text":"<p>\u6267\u884c\u4e0b\u4e0b\u9762\u7684\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2kudu.json\n</code></pre>"},{"location":"writer/kuduwriter/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 masterAddress \u662f string \u65e0 Kudu Master\u96c6\u7fa4RPC\u5730\u5740,\u591a\u4e2a\u5730\u5740\u7528\u9017\u53f7(,)\u5206\u9694 table \u662f string \u65e0 kudu \u8868\u540d writeMode \u5426 string upsert \u8868\u6570\u636e\u5199\u5165\u6a21\u5f0f\uff0c\u652f\u6301 upsert, insert \u4e24\u8005 timeout \u5426 int 100 \u5199\u5165\u6570\u636e\u8d85\u65f6\u65f6\u95f4(\u79d2), 0 \u8868\u793a\u4e0d\u53d7\u9650\u5236 column \u662f list \u65e0 \u8981\u5199\u5165\u7684\u8868\u5b57\u6bb5\uff0c\u914d\u7f6e\u65b9\u5f0f\u89c1\u4e0a\u793a\u4f8b skipFail \u5426 boolean false \u662f\u5426\u8df3\u8fc7\u63d2\u5165\u5931\u8d25\u7684\u8bb0\u5f55\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3atrue\uff0c\u5219\u63d2\u4ef6\u4e0d\u4f1a\u628a\u63d2\u5165\u5931\u8d25\u7684\u5f53\u4f5c\u5f02\u5e38 haveKerberos \u5426 boolean false \u662f\u5426\u542f\u7528 Kerberos \u8ba4\u8bc1\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u9700\u8981\u540c\u65f6\u914d\u7f6e\u4ee5\u4e0b\u4e24\u9879 kerberosKeytabFilePath \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u6587\u4ef6\u8def\u5f84, \u6bd4\u5982 <code>/your/path/addax.service.keytab</code> kerberosPrincipal \u5426 string \u65e0 \u7528\u4e8e Kerberos \u8ba4\u8bc1\u7684\u51ed\u8bc1\u4e3b\u4f53, \u6bd4\u5982 <code>addax/node1@WGZHAO.COM</code>"},{"location":"writer/kuduwriter/#column","title":"column","text":"<p><code>column</code> \u53ef\u4ee5\u76f4\u63a5\u6307\u5b9a\u8981\u5199\u5165\u7684\u5217\uff0c\u5982\u540c\u4e0a\u8ff0\u4f8b\u5b50\uff0c\u4e5f\u53ef\u4ee5\u8bbe\u7f6e <code>[\"*\"]</code> \u6765\u8868\u793a\u5199\u5165\u6240\u6709\u5217\u3002</p>"},{"location":"writer/mongodbwriter/","title":"MongoDB Writer","text":"<p>MongoDB Writer \u63d2\u4ef6\u7528\u4e8e\u5411 MongoDB \u5199\u5165\u6570\u636e\u3002</p>"},{"location":"writer/mongodbwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u8be5\u793a\u4f8b\u5c06\u6d41\u5f0f\u6570\u636e\u5199\u5165\u5230 MongoDB \u8868\u4e2d</p> job/stream2mongo.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"unique_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"sid\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"user_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"auction_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"content_type\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"pool_type\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"a1 a2 a3\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"c1 c2 c3\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"2020-09-06\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"tag1 tag2 tag3\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"property\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 1984,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 1900,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 75,\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"mongodbwriter\",\n        \"parameter\": {\n          \"username\": \"my_user\",\n          \"password\": \"password123\",\n          \"column\": [\n            {\n              \"name\": \"unique_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"sid\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"user_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"auction_id\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"content_type\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"pool_type\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"frontcat_id\",\n              \"type\": \"Array\",\n              \"splitter\": \" \"\n            },\n            {\n              \"name\": \"categoryid\",\n              \"type\": \"Array\",\n              \"splitter\": \" \"\n            },\n            {\n              \"name\": \"gmt_create\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"taglist\",\n              \"type\": \"Array\",\n              \"splitter\": \" \"\n            },\n            {\n              \"name\": \"property\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"scorea\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"scoreb\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"scorec\",\n              \"type\": \"int\"\n            }\n          ],\n          \"writeMode\": \"insert\",\n          \"connection\": {\n            \"address\": [\n              \"127.0.0.1:27017\"\n            ],\n            \"database\": \"my_database\",\n            \"collection\": \"addax_writer\",\n            \"authDb\": \"my_database\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/mongodbwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 address \u662f list \u65e0 MongoDB \u7684\u6570\u636e\u5730\u5740\u4fe1\u606f username \u5426 string \u65e0 MongoDB \u7684\u7528\u6237\u540d password \u5426 string \u65e0 MongoDB \u7684\u5bc6\u7801 collection \u662f string \u65e0 MongoDB \u7684\u96c6\u5408\u540d column \u662f <code>list&lt;map&gt;</code> \u65e0 MongoDB \u7684\u6587\u6863\u5217\u540d splitter \u5426 string \u65e0 \u7279\u6b8a\u5206\u9694\u7b26\uff0c\u8be6\u89c1\u4e0b\u6587 writeMode \u5426 string insert \u6307\u5b9a\u4e86\u4f20\u8f93\u6570\u636e\u65f6\u66f4\u65b0\u7684\u4fe1\u606f,\u652f\u6301 insert\uff0c update \u4e24\u79cd batchSize \u5426 int 2048 \u6307\u5b9a\u6279\u6b21\u8f93\u5165\u7684\u6570\u91cf isUpsert \u5426 boolean \u65e0 \u5f53\u8bbe\u7f6e\u4e3a true \u65f6\uff0c\u8868\u793a\u9488\u5bf9\u76f8\u540c\u7684 upsertKey \u505a\u66f4\u65b0\u64cd\u4f5c upsertKey \u5426 string \u65e0 upsertKey \u6307\u5b9a\u4e86\u6ca1\u884c\u8bb0\u5f55\u7684\u4e1a\u52a1\u4e3b\u952e\u3002\u7528\u6765\u505a\u66f4\u65b0\u65f6\u4f7f\u7528"},{"location":"writer/mongodbwriter/#column","title":"column","text":"<p><code>column</code> \u6307\u5b9a mongo collection \u7684\u5b57\u6bb5\u4ee5\u53ca\u7c7b\u578b\uff0c\u5982\u679c\u662f\u6570\u7ec4\u7c7b\u578b\uff0c\u8fd8\u9700\u8981\u6307\u5b9a\u63a5\u6536\u5230\u7684\u6570\u636e\u6309\u7167\u4ec0\u4e48\u5206\u5272\uff0c\u4e00\u4e2a <code>column</code> \u5b57\u6bb5\u81f3\u5c11\u9700\u8981\u6307\u5b9a <code>name</code> \u4ee5\u53ca <code>type</code>\uff0c\u6bd4\u5982</p> <pre><code>{\n  \"column\": [\n    {\n      \"name\": \"user_id\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre> <p>\u5982\u679c\u662f\u6570\u7ec4\u7c7b\u578b\uff0c\u5219\u9700\u8981\u914d\u7f6e <code>splitter</code> \u6765\u544a\u77e5\u5206\u9694\u7b26\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"column\": {\n    \"name\": \"taglist\",\n    \"type\": \"Array\",\n    \"splitter\": \" \"\n  }\n}\n</code></pre>"},{"location":"writer/mongodbwriter/#splitter","title":"splitter","text":"<p>\u5f53\u4e14\u4ec5\u5f53\u8981\u5904\u7406\u7684\u5b57\u7b26\u4e32\u8981\u7528\u5206\u9694\u7b26\u5206\u9694\u4e3a\u5b57\u7b26\u6570\u7ec4\u65f6\uff0c\u624d\u4f7f\u7528\u8fd9\u4e2a\u53c2\u6570\uff0c\u901a\u8fc7\u8fd9\u4e2a\u53c2\u6570\u6307\u5b9a\u7684\u5206\u9694\u7b26\uff0c\u5c06\u5b57\u7b26\u4e32\u5206\u9694\u5b58\u50a8\u5230 MongoDB \u7684\u6570\u7ec4\u4e2d</p>"},{"location":"writer/mongodbwriter/#writemode","title":"writeMode","text":"<p>\u4e0d\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u9ed8\u8ba4\u91c7\u53d6\u76f4\u63a5\u63d2\u5165\u8bb0\u5f55\u7684\u65b9\u5f0f\uff0c\u5982\u679c\u5e0c\u671b\u5b9e\u73b0\u63d2\u5165\u66f4\u65b0\uff08\u5373\u8bb0\u5f55\u5b58\u5728\u5219\u66f4\u65b0\u5426\u5219\u63d2\u5165\uff09\uff0c\u53ef\u4ee5\u6307\u5b9a\u4e3a <code>update</code> \u6a21\u5f0f\uff0c\u8be5\u6a21\u5f0f\u4e0b\uff0c\u5fc5\u987b\u540c\u65f6\u66f4\u65b0\u7684\u5b57\u6bb5\u662f\u54ea\u4e2a\uff0c\u6bd4\u5982\uff1a</p> <pre><code>{\n  \"writeMode\": \"update(unique_id)\"\n}\n</code></pre> <p>\u4e0a\u8ff0\u914d\u7f6e\u8868\u793a\u4f9d\u636e\u5b57\u6bb5 <code>unique_id</code> \u6765\u51b3\u5b9a\u5f53\u524d\u8bb0\u5f55\u662f\u63d2\u5165\u8fd8\u662f\u66f4\u65b0\uff0c\u5f53\u524d\u6682\u4e0d\u652f\u6301\u6307\u5b9a\u591a\u4e2a\u5b57\u6bb5</p>"},{"location":"writer/mongodbwriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b MongoDB \u6570\u636e\u7c7b\u578b Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes"},{"location":"writer/mysqlwriter/","title":"MySQL Writer","text":"<p>MySQL Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 MySQL \u76ee\u7684\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/mysqlwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684 MySQL \u8868\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table test.addax_tbl\n(\n  col1 varchar(20) ,\n  col2 int(4),\n  col3 datetime,\n  col4 boolean,\n  col5 binary\n) default charset utf8;\n</code></pre> <p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 MySQL \u5bfc\u5165\u7684\u6570\u636e\u3002</p> job/stream2mysql.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"mysqlwriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"session\": [\n            \"set session sql_mode='ANSI'\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2mysql.json</code></p>"},{"location":"writer/mysqlwriter/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2mysql.json\n</code></pre>"},{"location":"writer/mysqlwriter/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u4e00\u4e9b MySQL \u7279\u6709\u7684\u914d\u7f6e\u9879\u3002</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 writeMode \u662f string insert \u6570\u636e\u5199\u5165\u8868\u7684\u65b9\u5f0f\uff0c\u8be6\u89c1\u4e0b\u6587 batchSize \u5426 int 1024 \u5b9a\u4e49\u4e86\u63d2\u4ef6\u548c\u6570\u636e\u5e93\u670d\u52a1\u5668\u7aef\u6bcf\u6b21\u6279\u91cf\u6570\u636e\u83b7\u53d6\u6761\u6570"},{"location":"writer/mysqlwriter/#driver","title":"driver","text":"<p>\u5f53\u524d\u91c7\u7528\u7684 MySQL JDBC \u9a71\u52a8\u4e3a 8.0 \u4ee5\u4e0a\u7248\u672c\uff0c\u9a71\u52a8\u7c7b\u540d\u4f7f\u7528\u7684 <code>com.mysql.cj.jdbc.Driver</code>\uff0c\u800c\u4e0d\u662f <code>com.mysql.jdbc.Driver</code>\u3002 \u5982\u679c\u4f60\u9700\u8981\u91c7\u96c6\u7684 MySQL \u670d\u52a1\u4f4e\u4e8e <code>5.6</code>\uff0c\u9700\u8981\u4f7f\u7528\u5230 <code>Connector/J 5.1</code> \u9a71\u52a8\uff0c\u5219\u53ef\u4ee5\u91c7\u53d6\u4e0b\u9762\u7684\u6b65\u9aa4\uff1a</p> <ol> <li> <p>\u66ff\u6362\u63d2\u4ef6\u5185\u7f6e\u7684\u9a71\u52a8   <code>rm -f plugin/writer/mysqlwriter/libs/mysql-connector-java-*.jar</code></p> </li> <li> <p>\u62f7\u8d1d\u8001\u7684\u9a71\u52a8\u5230\u63d2\u4ef6\u76ee\u5f55   <code>cp mysql-connector-java-5.1.48.jar plugin/writer/mysqlwriter/libs/</code></p> </li> <li> <p>\u6307\u5b9a\u9a71\u52a8\u7c7b\u540d\u79f0   \u5728\u4f60\u7684 json \u6587\u4ef6\u7c7b\uff0c\u914d\u7f6e <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p> </li> </ol>"},{"location":"writer/mysqlwriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> \u8868\u793a\u91c7\u7528 <code>insert into</code></li> <li><code>replace</code>\u8868\u793a\u91c7\u7528<code>replace into</code>\u65b9\u5f0f</li> <li><code>update</code> \u8868\u793a\u91c7\u7528 <code>ON DUPLICATE KEY UPDATE</code> \u8bed\u53e5</li> </ul>"},{"location":"writer/oraclewriter/","title":"Oracle Writer","text":"<p>Oracle Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 Oracle \u76ee\u7684\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/oraclewriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 Oracle \u5bfc\u5165\u7684\u6570\u636e\u3002</p> job/stream2oracle.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"oraclewriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"preSql\": [\n            \"delete from test\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/oraclewriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u4e9b OracleWriter \u7279\u6709\u7684\u914d\u7f6e\u9879\u3002</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u9ed8\u8ba4\u503c \u63cf\u8ff0 writeMode \u5426 insert \u5199\u5165\u65b9\u5f0f\uff0c\u652f\u6301 insert\uff0c update\uff0c\u8be6\u89c1\u4e0b\u6587"},{"location":"writer/oraclewriter/#writemode","title":"writeMode","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c \u91c7\u53d6 <code>insert into</code> \u8bed\u6cd5\u5199\u5165 Oracle \u8868\uff0c\u5982\u679c\u4f60\u5e0c\u671b\u91c7\u53d6\u4e3b\u952e\u5b58\u5728\u65f6\u66f4\u65b0\uff0c\u4e0d\u5b58\u5728\u5219\u5199\u5165\u7684\u65b9\u5f0f\uff0c\u4e5f\u5c31\u662f Oracle \u7684 <code>merge into</code> \u8bed\u6cd5\uff0c \u53ef\u4ee5\u4f7f\u7528 <code>update</code> \u6a21\u5f0f\u3002\u5047\u5b9a\u8868\u7684\u4e3b\u952e\u4e3a <code>id</code> ,\u5219 <code>writeMode</code> \u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>\u5982\u679c\u662f\u8054\u5408\u552f\u4e00\u7d22\u5f15\uff0c\u5219\u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre>"},{"location":"writer/paimonwriter/","title":"Paimon Writer","text":"<p>Paimon Writer \u63d0\u4f9b\u5411 \u5df2\u6709\u7684paimon\u8868\u5199\u5165\u6570\u636e\u7684\u80fd\u529b\u3002</p>"},{"location":"writer/paimonwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id ,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\",]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"paimonwriter\",\n          \"parameter\": {\n            \"dbName\": \"test\",\n            \"tableName\": \"test2\",\n            \"writeMode\": \"truncate\",\n            \"paimonConfig\": {\n              \"warehouse\": \"file:///g:/paimon\",\n              \"metastore\": \"filesystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/paimonwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e dbName \u662f string \u65e0 \u8981\u5199\u5165\u7684paimon\u6570\u636e\u5e93\u540d tableName \u662f string \u65e0 \u8981\u5199\u5165\u7684paimon\u8868\u540d writeMode \u662f string \u65e0 \u5199\u5165\u6a21\u5f0f\uff0c\u8be6\u8ff0\u89c1\u4e0b paimonConfig \u662f json {} \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Paimon catalog\u548cHadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e"},{"location":"writer/paimonwriter/#writemode","title":"writeMode","text":"<p>\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ul> <li>append\uff0c\u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u5199\u5165\uff0c\u4e0d\u6e05\u9664\u539f\u6765\u7684\u6570\u636e\u3002</li> <li>truncate \u5199\u5165\u524d\u5148\u6e05\u7a7a\u8868\uff0c\u518d\u5199\u5165\u3002</li> </ul>"},{"location":"writer/paimonwriter/#paimonconfig","title":"paimonConfig","text":"<p><code>paimonConfig</code> \u91cc\u53ef\u4ee5\u914d\u7f6e\u4e0e Paimon catalog\u548cHadoop \u76f8\u5173\u7684\u4e00\u4e9b\u9ad8\u7ea7\u53c2\u6570\uff0c\u6bd4\u5982HA\u7684\u914d\u7f6e</p> <p>\u672c\u5730\u76ee\u5f55\u521b\u5efapaimon\u8868</p> <p>pom.xml</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;com.test&lt;/groupId&gt;\n    &lt;artifactId&gt;paimon-java-api-test&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n        &lt;hadoop.version&gt;3.2.4&lt;/hadoop.version&gt;\n        &lt;woodstox.version&gt;7.0.0&lt;/woodstox.version&gt;\n    &lt;/properties&gt;\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.paimon&lt;/groupId&gt;\n        &lt;artifactId&gt;paimon-bundle&lt;/artifactId&gt;\n        &lt;version&gt;1.0.0&lt;/version&gt;\n    &lt;/dependency&gt;\n\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;\n        &lt;version&gt;${hadoop.version}&lt;/version&gt;\n        &lt;exclusions&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-core-asl&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;com.fasterxml.woodstox&lt;/groupId&gt;\n                &lt;artifactId&gt;woodstox-core&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;commons-codec&lt;/groupId&gt;\n                &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;commons-net&lt;/groupId&gt;\n                &lt;artifactId&gt;commons-net&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;io.netty&lt;/groupId&gt;\n                &lt;artifactId&gt;netty&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;log4j&lt;/groupId&gt;\n                &lt;artifactId&gt;log4j&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;net.minidev&lt;/groupId&gt;\n                &lt;artifactId&gt;json-smart&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jettison&lt;/groupId&gt;\n                &lt;artifactId&gt;jettison&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n                &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt;\n                &lt;artifactId&gt;snappy-java&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\n                &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n                &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n        &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;\n        &lt;version&gt;${hadoop.version}&lt;/version&gt;\n        &lt;exclusions&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-core-asl&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;com.fasterxml.woodstox&lt;/groupId&gt;\n                &lt;artifactId&gt;woodstox-core&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;commons-codec&lt;/groupId&gt;\n                &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;commons-net&lt;/groupId&gt;\n                &lt;artifactId&gt;commons-net&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;io.netty&lt;/groupId&gt;\n                &lt;artifactId&gt;netty&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;log4j&lt;/groupId&gt;\n                &lt;artifactId&gt;log4j&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;net.minidev&lt;/groupId&gt;\n                &lt;artifactId&gt;json-smart&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.codehaus.jettison&lt;/groupId&gt;\n                &lt;artifactId&gt;jettison&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n                &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt;\n                &lt;artifactId&gt;snappy-java&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\n                &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n                &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n        &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n        &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;\n        &lt;version&gt;${hadoop.version}&lt;/version&gt;\n        &lt;exclusions&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n                &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;commons-codec&lt;/groupId&gt;\n                &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;io.netty&lt;/groupId&gt;\n                &lt;artifactId&gt;netty&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;\n                &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n        &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.fasterxml.woodstox&lt;/groupId&gt;\n        &lt;artifactId&gt;woodstox-core&lt;/artifactId&gt;\n        &lt;version&gt;${woodstox.version}&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre> <pre><code>import org.apache.paimon.catalog.Catalog;\nimport org.apache.paimon.catalog.CatalogContext;\nimport org.apache.paimon.catalog.CatalogFactory;\nimport org.apache.paimon.catalog.Identifier;\nimport org.apache.paimon.fs.Path;\nimport org.apache.paimon.schema.Schema;\nimport org.apache.paimon.types.DataTypes;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class CreatePaimonTable {\n\n    public static Catalog createFilesystemCatalog() {\n        CatalogContext context = CatalogContext.create(new Path(\"file:///g:/paimon\"));\n        return CatalogFactory.createCatalog(context);\n    }\n    /* \u5982\u679c\u662fminio\u5219\u4f8b\u5b50\u5982\u4e0b\n\n     public static Catalog createFilesystemCatalog() {\n        Options options = new Options();\n        options.set(\"warehouse\", \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/paimon\");\n        Configuration hadoopConf = new Configuration();\n        hadoopConf.set(\"fs.s3a.endpoint\", \"http://localhost:9000\");\n        hadoopConf.set(\"fs.s3a.access.key\", \"gy0dX5lALP176g6c9fYf\");\n        hadoopConf.set(\"fs.s3a.secret.key\", \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\");\n        hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\");\n        hadoopConf.set(\"fs.s3a.path.style.access\", \"true\");\n        hadoopConf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\");\n        CatalogContext context = CatalogContext.create(options,hadoopConf);\n\n\n        return CatalogFactory.createCatalog(context);\n    }\n     * \n     * \n     * */\n\n    public static void main(String[] args) {\n        Schema.Builder schemaBuilder = Schema.newBuilder();\n        schemaBuilder.primaryKey(\"id\");\n        schemaBuilder.column(\"id\", DataTypes.INT());\n        schemaBuilder.column(\"name\", DataTypes.STRING());\n        Map&lt;String, String&gt; options = new HashMap&lt;&gt;();\n        options.put(\"bucket\", \"1\");//\u7531\u4e8epaimon java api \u9650\u5236\u9700\u8981bucket&gt;0\n        options.put(\"bucket-key\", \"id\");\n        options.put(\"file.format\", \"orc\");\n        options.put(\"file.compression\", \"lz4\");\n        options.put(\"lookup.cache-spill-compression\", \"lz4\");\n        options.put(\"spill-compression\", \"LZ4\");\n        options.put(\"orc.compress\", \"lz4\");\n        options.put(\"manifest.format\", \"orc\");\n\n        schemaBuilder.options(options);\n        Schema schema = schemaBuilder.build();\n\n        Identifier identifier = Identifier.create(\"test\", \"test2\");\n        try {\n            Catalog catalog = CreatePaimonTable.createFilesystemCatalog();\n            catalog.createDatabase(\"test\",true);\n            catalog.createTable(identifier, schema, true);\n        } catch (Catalog.TableAlreadyExistException e) {\n            e.printStackTrace();\n        } catch (Catalog.DatabaseNotExistException e) {\n            e.printStackTrace();\n        } catch (Catalog.DatabaseAlreadyExistException e) {\n            throw new RuntimeException(e);\n        }\n\n\n    }\n}\n</code></pre> <p>Spark \u6216\u8005 flink \u73af\u5883\u521b\u5efa\u8868</p> <pre><code>CREATE TABLE if not exists test.test2(id int ,name string)  tblproperties (\n    'primary-key' = 'id',\n    'bucket' = '1',\n    'bucket-key' = 'id'\n    'file.format'='orc',\n    'file.compression'='lz4',\n    'lookup.cache-spill-compression'='lz4',\n    'spill-compression'='LZ4',\n    'orc.compress'='lz4',\n    'manifest.format'='orc'\n)\n</code></pre> <p>\u672c\u5730\u6587\u4ef6\u4f8b\u5b50</p> <pre><code>{\n                    \"name\": \"paimonwriter\",\n                    \"parameter\": {\n                        \"dbName\": \"test\",\n                        \"tableName\": \"test2\",\n                        \"writeMode\": \"truncate\",\n                        \"paimonConfig\": {\n                           \"warehouse\": \"file:///g:/paimon\",\n                           \"metastore\": \"filesystem\"\n                         }\n                    }\n}\n</code></pre> <p>s3 \u6216\u8005 minio catalog\u4f8b\u5b50 <pre><code>{\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"channel\": 3\n            },\n            \"errorLimit\": {\n                \"record\": 0,\n                \"percentage\": 0\n            }\n        },\n        \"content\": [\n            {\n                \"reader\": {\n                    \"name\": \"rdbmsreader\",\n                    \"parameter\": {\n                        \"username\": \"root\",\n                        \"password\": \"root\",\n                        \"column\": [\n                            \"*\"\n                        ],\n                        \"connection\": [\n                            {\n                                \"querySql\": [\n                                    \"select 1+0 id ,'test1' as name\"\n                                ],\n                                \"jdbcUrl\": [\n                                    \"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\"\n                                ]\n                            }\n                        ],\n                        \"fetchSize\": 1024\n                    }\n                },\n                \"writer\": {\n                    \"name\": \"paimonwriter\",\n                    \"parameter\": {\n                        \"dbName\": \"test\",\n                        \"tableName\": \"test2\",\n                        \"writeMode\": \"truncate\",\n                        \"paimonConfig\": {\n                            \"warehouse\": \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/paimon\",\n                            \"metastore\": \"filesystem\",\n                            \"fs.s3a.endpoint\": \"http://localhost:9000\",\n                            \"fs.s3a.access.key\": \"gy0dX5lALP176g6c9fYf\",\n                            \"fs.s3a.secret.key\": \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\",\n                            \"fs.s3a.connection.ssl.enabled\": \"false\",\n                            \"fs.s3a.path.style.access\": \"true\",\n                            \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n                        }\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre></p> <p>hdfs catalog\u4f8b\u5b50</p> <pre><code>{\n  \"paimonConfig\": {\n    \"warehouse\": \"hdfs://nameservice1/user/hive/paimon\",\n    \"metastore\": \"filesystem\",\n    \"fs.defaultFS\":\"hdfs://nameservice1\",\n    \"hadoop.security.authentication\" : \"kerberos\",\n    \"hadoop.kerberos.principal\" : \"hive/_HOST@XXXX.COM\",\n    \"hadoop.kerberos.keytab\" : \"/tmp/hive@XXXX.COM.keytab\",\n    \"ha.zookeeper.quorum\" : \"test-pr-nn1:2181,test-pr-nn2:2181,test-pr-nn3:2181\",\n    \"dfs.nameservices\" : \"nameservice1\",\n    \"dfs.namenode.rpc-address.nameservice1.namenode371\" : \"test-pr-nn2:8020\",\n    \"dfs.namenode.rpc-address.nameservice1.namenode265\": \"test-pr-nn1:8020\",\n    \"dfs.namenode.keytab.file\" : \"/tmp/hdfs@XXXX.COM.keytab\",\n    \"dfs.namenode.keytab.enabled\" : \"true\",\n    \"dfs.namenode.kerberos.principal\" : \"hdfs/_HOST@XXXX.COM\",\n    \"dfs.namenode.kerberos.internal.spnego.principal\" : \"HTTP/_HOST@XXXX.COM\",\n    \"dfs.ha.namenodes.nameservice1\" : \"namenode265,namenode371\",\n    \"dfs.datanode.keytab.file\" : \"/tmp/hdfs@XXXX.COM.keytab\",\n    \"dfs.datanode.keytab.enabled\" : \"true\",\n    \"dfs.datanode.kerberos.principal\" : \"hdfs/_HOST@XXXX.COM\",\n    \"dfs.client.use.datanode.hostname\" : \"false\",\n    \"dfs.client.failover.proxy.provider.nameservice1\" : \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n    \"dfs.balancer.keytab.file\" : \"/tmp/hdfs@XXXX.COM.keytab\",\n    \"dfs.balancer.keytab.enabled\" : \"true\",\n    \"dfs.balancer.kerberos.principal\" : \"hdfs/_HOST@XXXX.COM\"\n  }\n}\n</code></pre>"},{"location":"writer/paimonwriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b Paimon \u6570\u636e\u7c7b\u578b Integer TINYINT,SMALLINT,INT,INTEGER Long BIGINT Double FLOAT,DOUBLE,DECIMAL String STRING,VARCHAR,CHAR Boolean BOOLEAN Date DATE,TIMESTAMP Bytes BINARY"},{"location":"writer/postgresqlwriter/","title":"Postgresql Writer","text":"<p>Postgresql Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 PostgreSQL \u6570\u636e\u5e93\u5e93\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/postgresqlwriter/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u914d\u7f6e\u6f14\u793a\u4ece postgresql \u6307\u5b9a\u7684\u8868\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u63d2\u5165\u5230\u5177\u6709\u76f8\u540c\u8868\u7ed3\u6784\u7684\u53e6\u5916\u4e00\u5f20\u8868\u4e2d\uff0c\u7528\u6765\u6d4b\u8bd5\u8be5\u63d2\u4ef6\u6240\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u3002</p>"},{"location":"writer/postgresqlwriter/#_2","title":"\u8868\u7ed3\u6784\u4fe1\u606f","text":"<p>\u5047\u5b9a\u5efa\u8868\u8bed\u53e5\u4ee5\u53ca\u8f93\u5165\u63d2\u5165\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>create table if not exists addax_tbl\n(\n    c_bigint bigint,\n    c_bit bit(3),\n    c_bool boolean,\n    c_byte bytea,\n    c_char char(10),\n    c_varchar varchar(20),\n    c_date date,\n    c_double float8,\n    c_int integer,\n    c_json json,\n    c_number decimal(8, 3),\n    c_real real,\n    c_small smallint,\n    c_text text,\n    c_ts timestamp,\n    c_uuid uuid,\n    c_xml xml,\n    c_money money,\n    c_inet inet,\n    c_cidr cidr,\n    c_macaddr macaddr\n    );\n\ninsert into addax_tbl\nvalues (999988887777,\n        b'101',\n        TRUE,\n        '\\xDEADBEEF',\n        'hello',\n        'hello, world',\n        '2021-01-04',\n        999888.9972,\n        9876542,\n        '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'::json,\n        12345.123,\n        123.123,\n        126,\n        'this is a long text ',\n        '2020-01-04 12:13:14',\n        'A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'::uuid,\n        '&lt;foo&gt;bar&lt;/foo&gt;'::xml,\n        '52093.89'::money,\n        '192.168.1.1'::inet,\n        '192.168.1/24'::cidr,\n        '08002b:010203'::macaddr);\n</code></pre> <p>\u521b\u5efa\u9700\u8981\u63d2\u5165\u7684\u8868\u7684\u8bed\u53e5\u5982\u4e0b:</p> <pre><code>create table addax_tbl1 as select * from  addax_tbl where 1=2;\n</code></pre>"},{"location":"writer/postgresqlwriter/#_3","title":"\u4efb\u52a1\u914d\u7f6e","text":"<p>\u4ee5\u4e0b\u662f\u914d\u7f6e\u6587\u4ef6</p> job/pg2pg.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/pg2pg.json</code></p>"},{"location":"writer/postgresqlwriter/#_4","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/pg2pg.json\n</code></pre>"},{"location":"writer/postgresqlwriter/#_5","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/postgresqlwriter/#writemode","title":"writeMode","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c \u91c7\u53d6 <code>insert into</code> \u8bed\u6cd5\u5199\u5165 postgresql \u8868\uff0c\u5982\u679c\u4f60\u5e0c\u671b\u91c7\u53d6\u4e3b\u952e\u5b58\u5728\u65f6\u66f4\u65b0\uff0c\u4e0d\u5b58\u5728\u5219\u5199\u5165\u7684\u65b9\u5f0f\uff0c \u53ef\u4ee5\u4f7f\u7528 <code>update</code> \u6a21\u5f0f\u3002\u5047\u5b9a\u8868\u7684\u4e3b\u952e\u4e3a <code>id</code> ,\u5219 <code>writeMode</code> \u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>\u5982\u679c\u662f\u8054\u5408\u552f\u4e00\u7d22\u5f15\uff0c\u5219\u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre> <p>\u6ce8\uff1a <code>update</code> \u6a21\u5f0f\u5728 <code>3.1.6</code> \u7248\u672c\u9996\u6b21\u589e\u52a0\uff0c\u4e4b\u524d\u7248\u672c\u5e76\u4e0d\u652f\u6301\u3002</p>"},{"location":"writer/postgresqlwriter/#_6","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u76ee\u524d PostgresqlWriter \u652f\u6301\u5927\u90e8\u5206 PostgreSQL \u7c7b\u578b\uff0c\u4f46\u4e5f\u5b58\u5728\u90e8\u5206\u6ca1\u6709\u652f\u6301\u7684\u60c5\u51b5\uff0c\u8bf7\u6ce8\u610f\u68c0\u67e5\u4f60\u7684\u7c7b\u578b\u3002</p> <p>\u4e0b\u9762\u5217\u51fa PostgresqlWriter \u9488\u5bf9 PostgreSQL \u7c7b\u578b\u8f6c\u6362\u5217\u8868:</p> Addax \u5185\u90e8\u7c7b\u578b PostgreSQL \u6570\u636e\u7c7b\u578b Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit, inet,cidr,macaddr,uuid,xml,json Date date, time, timestamp Boolean bool Bytes bytea"},{"location":"writer/postgresqlwriter/#_7","title":"\u5df2\u77e5\u9650\u5236","text":"<p>\u9664\u4ee5\u4e0a\u5217\u51fa\u7684\u6570\u636e\u7c7b\u578b\u5916\uff0c\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u7406\u8bba\u4e0a\u5747\u4e3a\u8f6c\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u4f46\u4e0d\u786e\u4fdd\u51c6\u786e\u6027</p>"},{"location":"writer/rdbmswriter/","title":"RDBMS Writer","text":"<p>RDBMS Writer \u63d2\u4ef6\u652f\u6301\u4ece\u4f20\u7edf RDBMS \u8bfb\u53d6\u6570\u636e\u3002\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u5173\u7cfb\u6570\u636e\u5e93\u8bfb\u53d6\u63d2\u4ef6\uff0c\u53ef\u4ee5\u901a\u8fc7\u6ce8\u518c\u6570\u636e\u5e93\u9a71\u52a8\u7b49\u65b9\u5f0f\u652f\u6301\u66f4\u591a\u5173\u7cfb\u6570\u636e\u5e93\u8bfb\u53d6\u3002</p> <p>\u540c\u65f6 RDBMS Writer \u53c8\u662f\u5176\u4ed6\u5173\u7cfb\u578b\u6570\u636e\u5e93\u8bfb\u53d6\u63d2\u4ef6\u7684\u7684\u57fa\u7840\u7c7b\u3002\u4ee5\u4e0b\u8bfb\u53d6\u63d2\u4ef6\u5747\u4f9d\u8d56\u8be5\u63d2\u4ef6</p> <ul> <li>Oracle Writer</li> <li>MySQL Writer</li> <li>PostgreSQL Writer</li> <li>ClickHouse Writer</li> <li>SQLServer Writer</li> <li>Access Writer</li> <li>Databend Writer</li> </ul> <p>\u6ce8\u610f, \u5982\u679c\u5df2\u7ecf\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u6570\u636e\u5e93\u5199\u5165\u63d2\u4ef6\u7684\uff0c\u63a8\u8350\u4f7f\u7528\u4e13\u7528\u63d2\u4ef6\uff0c\u5982\u679c\u4f60\u9700\u8981\u5199\u5165\u7684\u6570\u636e\u5e93\u6ca1\u6709\u4e13\u95e8\u63d2\u4ef6\uff0c\u5219\u8003\u8651\u4f7f\u7528\u8be5\u901a\u7528\u63d2\u4ef6\u3002 \u5728\u4f7f\u7528\u4e4b\u524d\uff0c\u8fd8\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u624d\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0c\u5426\u5219\u8fd0\u884c\u4f1a\u51fa\u73b0\u5f02\u5e38\u3002</p>"},{"location":"writer/rdbmswriter/#_1","title":"\u914d\u7f6e\u9a71\u52a8","text":"<p>\u5047\u5b9a\u4f60\u9700\u8981\u5199\u5165 IBM DB2 \u7684\u6570\u636e\uff0c\u56e0\u4e3a\u6ca1\u6709\u63d0\u4f9b\u4e13\u95e8\u7684\u8bfb\u53d6\u63d2\u4ef6\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8be5\u63d2\u4ef6\u6765\u5b9e\u73b0\uff0c\u5728\u4f7f\u7528\u4e4b\u524d\uff0c\u9700\u8981\u6267\u884c\u4e0b\u9762\u4e24\u4e2a\u64cd\u4f5c\uff1a</p> <ol> <li>\u4e0b\u8f7d\u5bf9\u5e94\u7684 JDBC \u9a71\u52a8\uff0c\u5e76\u62f7\u8d1d\u5230 <code>plugin/writer/rdbmswriter/libs</code> \u76ee\u5f55</li> <li>\u4fee\u6539\u4efb\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u627e\u5230 <code>driver</code> \u4e00\u9879\uff0c\u586b\u5199\u6b63\u786e\u7684 JDBC \u9a71\u52a8\u540d\uff0c\u6bd4\u5982 DB2 \u7684\u9a71\u52a8\u540d\u4e3a <code>com.ibm.db2.jcc.DB2Driver</code></li> </ol> <p>\u4ee5\u4e0b\u5217\u51fa\u5e38\u89c1\u7684\u6570\u636e\u5e93\u4ee5\u53ca\u5bf9\u5e94\u7684\u9a71\u52a8\u540d\u79f0</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"writer/rdbmswriter/#_2","title":"\u914d\u7f6e\u8bf4\u660e","text":"<p>\u914d\u7f6e\u4e00\u4e2a\u5199\u5165RDBMS\u7684\u4f5c\u4e1a\u3002</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"rdbmswriter\",\n        \"parameter\": {\n          \"username\": \"username\",\n          \"password\": \"password\",\n          \"driver\": \"dm.jdbc.driver.DmDriver\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from XXX;\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:dm://ip:port/database\",\n            \"table\": [\n              \"table\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/rdbmswriter/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 jdbcUrl \u662f string \u65e0 \u5bf9\u7aef\u6570\u636e\u5e93\u7684JDBC\u8fde\u63a5\u4fe1\u606f\uff0cjdbcUrl\u6309\u7167RDBMS\u5b98\u65b9\u89c4\u8303\uff0c\u5e76\u53ef\u4ee5\u586b\u5199\u8fde\u63a5\u9644\u4ef6\u63a7\u5236\u4fe1\u606f \uff5c driver \u662f string \u65e0 \u81ea\u5b9a\u4e49\u9a71\u52a8\u7c7b\u540d\uff0c\u89e3\u51b3\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u8be6\u89c1\u4e0b\u9762\u63cf\u8ff0 username \u662f string \u65e0 \u6570\u636e\u6e90\u7684\u7528\u6237\u540d password \u5426 string \u65e0 \u6570\u636e\u6e90\u6307\u5b9a\u7528\u6237\u540d\u7684\u5bc6\u7801 table \u662f list \u65e0 \u6240\u9009\u53d6\u7684\u9700\u8981\u540c\u6b65\u7684\u8868\u540d,\u4f7f\u7528JSON\u6570\u636e\u683c\u5f0f\uff0c\u5f53\u914d\u7f6e\u4e3a\u591a\u5f20\u8868\u65f6\uff0c\u7528\u6237\u81ea\u5df1\u9700\u4fdd\u8bc1\u591a\u5f20\u8868\u662f\u540c\u4e00\u8868\u7ed3\u6784 column \u662f list \u65e0 \u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u8be6\u7ec6\u63cf\u8ff0\u89c1\u540e preSql \u5426 list \u65e0 \u6267\u884c\u6570\u636e\u540c\u6b65\u4efb\u52a1\u4e4b\u524d\u7387\u5148\u6267\u884c\u7684sql\u8bed\u53e5\uff0c\u76ee\u524d\u53ea\u5141\u8bb8\u6267\u884c\u4e00\u6761SQL\u8bed\u53e5\uff0c\u4f8b\u5982\u6e05\u9664\u65e7\u6570\u636e,\u6d89\u53ca\u5230\u7684\u8868\u53ef\u7528 <code>@table</code>\u8868\u793a postSql \u5426 list \u65e0 \u6267\u884c\u6570\u636e\u540c\u6b65\u4efb\u52a1\u4e4b\u540e\u6267\u884c\u7684sql\u8bed\u53e5\uff0c\u76ee\u524d\u53ea\u5141\u8bb8\u6267\u884c\u4e00\u6761SQL\u8bed\u53e5\uff0c\u4f8b\u5982\u52a0\u4e0a\u67d0\u4e00\u4e2a\u65f6\u95f4\u6233 batchSize \u5426 int 1024 \u5b9a\u4e49\u4e86\u63d2\u4ef6\u548c\u6570\u636e\u5e93\u670d\u52a1\u5668\u7aef\u6bcf\u6b21\u6279\u91cf\u6570\u636e\u83b7\u53d6\u6761\u6570\uff0c\u8c03\u9ad8\u8be5\u503c\u53ef\u80fd\u5bfc\u81f4 Addax \u51fa\u73b0OOM\u6216\u8005\u76ee\u6807\u6570\u636e\u5e93\u4e8b\u52a1\u63d0\u4ea4\u5931\u8d25\u5bfc\u81f4\u6302\u8d77 session \u662f\u5426 list \u65e0 \u9488\u5bf9\u672c\u5730\u8fde\u63a5,\u4fee\u6539\u4f1a\u8bdd\u914d\u7f6e,\u8be6\u89c1\u4e0b\u6587"},{"location":"writer/rdbmswriter/#column","title":"column","text":"<p>\u6240\u914d\u7f6e\u7684\u8868\u4e2d\u9700\u8981\u540c\u6b65\u7684\u5217\u540d\u96c6\u5408\uff0c\u4f7f\u7528JSON\u7684\u6570\u7ec4\u63cf\u8ff0\u5b57\u6bb5\u4fe1\u606f\u3002\u7528\u6237\u4f7f\u7528 <code>*</code> \u4ee3\u8868\u9ed8\u8ba4\u4f7f\u7528\u6240\u6709\u5217\u914d\u7f6e\uff0c\u4f8b\u5982 <code>[\"*\"]</code>\u3002</p> <p>\u652f\u6301\u5217\u88c1\u526a\uff0c\u5373\u5217\u53ef\u4ee5\u6311\u9009\u90e8\u5206\u5217\u8fdb\u884c\u5bfc\u51fa\u3002</p> <p>\u652f\u6301\u5217\u6362\u5e8f\uff0c\u5373\u5217\u53ef\u4ee5\u4e0d\u6309\u7167\u8868schema\u4fe1\u606f\u8fdb\u884c\u5bfc\u51fa\u3002</p> <p>\u652f\u6301\u5e38\u91cf\u914d\u7f6e\uff0c\u7528\u6237\u9700\u8981\u6309\u7167JSON\u683c\u5f0f:</p> <p><code>[\"id\", \"`table`\", \"1\", \"'bazhen.csy'\", \"null\", \"to_char(a + 1)\", \"2.3\" , \"true\"]</code></p> <ul> <li><code>id</code> \u4e3a\u666e\u901a\u5217\u540d</li> <li><code>`table`</code> \u4e3a\u5305\u542b\u4fdd\u7559\u5728\u7684\u5217\u540d\uff0c</li> <li><code>1</code> \u4e3a\u6574\u5f62\u6570\u5b57\u5e38\u91cf\uff0c</li> <li><code>'bazhen.csy'</code>\u4e3a\u5b57\u7b26\u4e32\u5e38\u91cf</li> <li><code>null</code> \u4e3a\u7a7a\u6307\u9488\uff0c\u6ce8\u610f\uff0c\u8fd9\u91cc\u7684 <code>null</code> \u5fc5\u987b\u4ee5\u5b57\u7b26\u4e32\u5f62\u5f0f\u51fa\u73b0\uff0c\u5373\u7528\u53cc\u5f15\u53f7\u5f15\u7528</li> <li><code>to_char(a + 1)</code>\u4e3a\u8868\u8fbe\u5f0f\uff0c</li> <li><code>2.3</code> \u4e3a\u6d6e\u70b9\u6570\uff0c</li> <li><code>true</code> \u4e3a\u5e03\u5c14\u503c\uff0c\u540c\u6837\u7684\uff0c\u8fd9\u91cc\u7684\u5e03\u5c14\u503c\u4e5f\u5fc5\u987b\u7528\u53cc\u5f15\u53f7\u5f15\u7528</li> </ul> <p>Column\u5fc5\u987b\u663e\u793a\u586b\u5199\uff0c\u4e0d\u5141\u8bb8\u4e3a\u7a7a\uff01</p>"},{"location":"writer/rdbmswriter/#jdbcurl","title":"jdbcUrl","text":"<p><code>jdbcUrl</code> \u914d\u7f6e\u9664\u4e86\u914d\u7f6e\u5fc5\u8981\u7684\u4fe1\u606f\u5916\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u5728\u589e\u52a0\u6bcf\u79cd\u7279\u5b9a\u9a71\u52a8\u7684\u7279\u5b9a\u914d\u7f6e\u5c5e\u6027\uff0c\u8fd9\u91cc\u7279\u522b\u63d0\u5230\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u914d\u7f6e\u5c5e\u6027\u5bf9\u4ee3\u7406\u7684\u652f\u6301\u4ece\u800c\u5b9e\u73b0\u901a\u8fc7\u4ee3\u7406\u8bbf\u95ee\u6570\u636e\u5e93\u7684\u529f\u80fd\u3002  \u6bd4\u5982\u5bf9\u4e8e PrestoSQL \u6570\u636e\u5e93\u7684 JDBC \u9a71\u52a8\u800c\u8a00\uff0c\u652f\u6301 <code>socksProxy</code> \u53c2\u6570\uff0c\u6bd4\u5982\u4e00\u4e2a\u53ef\u80fd\u7684 <code>jdbcUrl</code> \u4e3a</p> <p><code>jdbc:presto://127.0.0.1:8080/hive?socksProxy=192.168.1.101:1081</code></p> <p>\u5927\u90e8\u5206\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7684 JDBC \u9a71\u52a8\u652f\u6301 <code>socksProxyHost,socksProxyPort</code> \u53c2\u6570\u6765\u652f\u6301\u4ee3\u7406\u8bbf\u95ee\u3002\u4e5f\u6709\u4e00\u4e9b\u7279\u522b\u7684\u60c5\u51b5\u3002</p> <p>\u4ee5\u4e0b\u662f\u5404\u7c7b\u6570\u636e\u5e93 JDBC \u9a71\u52a8\u6240\u652f\u6301\u7684\u4ee3\u7406\u7c7b\u578b\u4ee5\u53ca\u914d\u7f6e\u65b9\u5f0f</p> \u6570\u636e\u5e93 \u4ee3\u7406\u7c7b\u578b \u4ee3\u7406\u914d\u7f6e \u4f8b\u5b50 MySQL socks socksProxyHost,socksProxyPort <code>socksProxyHost=192.168.1.101&amp;socksProxyPort=1081</code> Presto socks socksProxy <code>socksProxy=192.168.1.101:1081</code> Presto http httpProxy <code>httpProxy=192.168.1.101:3128</code>"},{"location":"writer/rdbmswriter/#driver","title":"driver","text":"<p>\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e00\u4e2a\u6570\u636e\u5e93\u7684JDBC\u9a71\u52a8\u662f\u56fa\u5b9a\u7684\uff0c\u4f46\u6709\u4e9b\u56e0\u4e3a\u7248\u672c\u7684\u4e0d\u540c\uff0c\u6240\u5efa\u8bae\u7684\u9a71\u52a8\u7c7b\u540d\u4e0d\u540c\uff0c\u6bd4\u5982 MySQL\u3002  \u65b0\u7684 MySQL JDBC \u9a71\u52a8\u7c7b\u578b\u63a8\u8350\u4f7f\u7528 <code>com.mysql.cj.jdbc.Driver</code> \u800c\u4e0d\u662f\u4ee5\u524d\u7684 <code>com.mysql.jdbc.Drver</code>\u3002 \u5982\u679c\u60f3\u8981\u4f7f\u7528\u5c31\u7684\u9a71\u52a8\u540d\u79f0\uff0c\u5219\u53ef\u4ee5\u914d\u7f6e <code>driver</code> \u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/rdbmswriter/#session","title":"session","text":"<p>\u63cf\u8ff0\uff1a\u8bbe\u7f6e\u6570\u636e\u5e93\u8fde\u63a5\u65f6\u7684session\u4fe1\u606f\uff0c\u6bd4\u5982\u9488\u5bf9 Oracle \u6570\u636e\u5e93\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"session\": [\n    \"alter session set nls_date_format = 'dd.mm.yyyy hh24:mi:ss';\",\n    \"alter session set NLS_LANG = 'AMERICAN';\"\n  ]\n}\n</code></pre>"},{"location":"writer/rediswriter/","title":"Redis Writer","text":"<p>Redis Writer \u63d0\u4f9b\u4e86\u8fd8\u539f Redis dump \u547d\u4ee4\u7684\u80fd\u529b\uff0c\u5e76\u5199\u5165\u5230\u76ee\u6807 Redis\u3002\u652f\u6301 redis cluster \u96c6\u7fa4\u3001proxy\u3001\u4ee5\u53ca\u5355\u673a</p>"},{"location":"writer/rediswriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"uri\": \"tcp://127.0.0.1:7003\"\n            }\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"redisCluster\": false,\n          \"flushDB\": false\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/rediswriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 uri \u662f string \u5426 redis\u94fe\u63a5 redisCluster \u5426 boolean false \u662f\u5426\u4e3aredis cluster\u96c6\u7fa4,\u5982\u679c\u662f proxy \u6216\u5355\u673a\u5ffd\u7565\u8be5\u9879 flushDB \u5426 boolean false \u8fc1\u79fb\u524d\u662f\u5426\u6e05\u7a7a\u76ee\u6807 Redis batchSize \u5426 string 1000 \u6bcf\u6b21\u6279\u91cf\u5904\u7406\u6570\u91cf\u3002\u5982\u679ckey\u8fc7\u5927/\u5c0f,\u53ef\u4ee5\u76f8\u5e94\u7684\u8c03\u6574 timeout \u5426 string 60000 \u6bcf\u6b21\u6267\u884c\u6700\u5927\u8d85\u65f6\u65f6\u95f4, \u5355\u4f4d\u6beb\u79d2(ms)"},{"location":"writer/s3writer/","title":"S3 Writer","text":"<p>S3 Writer \u63d2\u4ef6\u7528\u4e8e\u5c06\u6570\u636e\u5199\u5165 Amazon AWS S3 \u5b58\u50a8\uff0c\u4ee5\u53ca\u517c\u5bb9 S3 \u534f\u8bae\u7684\u5b58\u50a8\uff0c\u6bd4\u5982 MinIO\u3002</p> <p>\u5728\u5b9e\u73b0\u4e0a\uff0c\u672c\u63d2\u4ef6\u57fa\u4e8e S3 \u5b98\u65b9\u7684 SDK 2.0 \u7f16\u5199\u3002</p>"},{"location":"writer/s3writer/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u4e0b\u9762\u7684\u914d\u7f6e\u7528\u4e8e\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u5199\u5165\u5230\u6307\u5b9a\u7684 S3 bucket \u4e0a\u3002</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"s3writer\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": \"upload.csv\",\n          \"region\": \"ap-northeast-1\",\n          \"encoding\": \"\",\n          \"fieldDelimiter\": \",\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/s3writer/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 endpoint \u662f string \u65e0 S3 Server\u7684 EndPoint\u5730\u5740\uff0c\u4f8b\u5982 <code>s3.xx.amazonaws.com</code> region \u662f string \u65e0 S3 Server\u7684 Region \u5730\u5740\uff0c\u4f8b\u5982 <code>ap-southeast-1</code> accessId \u662f string \u65e0 \u8bbf\u95ee ID accessKey \u662f string \u65e0 \u8bbf\u95ee Key bucket \u662f string \u65e0 \u8981\u5199\u5165\u7684 bucket object \u662f string \u65e0 \u8981\u5199\u5165\u7684 object\uff0c\u6ce8\u610f\u4e8b\u9879\u89c1\u4e0b fieldDelimiter \u5426 char <code>','</code> \u5b57\u6bb5\u7684\u5206\u9694\u7b26 nullFormat \u5426 char <code>\\N</code> \u5f53\u503c\u4e3a\u7a7a\u65f6\uff0c\u7528\u4ec0\u4e48\u5b57\u7b26\u8868\u793a header \u5426 list \u65e0 \u5199\u5165\u6587\u4ef6\u5934\u4fe1\u606f\uff0c\u6bd4\u5982 <code>[\"id\",\"title\",\"url\"]</code> maxFileSize \u5426 int <code>100000</code> \u5355\u4e2a object \u7684\u5927\u5c0f\uff0c\u5355\u4f4d\u4e3a MB encoding \u5426 string <code>utf-8</code> \u6587\u4ef6\u7f16\u7801\u683c\u5f0f writeMode \u5426 string <code>append</code> \u5199\u5165\u6a21\u5f0f\uff0c\u8be6\u89c1 hdfswriter \u4e2d\u76f8\u5173\u63cf\u8ff0 pathStyleAccessEnabled \u5426 bool false \u662f\u5426\u4f7f\u7528path access\u65b9\u5f0f\u8bbf\u95ee sslEnabled \u5426 bool true \u662f\u5426\u4f7f\u7528ssl\u65b9\u5f0f\u8bbf\u95ee fileType \u5426 string <code>text</code> \u6587\u4ef6\u7c7b\u578b text, orc ,parquet compress \u5426 string orc \u9ed8\u8ba4<code>NONE</code> parquet \u9ed8\u8ba4 <code>UNCOMPRESSED</code> orc\u6216parquet\u6587\u4ef6\u7684\u538b\u7f29\u65b9\u5f0f,\u9ed8\u8ba4\u4e0d\u538b\u7f29"},{"location":"writer/s3writer/#object","title":"object","text":"<p>\u4e0a\u8ff0\u914d\u7f6e\u4e2d\u7684 <code>object</code> \u914d\u7f6e\u7684\u867d\u7136\u662f <code>upload.csv</code> \u6587\u4ef6\uff0c\u5b9e\u9645\u4e0a\u5728 S3 \u5199\u5165\u7684\u6587\u4ef6\u540d\u4f1a\u5728\u6307\u5b9a\u7684\u6587\u4ef6\u540d\u540e\u9762\u52a0\u4e0a <code>uuid</code> \u540e\u7f00\uff0c \u7c7b\u4f3c <code>upload_c0d2ca7df0444933a6f18ea76718b569.csv</code>\u3002 \u8fd9\u662f\u7528\u4e8e\u5728\u591a\u901a\u9053\u5199\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u6587\u4ef6\u540d\u4e0d\u4f1a\u91cd\u540d\u3002</p>"},{"location":"writer/s3writer/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b S3 \u6570\u636e\u7c7b\u578b Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext, year,xml Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary"},{"location":"writer/sqlitewriter/","title":"SQLite Writer","text":"<p>SQLite Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 SQLite \u6570\u636e\u5e93\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/sqlitewriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684\u8868\u5982\u4e0b\uff1a</p> <pre><code>create table addax_tbl\n(\n    col1 varchar(20) ,\n    col2 int(4),\n    col3 datetime,\n    col4 boolean,\n    col5 binary\n);\n</code></pre> <p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 SQLite \u7684\u6570\u636e\u3002</p> job/stream2sqlite.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"sqlitewriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite://tmp/writer.sqlite3\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2sqlite.json</code></p>"},{"location":"writer/sqlitewriter/#_2","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/stream2sqlite.json\n</code></pre>"},{"location":"writer/sqlitewriter/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002\u56e0\u4e3a SQLite \u8fde\u63a5\u65e0\u9700\u8d26\u53f7\u5bc6\u7801\uff0c\u56e0\u6b64\u5176\u4ed6\u6570\u636e\u5e93\u5199\u5165\u63d2\u4ef6\u9700\u8981\u914d\u7f6e\u7684 <code>username</code>, <code>password</code> \u5728\u8fd9\u91cc\u4e0d\u9700\u8981\u3002</p>"},{"location":"writer/sqlitewriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> \u8868\u793a\u91c7\u7528 <code>insert into</code></li> <li><code>replace</code>\u8868\u793a\u91c7\u7528<code>replace into</code>\u65b9\u5f0f</li> <li><code>update</code> \u8868\u793a\u91c7\u7528 <code>ON DUPLICATE KEY UPDATE</code> \u8bed\u53e5</li> </ul>"},{"location":"writer/sqlitewriter/#_4","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b SQLite \u6570\u636e\u7c7b\u578b Long integer Double real String varchar Date datetime Boolean bool Bytes blob, binary"},{"location":"writer/sqlserverwriter/","title":"SQLServer Writer","text":"<p>SQLServer Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 SQL Server \u5e93\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/sqlserverwriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u4e00\u4efd\u4ece\u5185\u5b58\u4ea7\u751f\u5230 SQL Server \u5bfc\u5165\u7684\u6570\u636e\u3002</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"sqlserverwriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"db_id\",\n            \"db_type\",\n            \"db_ip\",\n            \"db_port\",\n            \"db_role\",\n            \"db_name\",\n            \"db_username\",\n            \"db_password\",\n            \"db_modify_time\",\n            \"db_modify_user\",\n            \"db_description\",\n            \"db_tddl_info\"\n          ],\n          \"preSql\": [\n            \"delete from @table where db_id = -1;\"\n          ],\n          \"postSql\": [\n            \"update @table set db_modify_time = now() where db_id = 1;\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"db_info_for_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/sqlserverwriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/sqlserverwriter/#writemode","title":"writeMode","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c \u91c7\u53d6 <code>insert into</code> \u8bed\u6cd5\u5199\u5165 SQL Server \u8868\uff0c\u5982\u679c\u4f60\u5e0c\u671b\u91c7\u53d6\u4e3b\u952e\u5b58\u5728\u65f6\u66f4\u65b0\uff0c\u4e0d\u5b58\u5728\u5219\u5199\u5165\u7684\u65b9\u5f0f\uff0c\u4e5f\u5c31\u662f SQL Server \u7684 <code>MERGE INTO</code> \u8bed\u6cd5, \u53ef\u4ee5\u4f7f\u7528 <code>update</code> \u6a21\u5f0f\u3002 \u5047\u5b9a\u8868\u7684\u4e3b\u952e\u4e3a <code>id</code> ,\u5219 <code>writeMode</code> \u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"writeMode\": \"update(id)\"\n}\n</code></pre> <p>\u5982\u679c\u662f\u8054\u5408\u552f\u4e00\u7d22\u5f15\uff0c\u5219\u914d\u7f6e\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"writeMode\": \"update(col1, col2)\"\n}\n</code></pre>"},{"location":"writer/starrockswriter/","title":"StarRocks Writer","text":"<p>StarRocks Writer \u63d2\u4ef6\u7528\u4e8e\u5411 Starrocks \u6570\u636e\u5e93\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u5199\u5165\u6570\u636e\u3002 \u5176\u5b9e\u73b0\u4e0a\u662f\u901a\u8fc7\u8bbf\u95ee Doris http \u8fde\u63a5(8030) \uff0c\u7136\u540e\u901a\u8fc7 stream load \u52a0\u8f7d\u6570\u636e\u5230\u6570\u636e\u4e2d\uff0c\u76f8\u6bd4 <code>insert into</code> \u65b9\u5f0f\u6548\u7387\u8981\u9ad8\u4e0d\u5c11\uff0c\u4e5f\u662f\u5b98\u65b9\u63a8\u8350\u7684\u751f\u4ea7\u73af\u5883\u4e0b\u7684\u6570\u636e\u52a0\u8f7d\u65b9\u5f0f\u3002</p> <p>StarRocks \u662f\u4e00\u4e2a\u517c\u5bb9 MySQL \u534f\u8bae\u7684\u6570\u636e\u5e93\u540e\u7aef\uff0c\u56e0\u6b64 Doris \u8bfb\u53d6\u53ef\u4ee5\u4f7f\u7528 MySQLReader \u8fdb\u884c\u8bbf\u95ee\u3002</p>"},{"location":"writer/starrockswriter/#_1","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684\u8868\u7684\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff1a</p> <pre><code>CREATE DATABASE example_db;\nCREATE TABLE example_db.table1\n(\n    siteid   INT         DEFAULT '10',\n    citycode SMALLINT,\n    username VARCHAR(32) DEFAULT '',\n    pv       BIGINT SUM DEFAULT '0'\n) AGGREGATE KEY(siteid, citycode, username)\nDISTRIBUTED BY HASH(siteid) BUCKETS 10\nPROPERTIES(\"replication_num\" = \"1\");\n</code></pre> <p>\u4e0b\u9762\u914d\u7f6e\u4e00\u4e2a\u4ece\u5185\u5b58\u8bfb\u53d6\u6570\u636e\uff0c\u7136\u540e\u5199\u5165\u5230 doris \u8868\u7684\u914d\u7f6e\u6587\u4ef6</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"starrockswriter\",\n        \"parameter\": {\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"database\": \"example_db\",\n          \"table\": \"table1\",\n          \"jdbcUrl\": \"jdbc:mysql://172.28.17.100:9030/\",\n          \"loadUrl\": [\n            \"172.28.17.100:8030\",\n            \"172.28.17.100:8030\"\n          ],\n          \"loadProps\": {\n            \"column_separator\": \"\\\\x01\",\n            \"row_delimiter\": \"\\\\x02\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a <code>job/stream2starrocks.json</code></p> <p>\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4</p> <pre><code>bin/addax.sh job/stream2starrocks.json\n</code></pre>"},{"location":"writer/starrockswriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\uff0c\u5e76\u589e\u52a0\u4e86\u4e0b\u9762\u7684\u914d\u7f6e\u9879\u3002</p> \u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 loadUrl \u662f string \u65e0 StarRocks FE \u7684\u5730\u5740\u7528\u4e8eStreamLoad1\uff0c\u53ef\u4ee5\u4e3a\u591a\u4e2afe\u5730\u5740\uff0c<code>fe_ip:fe_http_port</code> maxBatchRows \u5426 int 500000 \u5355\u6b21StreamLoad\u5bfc\u5165\u7684\u6700\u5927\u884c\u6570 maxBatchSize \u5426 int 104857600 \u5355\u6b21StreamLoad\u5bfc\u5165\u7684\u6700\u5927\u5b57\u8282\u6570 flushInterval \u5426 int 300000 \u4e0a\u4e00\u6b21StreamLoad\u7ed3\u675f\u81f3\u4e0b\u4e00\u6b21\u5f00\u59cb\u7684\u65f6\u95f4\u95f4\u9694(\u5355\u4f4d\uff1ams) loadProps \u5426 map streamLoad \u7684\u8bf7\u6c42\u53c2\u6570\uff0c\u8be6\u60c5\u53c2\u7167StreamLoad\u4ecb\u7ecd\u9875\u9762"},{"location":"writer/starrockswriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u9ed8\u8ba4\u4f20\u5165\u7684\u6570\u636e\u5747\u4f1a\u88ab\u8f6c\u4e3a\u5b57\u7b26\u4e32\uff0c\u5e76\u4ee5<code>\\t</code>\u4f5c\u4e3a\u5217\u5206\u9694\u7b26\uff0c<code>\\n</code>\u4f5c\u4e3a\u884c\u5206\u9694\u7b26\uff0c\u7ec4\u6210<code>csv</code>\u6587\u4ef6\u8fdb\u884cStreamLoad\u5bfc\u5165\u64cd\u4f5c\u3002 \u5982\u9700\u66f4\u6539\u5217\u5206\u9694\u7b26\uff0c \u5219\u6b63\u786e\u914d\u7f6e <code>loadProps</code> \u5373\u53ef\uff1a <pre><code>\"loadProps\": {\n    \"column_separator\": \"\\\\x01\",\n    \"row_delimiter\": \"\\\\x02\"\n}\n</code></pre></p> <p>\u5982\u9700\u66f4\u6539\u5bfc\u5165\u683c\u5f0f\u4e3a<code>json</code>\uff0c \u5219\u6b63\u786e\u914d\u7f6e <code>loadProps</code> \u5373\u53ef\uff1a <pre><code>\"loadProps\": {\n    \"format\": \"json\",\n    \"strip_outer_array\": true\n}\n</code></pre></p>"},{"location":"writer/streamwriter/","title":"Stream Writer","text":"<p>Stream Writer \u662f\u4e00\u4e2a\u5c06\u6570\u636e\u5199\u5165\u5185\u5b58\u7684\u63d2\u4ef6\uff0c\u4e00\u822c\u7528\u6765\u5c06\u83b7\u53d6\u5230\u7684\u6570\u636e\u5199\u5230\u7ec8\u7aef\uff0c\u7528\u6765\u8c03\u8bd5\u8bfb\u53d6\u63d2\u4ef6\u7684\u6570\u636e\u5904\u7406\u60c5\u51b5\u3002</p> <p>\u4e00\u4e2a\u5178\u578b\u7684 Stream Writer \u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"print\": true,\n    \"nullFormat\": \"NULL\"\n  }\n}\n</code></pre> <p>\u4e0a\u8ff0\u914d\u7f6e\u4f1a\u5c06\u83b7\u53d6\u7684\u6570\u636e\u76f4\u63a5\u6253\u5370\u5230\u7ec8\u7aef\u3002 \u5176\u4e2d <code>nullFormat</code> \u7528\u6765\u6307\u5b9a\u5f53\u503c\u4e3a\u7a7a\u65f6\uff0c\u5982\u4f55\u5728\u7ec8\u7aef\u8868\u793a\uff0c\u9ed8\u8ba4\u662f\u5b57\u7b26\u4e32 <code>NULL</code>, \u5982\u679c\u4e0d\u60f3\u6253\u5370\u7a7a\u503c\uff0c\u53ef\u4ee5\u5c06\u5176\u8bbe\u7f6e\u4e3a <code>\"\"</code>\u3002</p> <p>\u8be5\u63d2\u4ef6\u4e5f\u652f\u6301\u5c06\u6570\u636e\u5199\u5165\u5230\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"path\": \"/tmp/out\",\n    \"fileName\": \"out.txt\",\n    \"fieldDelimiter\": \",\",\n    \"recordNumBeforeSleep\": \"100\",\n    \"sleepTime\": \"5\"\n  }\n}\n</code></pre> <p>\u4e0a\u8ff0\u914d\u7f6e\u4e2d:</p> <ul> <li><code>fieldDelimiter</code> \u8868\u793a\u5b57\u6bb5\u5206\u9694\u7b26\uff0c\u9ed8\u8ba4\u4e3a\u5236\u8868\u7b26(<code>\\t</code>)</li> <li><code>recordNumBeforeSleep</code> \u8868\u793a\u83b7\u53d6\u591a\u5c11\u6761\u8bb0\u5f55\u540e\uff0c\u6267\u884c\u4f11\u7720\uff0c\u9ed8\u8ba4\u4e3a0\uff0c\u8868\u793a\u4e0d\u542f\u7528\u8be5\u529f\u80fd</li> <li><code>sleepTime</code> \u5219\u8868\u793a\u4f11\u7720\u591a\u957f\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\uff0c\u9ed8\u8ba4\u4e3a0\uff0c\u8868\u793a\u4e0d\u542f\u7528\u8be5\u529f\u80fd\u3002</li> </ul> <p>\u4e0a\u8ff0\u914d\u7f6e\u7684\u542b\u4e49\u662f\u5c06\u6570\u636e\u5199\u5165\u5230 <code>/tmp/out/out.txt</code> \u6587\u4ef6\uff0c \u6bcf\u83b7\u53d6100\u6761\u8bb0\u5f55\u540e\uff0c\u4f11\u77205\u79d2\u3002</p>"},{"location":"writer/sybasewriter/","title":"Sybase Writer","text":"<p>Sybase Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5199\u5165\u6570\u636e\u5230 Sybase \u5e93\u8868\u7684\u529f\u80fd\u3002</p>"},{"location":"writer/sybasewriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<p>\u6211\u4eec\u53ef\u4ee5\u7528 Docker \u5bb9\u5668\u6765\u542f\u52a8\u4e00\u4e2a Sybase \u6570\u636e\u5e93</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u4e00\u5f20\u5982\u4e0b\u8868</p> <pre><code>create table addax_writer \n(\n    id int,\n    name varchar(255),\n    salary float(2),\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>\u518d\u4f7f\u7528\u4e0b\u9762\u7684\u4efb\u52a1\u914d\u7f6e\u6587\u4ef6</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"100,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"10,100\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"double\"\n            },\n            {\n              \"incr\": \"2022-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            },\n            {\n              \"incr\": \"2023-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"sybasewriter\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\",\n            \"salary\",\n            \"created_at\",\n            \"updated_at\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.addax_writer\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/sybasewriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/tdenginewriter/","title":"TDengine Writer","text":"<p>TDengine Writer \u63d2\u4ef6\u5b9e\u73b0\u4e86\u5c06\u6570\u636e\u5199\u5165 TDengine \u6570\u636e\u5e93\u7cfb\u7edf\u3002\u5728\u5e95\u5c42\u5b9e\u73b0\u4e0a\uff0cTDengine Writer \u901a\u8fc7JDBC JNI \u9a71\u52a8\u8fde\u63a5\u8fdc\u7a0b TDengine \u6570\u636e\u5e93\uff0c \u5e76\u6267\u884c\u76f8\u5e94\u7684sql\u8bed\u53e5\u5c06\u6570\u636e\u6279\u91cf\u5199\u5165 TDengine \u5e93\u4e2d\u3002</p>"},{"location":"writer/tdenginewriter/#_1","title":"\u524d\u7f6e\u6761\u4ef6","text":"<p>\u8003\u8651\u5230\u6027\u80fd\u95ee\u9898\uff0c\u8be5\u63d2\u4ef6\u4f7f\u7528\u4e86 TDengine \u7684 JDBC-JNI \u9a71\u52a8\uff0c \u8be5\u9a71\u52a8\u76f4\u63a5\u8c03\u7528\u5ba2\u6237\u7aef API\uff08<code>libtaos.so</code> \u6216 <code>taos.dll</code>\uff09\u5c06\u5199\u5165\u548c\u67e5\u8be2\u8bf7\u6c42\u53d1\u9001\u5230 <code>taosd</code> \u5b9e\u4f8b\u3002 \u56e0\u6b64\u5728\u4f7f\u7528\u4e4b\u524d\u9700\u8981\u914d\u7f6e\u597d\u52a8\u6001\u5e93\u94fe\u63a5\u6587\u4ef6\u3002</p> <p>\u9996\u5148\u5c06 <code>plugin/writer/tdenginewriter/libs/libtaos.so.2.0.16.0</code> \u62f7\u8d1d\u5230 <code>/usr/lib64</code> \u76ee\u5f55\uff0c\u7136\u540e\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u8f6f\u94fe\u63a5</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"writer/tdenginewriter/#_2","title":"\u793a\u4f8b","text":"<p>\u5047\u5b9a\u8981\u5199\u5165\u7684\u8868\u5982\u4e0b\uff1a</p> <pre><code>create table test.addax_test (\n    ts timestamp,\n    name nchar(100),\n    file_size int,\n    file_date timestamp,\n    flag_open bool,\n    memo nchar(100)\n);\n</code></pre> <p>\u4ee5\u4e0b\u662f\u914d\u7f6e\u6587\u4ef6</p> job/stream2tdengine.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"2017-08-01 00:01:02,2020-01-01 12:13:14\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"tdenginewriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"taosdata\",\n          \"column\": [\n            \"ts\",\n            \"name\",\n            \"file_size\",\n            \"file_date\",\n            \"flag_open\",\n            \"memo\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:TAOS://127.0.0.1:6030/test\",\n            \"table\": [\n              \"addax_test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u4e3a   <code>job/stream2tdengine.json</code></p>"},{"location":"writer/tdenginewriter/#_3","title":"\u6267\u884c\u91c7\u96c6\u547d\u4ee4","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u6570\u636e\u91c7\u96c6</p> <pre><code>bin/addax.sh job/tdengine2stream.json\n</code></pre> <p>\u547d\u4ee4\u8f93\u51fa\u7c7b\u4f3c\u5982\u4e0b\uff1a</p> <pre><code>2021-02-20 15:52:07.691 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-02-20 15:52:07.748 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"random\":\"2017-08-01 00:01:02,2020-01-01 12:13:14\",\n                            \"type\":\"date\"\n                        },\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"Addax\"\n                        },\n                        {\n                            \"type\":\"long\",\n                            \"value\":19880808\n                        },\n                        {\n                            \"type\":\"date\",\n                            \"value\":\"1988-08-08 08:08:08\"\n                        },\n                        {\n                            \"type\":\"bool\",\n                            \"value\":true\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        }\n                    ],\n                    \"sliceRecordCount\":1000\n                },\n                \"name\":\"streamreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"password\":\"*****\",\n                    \"column\":[\n                        \"ts\",\n                        \"name\",\n                        \"file_size\",\n                        \"file_date\",\n                        \"flag_open\",\n                        \"memo\"\n                    ],\n                    \"connection\":[\n                        {\n                            \"jdbcUrl\":\"jdbc:TAOS://127.0.0.1:6030/test\",\n                            \"table\":[\n                                \"addax_test\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"root\",\n                    \"preSql\":[]\n                },\n                \"name\":\"tdenginewriter\"\n            }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-02-20 15:52:07.786 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-20 15:52:07.787 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-20 15:52:07.789 [main] INFO  JobContainer - Set jobId = 0\njava.library.path:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n2021-02-20 15:52:08.048 [job-0] INFO  OriginalConfPretreatmentUtil - table:[addax_test] all columns:[ts,name,file_size,file_date,flag_open,memo].\n2021-02-20 15:52:08.056 [job-0] INFO  OriginalConfPretreatmentUtil - Write data [\nINSERT INTO %s (ts,name,file_size,file_date,flag_open,memo) VALUES(?,?,?,?,?,?)\n], which jdbcUrl like:[jdbc:TAOS://127.0.0.1:6030/test]\n\n2021-02-20 15:52:11.158 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-20 15:52:07\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-20 15:52:11\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :           11.07KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :            333rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                1000\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"writer/tdenginewriter/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u63d2\u4ef6\u57fa\u4e8e RDBMS Writer \u5b9e\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u53c2\u8003 RDBMS Writer \u7684\u6240\u6709\u914d\u7f6e\u9879\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u4e00\u4e9b TDengine \u7279\u6709\u7684\u914d\u7f6e\u9879\u3002</p>"},{"location":"writer/tdenginewriter/#jdbc-restful","title":"\u4f7f\u7528 JDBC-RESTful \u63a5\u53e3","text":"<p>\u5982\u679c\u4e0d\u60f3\u4f9d\u8d56\u672c\u5730\u5e93\uff0c\u6216\u8005\u6ca1\u6709\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 <code>JDBC-RESTful</code> \u63a5\u53e3\u6765\u5199\u5165\u8868\uff0c\u76f8\u6bd4 JDBC-JNI \u800c\u8a00\uff0c\u914d\u7f6e\u533a\u522b\u662f\uff1a</p> <ul> <li>driverClass \u6307\u5b9a\u4e3a <code>com.taosdata.jdbc.rs.RestfulDriver</code></li> <li>jdbcUrl \u4ee5 <code>jdbc:TAOS-RS://</code> \u5f00\u5934\uff1b</li> <li>\u4f7f\u7528 <code>6041</code> \u4f5c\u4e3a\u8fde\u63a5\u7aef\u53e3</li> </ul> <p>\u6240\u4ee5\u4e0a\u8ff0\u914d\u7f6e\u4e2d\u7684 <code>connection</code> \u5e94\u8be5\u4fee\u6539\u4e3a\u5982\u4e0b\uff1a</p> <pre><code>{\n  \"connection\": [\n    {\n      \"jdbcUrl\": \"jdbc:TAOS-RS://127.0.0.1:6041/test\",\n      \"table\": [\n        \"addax_test\"\n      ],\n      \"driver\": \"com.taosdata.jdbc.rs.RestfulDriver\"\n    }\n  ]\n}\n</code></pre>"},{"location":"writer/tdenginewriter/#_5","title":"\u7c7b\u578b\u8f6c\u6362","text":"<p>\u76ee\u524d TDenginereader \u652f\u6301 TDengine \u6240\u6709\u7c7b\u578b\uff0c\u5177\u4f53\u5982\u4e0b</p> Addax \u5185\u90e8\u7c7b\u578b TDengine \u6570\u636e\u7c7b\u578b Long SMALLINT, TINYINT, INT, BIGINT, TIMESTAMP Double FLOAT, DOUBLE String BINARY, NCHAR Boolean BOOL"},{"location":"writer/tdenginewriter/#_6","title":"\u5f53\u524d\u652f\u6301\u7248\u672c","text":"<p>TDengine 2.0.16</p>"},{"location":"writer/tdenginewriter/#_7","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>TDengine JDBC-JNI \u9a71\u52a8\u548c\u52a8\u6001\u5e93\u7248\u672c\u8981\u6c42\u4e00\u4e00\u5339\u914d\uff0c\u56e0\u6b64\u5982\u679c\u4f60\u7684\u6570\u636e\u7248\u672c\u5e76\u4e0d\u662f <code>2.0.16</code>\uff0c\u5219\u9700\u8981\u540c\u65f6\u66ff\u6362\u52a8\u6001\u5e93\u548c\u63d2\u4ef6\u76ee\u5f55\u4e2d\u7684JDBC\u9a71\u52a8</li> <li>TDengine \u7684\u65f6\u5e8f\u5b57\u6bb5\uff08timestamp\uff09\u9ed8\u8ba4\u6700\u5c0f\u503c\u4e3a <code>1500000000000</code>\uff0c\u5373 <code>2017-07-14 10:40:00.0</code>\uff0c\u5982\u679c\u4f60\u5199\u5165\u7684\u65f6\u8bb8\u65f6\u95f4\u6233\u5c0f\u4e8e\u8be5\u503c\uff0c\u5219\u4f1a\u62a5\u9519</li> </ul>"},{"location":"writer/txtfilewriter/","title":"TxtFile Writer","text":"<p>TxtFile Writer \u63d0\u4f9b\u4e86\u5411\u672c\u5730\u6587\u4ef6\u5199\u5165\u7c7b CSV \u683c\u5f0f\u7684\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u8868\u6587\u4ef6\u3002</p>"},{"location":"writer/txtfilewriter/#_1","title":"\u914d\u7f6e\u6837\u4f8b","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/data\"\n          ],\n          \"encoding\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/result\",\n          \"fileName\": \"luohw\",\n          \"writeMode\": \"truncate\",\n          \"dateFormat\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/txtfilewriter/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u914d\u7f6e\u9879 \u662f\u5426\u5fc5\u987b \u6570\u636e\u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 path \u662f string \u65e0 \u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u5199\u5165 Path \u76ee\u5f55\u4e0b\u5c5e\u591a\u4e2a\u6587\u4ef6 fileName \u662f string \u65e0 \u5199\u5165\u7684\u6587\u4ef6\u540d\uff0c\u8be5\u6587\u4ef6\u540d\u4f1a\u6dfb\u52a0\u968f\u673a\u7684\u540e\u7f00\u4f5c\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u5199\u5165\u5b9e\u9645\u6587\u4ef6\u540d writeMode \u662f string \u65e0 \u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff0c\u8be6\u89c1\u4e0b\u6587 fieldDelimiter \u662f string <code>,</code> \u63cf\u8ff0\uff1a\u8bfb\u53d6\u7684\u5b57\u6bb5\u5206\u9694\u7b26 compress \u5426 string \u65e0 \u6587\u672c\u538b\u7f29\u7c7b\u578b\uff0c\u652f\u6301\u538b\u7f29\u7c7b\u578b\u4e3a <code>zip</code>\u3001<code>lzo</code>\u3001<code>lzop</code>\u3001<code>tgz</code>\u3001<code>bzip2</code> encoding \u5426 string utf-8 \u8bfb\u53d6\u6587\u4ef6\u7684\u7f16\u7801\u914d\u7f6e nullFormat \u5426 string <code>\\N</code> \u5b9a\u4e49\u54ea\u4e9b\u5b57\u7b26\u4e32\u53ef\u4ee5\u8868\u793a\u4e3a null dateFormat \u5426 string \u65e0 \u65e5\u671f\u7c7b\u578b\u7684\u6570\u636e\u5e8f\u5217\u5316\u5230\u6587\u4ef6\u4e2d\u65f6\u7684\u683c\u5f0f\uff0c\u4f8b\u5982 <code>\"yyyy-MM-dd\"</code> fileFormat \u5426 string text \u6587\u4ef6\u5199\u51fa\u7684\u683c\u5f0f\uff0c\u8be6\u89c1\u4e0b\u6587 table \u662f string \u65e0 sql \u6a21\u5f0f\u65f6\u9700\u8981\u6307\u5b9a\u8868\u540d\uff0c column \u5426 list \u65e0 sql \u6a21\u5f0f\u65f6\u53ef\u9009\u6307\u5b9a\u5217\u540d\uff0c extendedInsert \u5426 boolean true sql \u6a21\u5f0f\u65f6\u662f\u5426\u4f7f\u7528\u6279\u91cf\u63d2\u5165\u8bed\u6cd5\uff0c\u8be6\u89c1\u4e0b\u6587 batchSize \u5426 int 2048 sql \u6a21\u5f0f\u65f6\u6279\u91cf\u63d2\u5165\u8bed\u6cd5\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u8be6\u89c1\u4e0b\u6587 header \u5426 list \u65e0 text \u5199\u51fa\u65f6\u7684\u8868\u5934\uff0c\u793a\u4f8b <code>['id', 'name', 'age']</code>"},{"location":"writer/txtfilewriter/#writemode","title":"writeMode","text":"<p>\u5199\u5165\u524d\u6570\u636e\u6e05\u7406\u5904\u7406\u6a21\u5f0f\uff1a</p> <ul> <li>truncate\uff0c\u5199\u5165\u524d\u6e05\u7406\u76ee\u5f55\u4e0b\u4e00 fileName \u524d\u7f00\u7684\u6240\u6709\u6587\u4ef6\u3002</li> <li>append\uff0c\u5199\u5165\u524d\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u4f7f\u7528 filename \u5199\u5165\uff0c\u5e76\u4fdd\u8bc1\u6587\u4ef6\u540d\u4e0d\u51b2\u7a81\u3002</li> <li>nonConflict\uff0c\u5982\u679c\u76ee\u5f55\u4e0b\u6709 fileName \u524d\u7f00\u7684\u6587\u4ef6\uff0c\u76f4\u63a5\u62a5\u9519\u3002</li> </ul>"},{"location":"writer/txtfilewriter/#fileformat","title":"fileFormat","text":"<p>\u6587\u4ef6\u5199\u51fa\u7684\u683c\u5f0f\uff0c\u5305\u62ec csv \u548c text \u548c <code>4.1.3</code> \u7248\u672c\u5f15\u5165\u7684 sql \u4e09\u79cd\uff0ccsv \u662f\u4e25\u683c\u7684 csv \u683c\u5f0f\uff0c\u5982\u679c\u5f85\u5199\u6570\u636e\u5305\u62ec\u5217\u5206\u9694\u7b26\uff0c\u5219\u4f1a\u6309\u7167 csv \u7684\u8f6c\u4e49\u8bed\u6cd5\u8f6c\u4e49\uff0c\u8f6c\u4e49\u7b26\u53f7\u4e3a\u53cc\u5f15\u53f7 <code>\"</code>\uff1b text \u683c\u5f0f\u662f\u7528\u5217\u5206\u9694\u7b26\u7b80\u5355\u5206\u5272\u5f85\u5199\u6570\u636e\uff0c\u5bf9\u4e8e\u5f85\u5199\u6570\u636e\u5305\u62ec\u5217\u5206\u9694\u7b26\u60c5\u51b5\u4e0b\u4e0d\u505a\u8f6c\u4e49\u3002 sql \u683c\u5f0f\u8868\u793a\u5c06\u6570\u636e\u4ee5 SQL \u8bed\u53e5 (<code>INSERT INTO ... VALUES</code>) \u7684\u65b9\u5f0f\u5199\u5165\u5230\u6587\u4ef6</p>"},{"location":"writer/txtfilewriter/#table","title":"table","text":"<p>\u4ec5\u5728 sql \u6587\u4ef6\u683c\u5f0f\u4e0b\u9700\u8981\uff0c\u7528\u6765\u6307\u5b9a\u5199\u5165\u7684\u8868\u540d</p>"},{"location":"writer/txtfilewriter/#column","title":"column","text":"<p>\u5728 sql \u6587\u4ef6\u683c\u5f0f\u4e0b\uff0c\u53ef\u4ee5\u6307\u5b9a\u5199\u5165\u7684\u5217\u540d\uff0c\u5982\u679c\u6307\u5b9a\uff0c\u5219 sql \u8bed\u53e5\u7c7b\u4f3c <code>INSERT INTO table (col1, col2, col3) VALUES (val1, val2, val3)</code>\uff0c\u6a21\u5f0f \u5426\u5219\u4e3a <code>INSERT INTO table VALUES (val1, val2, val3)</code>\u3002\u6a21\u5f0f</p>"},{"location":"writer/txtfilewriter/#extendedinsert","title":"extendedInsert","text":"<p>\u662f\u5426\u542f\u7528\u6279\u91cf\u63d2\u5165\u8bed\u6cd5\uff0c\u5982\u679c\u542f\u7528\uff0c\u5219\u4f1a\u5c06 batchSize \u4e2a\u6570\u636e\u4e00\u6b21\u6027\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff0c\u5426\u5219\u6bcf\u4e2a\u6570\u636e\u4e00\u884c\u3002\u8be5\u53c2\u6570\u501f\u9274\u4e86 <code>mysqldump</code> \u5de5\u5177\u7684 extended-insert \u53c2\u6570\u8bed\u6cd5</p>"},{"location":"writer/txtfilewriter/#batchsize","title":"batchSize","text":"<p>\u6279\u91cf\u63d2\u5165\u8bed\u6cd5\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u5982\u679c extendedInsert \u4e3a true\uff0c\u5219\u6bcf batchSize \u4e2a\u6570\u636e\u4e00\u6b21\u6027\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff0c\u5426\u5219\u6bcf\u4e2a\u6570\u636e\u4e00\u884c\u3002</p>"},{"location":"writer/txtfilewriter/#_3","title":"\u7c7b\u578b\u8f6c\u6362","text":"Addax \u5185\u90e8\u7c7b\u578b \u672c\u5730\u6587\u4ef6 \u6570\u636e\u7c7b\u578b Long Long Double Double string string Boolean Boolean Date Date"}]}