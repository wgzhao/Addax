{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Addax Introduction","text":""},{"location":"#overview","title":"Overview","text":"<p>Addax is a heterogeneous data source offline synchronization tool originally derived from Alibaba's DataX, dedicated to implementing stable and efficient data synchronization between various heterogeneous data sources including relational databases (MySQL, Oracle, etc.), HDFS, Hive, HBase, FTP, and more.</p> <p></p> <p>To solve the problem of heterogeneous data source synchronization, Addax transforms complex network synchronization links into star-shaped data links. Addax serves as the intermediate transmission carrier responsible for connecting various data sources. When a new data source needs to be integrated, you only need to connect this data source to Addax to achieve seamless data synchronization with existing data sources.</p>"},{"location":"#framework-design","title":"Framework Design","text":"<pre><code>graph LR\nMySQL\nsubgraph Addax\n    direction LR\n    subgraph reader[\"Reader Plugin\"]\n        mr[\"MySQLReader\"]\n    end\n    subgraph writer[\"Writer Plugin\"]\n    hw[\"HDFSWriter\"]\n    end\n    Framework\n    mr --&gt; Framework --&gt; writer\nend\n\nMySQL ==&gt; Addax ==&gt; HDFS\n</code></pre> <p>Addax serves as an offline data synchronization framework, built with a Framework + plugin architecture. It abstracts data source reading and writing into Reader/Writer plugins, which are integrated into the entire synchronization framework.</p> <ul> <li>Reader: The Reader is the data collection module, responsible for collecting data from data sources and sending it to the Framework.</li> <li>Writer: The Writer is the data writing module, responsible for continuously fetching data from the Framework and writing it to the destination.</li> <li>Framework: The Framework connects Reader and Writer, serving as the data transmission channel between them, handling buffering, flow control, concurrency, data transformation, and other core technical issues.</li> </ul> <p>Addax Framework provides simple interfaces for plugin interaction and a simple plugin integration mechanism. By simply adding a plugin, you can seamlessly connect to other data sources.</p>"},{"location":"#core-architecture","title":"Core Architecture","text":"<p>This section uses a sequence diagram of an Addax job lifecycle to briefly explain the relationships between various modules from an overall architectural design perspective.</p> <pre><code>graph TB\nsubgraph Job\nend\nsubgraph task\n  direction TB\n  t1[\"Task\"]\n  t2[\"Task\"]\n  t3[\"Task\"]\n  t4[\"Task\"]\n  t5[\"Task\"]\n  t6[\"Task\"]\nend\nsubgraph taskgroup[\" \"]\n    direction TB\n  subgraph tg1[\"TaskGroup\"]\n    subgraph tg1_Task[\"Task\"]\n      tg1_r[\"Reader\"]\n      tg1_c[\"Channel\"]\n      tg1_w[\"Writer\"]\n    end\n    t7[\"Task\"]\n    t8[\"Task\"]\n  end\n\n  subgraph tg2[\"TaskGroup\"]\n    subgraph tg2_Task[\"Task\"]\n      direction LR\n      tg2_r[\"Reader\"]\n      tg2_c[\"Channel\"]\n      tg2_w[\"Writer\"]\n    end\n    t9[\"Task\"]\n    t10[\"Task\"]\n  end\n\n  subgraph tg3[\"TaskGroup\"]\n    direction LR\n    subgraph tg3_Task[\"Task\"]\n      tg3_r[\"Reader\"]\n      tg3_c[\"Channel\"]\n      tg3_w[\"Writer\"]\n    end\n    t11[\"Task\"]\n    t12[\"Task\"]\n  end\nend\n\nJob == split ==&gt; task\ntask == Schedule ==&gt; taskgroup</code></pre>"},{"location":"#core-module-introduction","title":"Core Module Introduction","text":"<ol> <li>Addax completes a single data synchronization job, which we call a Job. After Addax receives a Job, it will start a process to complete the entire job synchronization process. The Addax Job module is the central management node for a single job, responsible for data cleaning, sub-task splitting (converting a single job calculation into multiple sub-Tasks), TaskGroup management, and other functions.</li> <li>After the Addax Job starts, it will split the Job into multiple small Tasks (sub-tasks) according to different source-side splitting strategies for concurrent execution. Task is the smallest unit of Addax jobs, and each Task is responsible for a portion of the data synchronization work.</li> <li>After splitting multiple Tasks, the Addax Job will call the Scheduler module, which reorganizes the split Tasks into TaskGroups according to the configured concurrency. Each TaskGroup is responsible for running all assigned Tasks with a certain degree of concurrency. The default concurrency of a single task group is 5.</li> <li>Each Task is started by the TaskGroup. After the Task starts, it will start the <code>Reader\u2014&gt;Channel\u2014&gt;Writer</code> thread to complete the task synchronization work.</li> <li>After the Addax job runs, the Job monitors and waits for multiple TaskGroup modules to complete tasks. After all TaskGroup tasks are completed, the Job exits successfully. Otherwise, it exits abnormally with a non-zero exit code.</li> </ol>"},{"location":"#scheduling-process","title":"Scheduling Process","text":"<p>For example, a user submitted a job configured with 20 concurrent threads, aiming to synchronize data from 100 MySQL sub-tables to Oracle. The scheduling decision logic is:</p> <ol> <li>Addax Job splits into 100 Tasks based on database and table partitioning.</li> <li>Based on 20 concurrent threads, it calculates that <code>20/5 = 4</code> TaskGroups need to be allocated.</li> <li>The 4 TaskGroups evenly distribute the 100 split Tasks, with each TaskGroup responsible for running 25 Tasks with 5 concurrent threads.</li> </ol>"},{"location":"#core-advantages","title":"Core Advantages","text":""},{"location":"#reliable-data-quality-monitoring","title":"Reliable Data Quality Monitoring","text":"<ul> <li>Perfect solution for data transmission type distortion issues</li> </ul> <p>Supports all strong data types, each plugin has its own data type conversion strategy, allowing data to be transmitted to the destination completely and without loss.</p> <ul> <li>Provides full-link traffic and data volume runtime monitoring for jobs</li> </ul> <p>During operation, comprehensive displays of job status, data traffic, data speed, execution progress, and other information can be provided, allowing users to understand job status in real-time. It can also intelligently compare the speed between source and destination during job execution, providing users with more performance troubleshooting information.</p> <ul> <li>Provides dirty data detection</li> </ul> <p>In the process of large-scale data transmission, many data transmission errors (such as type conversion errors) will inevitably occur due to various reasons. Addax considers such data as dirty data. Addax can currently achieve precise filtering, identification, collection, and display of dirty data, providing users with multiple dirty data processing modes to accurately control data quality!</p>"},{"location":"#rich-data-transformation-functions","title":"Rich Data Transformation Functions","text":"<p>As an ETL tool serving big data, in addition to providing data snapshot migration functions, it also provides rich data transformation functions, allowing data to easily complete data desensitization, completion, filtering, and other data transformation functions during transmission. It also provides automatic <code>groovy</code> functions, allowing users to customize transformation functions. For details, please see the transformer detailed introduction.</p>"},{"location":"#precise-speed-control","title":"Precise Speed Control","text":"<p>Provides three flow control modes including channel (concurrency), record flow, and byte flow, allowing you to control your job speed at will, letting your job achieve optimal synchronization speed within the range that the database can bear.</p> <pre><code>{\n  \"speed\": {\n    \"channel\": 5,\n    \"byte\": 1048576,\n    \"record\": 10000\n  }\n}\n</code></pre>"},{"location":"#strong-synchronization-performance","title":"Strong Synchronization Performance","text":"<p>Each reader plugin has one or more splitting strategies, all of which can reasonably split jobs into multiple Tasks for parallel execution. The single-machine multi-threaded execution model can make speed increase linearly with concurrency. When both source and destination performance are sufficient, a single job can definitely saturate the network card.</p>"},{"location":"#robust-error-tolerance-mechanism","title":"Robust Error Tolerance Mechanism","text":"<p>Jobs are extremely susceptible to interference from external factors, and factors such as network interruptions and unstable data sources can easily cause synchronization jobs to report errors and stop halfway. Therefore, stability is a basic requirement for Addax. In the design of Addax, the stability of both framework and plugins has been improved. Currently, Addax can achieve multi-level local/global retries at the thread level and job level, ensuring stable operation of user jobs.</p>"},{"location":"commandline/","title":"Command Line Usage","text":"<p>Addax provides a simple command-line interface for executing data synchronization jobs.</p>"},{"location":"commandline/#basic-syntax","title":"Basic Syntax","text":"<pre><code>bin/addax.sh [options] job_config_file\n</code></pre>"},{"location":"commandline/#command-line-options","title":"Command Line Options","text":""},{"location":"commandline/#-h-help","title":"<code>-h, --help</code>","text":"<p>Display help information and exit.</p> <pre><code>bin/addax.sh -h\n</code></pre>"},{"location":"commandline/#-v-version","title":"<code>-v, --version</code>","text":"<p>Display version information and exit.</p> <pre><code>bin/addax.sh -v\n</code></pre>"},{"location":"commandline/#-m-mode","title":"<code>-m, --mode</code>","text":"<p>Set execution mode. Available options:</p> <ul> <li><code>standalone</code>: Standalone mode (default)</li> <li><code>local</code>: Local mode</li> </ul> <pre><code>bin/addax.sh -m standalone job.json\n</code></pre>"},{"location":"commandline/#-j-jvm","title":"<code>-j, --jvm</code>","text":"<p>Set JVM parameters.</p> <pre><code>bin/addax.sh -j \"-Xms1g -Xmx4g\" job.json\n</code></pre>"},{"location":"commandline/#-p-params","title":"<code>-p, --params</code>","text":"<p>Pass runtime parameters for variable substitution in job configuration.</p> <pre><code>bin/addax.sh -p \"-Dhost=localhost -Dport=3306\" job.json\n</code></pre>"},{"location":"commandline/#-reader-plugin","title":"<code>--reader-plugin</code>","text":"<p>Display information about a specific reader plugin.</p> <pre><code>bin/addax.sh --reader-plugin mysqlreader\n</code></pre>"},{"location":"commandline/#-writer-plugin","title":"<code>--writer-plugin</code>","text":"<p>Display information about a specific writer plugin.</p> <pre><code>bin/addax.sh --writer-plugin postgresqlwriter\n</code></pre>"},{"location":"commandline/#usage-examples","title":"Usage Examples","text":""},{"location":"commandline/#basic-job-execution","title":"Basic Job Execution","text":"<p>Execute a simple synchronization job:</p> <pre><code>bin/addax.sh job/mysql_to_postgres.json\n</code></pre>"},{"location":"commandline/#job-with-custom-jvm-settings","title":"Job with Custom JVM Settings","text":"<p>Execute job with custom memory settings:</p> <pre><code>bin/addax.sh -j \"-Xms2g -Xmx8g -XX:+UseG1GC\" job/large_table_sync.json\n</code></pre>"},{"location":"commandline/#job-with-runtime-parameters","title":"Job with Runtime Parameters","text":"<p>Execute job with variable substitution:</p> <pre><code>bin/addax.sh -p \"-Dsource.host=db1.example.com -Dtarget.host=db2.example.com\" job/template.json\n</code></pre> <p>Where <code>template.json</code> contains variables like:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"parameter\": {\n            \"jdbcUrl\": \"jdbc:mysql://${source.host}:3306/mydb\"\n          }\n        },\n        \"writer\": {\n          \"parameter\": {\n            \"jdbcUrl\": \"jdbc:postgresql://${target.host}:5432/mydb\"\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"commandline/#debug-mode","title":"Debug Mode","text":"<p>Run job with debug output:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true\" job.json\n</code></pre>"},{"location":"commandline/#performance-monitoring","title":"Performance Monitoring","text":"<p>Run job with performance monitoring enabled:</p> <pre><code>bin/addax.sh -j \"-Daddax.monitor=true\" job.json\n</code></pre>"},{"location":"commandline/#exit-codes","title":"Exit Codes","text":"<p>Addax uses the following exit codes:</p> <ul> <li><code>0</code>: Job completed successfully</li> <li><code>1</code>: Job failed due to configuration error</li> <li><code>2</code>: Job failed due to runtime error</li> <li><code>3</code>: Job killed by user or system</li> </ul>"},{"location":"commandline/#configuration-override","title":"Configuration Override","text":"<p>You can override configuration settings via command line parameters:</p>"},{"location":"commandline/#override-speed-settings","title":"Override Speed Settings","text":"<pre><code>bin/addax.sh -p \"-Djob.setting.speed.channel=5\" job.json\n</code></pre>"},{"location":"commandline/#override-error-limits","title":"Override Error Limits","text":"<pre><code>bin/addax.sh -p \"-Djob.setting.errorLimit.record=100\" job.json\n</code></pre>"},{"location":"commandline/#plugin-information","title":"Plugin Information","text":""},{"location":"commandline/#list-available-plugins","title":"List Available Plugins","text":"<pre><code># List all reader plugins\nbin/addax.sh --reader-plugin\n\n# List all writer plugins  \nbin/addax.sh --writer-plugin\n</code></pre>"},{"location":"commandline/#get-plugin-details","title":"Get Plugin Details","text":"<pre><code># Get MySQL reader details\nbin/addax.sh --reader-plugin mysqlreader\n\n# Get PostgreSQL writer details\nbin/addax.sh --writer-plugin postgresqlwriter\n</code></pre>"},{"location":"commandline/#environment-variables","title":"Environment Variables","text":"<p>You can set the following environment variables to customize Addax behavior:</p>"},{"location":"commandline/#addax_home","title":"<code>ADDAX_HOME</code>","text":"<p>Set the Addax installation directory:</p> <pre><code>export ADDAX_HOME=/opt/addax\n</code></pre>"},{"location":"commandline/#java_home","title":"<code>JAVA_HOME</code>","text":"<p>Set the Java installation directory:</p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk\n</code></pre>"},{"location":"commandline/#addax_opts","title":"<code>ADDAX_OPTS</code>","text":"<p>Set default JVM options:</p> <pre><code>export ADDAX_OPTS=\"-Xms1g -Xmx4g\"\n</code></pre>"},{"location":"commandline/#logging-configuration","title":"Logging Configuration","text":"<p>Addax uses logback for logging. You can customize logging by:</p>"},{"location":"commandline/#setting-log-level","title":"Setting Log Level","text":"<pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-debug.xml\" job.json\n</code></pre>"},{"location":"commandline/#custom-log-file","title":"Custom Log File","text":"<pre><code>bin/addax.sh -j \"-Daddax.log.file=/var/log/addax/job.log\" job.json\n</code></pre>"},{"location":"commandline/#best-practices","title":"Best Practices","text":""},{"location":"commandline/#resource-management","title":"Resource Management","text":"<ul> <li>Use appropriate JVM heap sizes based on your data volume</li> <li>Monitor memory usage during large data transfers</li> <li>Set reasonable channel numbers based on system capacity</li> </ul>"},{"location":"commandline/#error-handling","title":"Error Handling","text":"<ul> <li>Always check exit codes in scripts</li> <li>Set appropriate error limits for your use case</li> <li>Review logs for detailed error information</li> </ul>"},{"location":"commandline/#security","title":"Security","text":"<ul> <li>Avoid passing passwords via command line (use configuration files)</li> <li>Use encrypted password files when possible</li> <li>Limit file permissions on configuration files</li> </ul>"},{"location":"commandline/#performance","title":"Performance","text":"<ul> <li>Test with different channel numbers to find optimal concurrency</li> <li>Use speed limits to prevent overwhelming source/target systems</li> <li>Monitor system resources during execution</li> </ul> <p>For more detailed information about specific plugins and configuration options, please refer to the job configuration guide and individual plugin documentation.</p>"},{"location":"debug/","title":"Debug Mode","text":"<p>Addax provides debugging capabilities to help troubleshoot issues during data synchronization jobs.</p>"},{"location":"debug/#enabling-debug-mode","title":"Enabling Debug Mode","text":""},{"location":"debug/#command-line-debug","title":"Command Line Debug","text":"<p>Enable debug mode by adding JVM parameters:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true\" job.json\n</code></pre>"},{"location":"debug/#configuration-debug","title":"Configuration Debug","text":"<p>Add debug configuration in your job file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"debug\": true\n    }\n  }\n}\n</code></pre>"},{"location":"debug/#debug-features","title":"Debug Features","text":""},{"location":"debug/#detailed-logging","title":"Detailed Logging","text":"<p>Debug mode provides more detailed logging including:</p> <ul> <li>SQL statements being executed</li> <li>Data transformation details</li> <li>Performance metrics</li> <li>Error stack traces</li> </ul>"},{"location":"debug/#data-sampling","title":"Data Sampling","text":"<p>When debug mode is enabled, Addax will log sample data:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.sample=10\" job.json\n</code></pre> <p>This will log the first 10 records for inspection.</p>"},{"location":"debug/#memory-monitoring","title":"Memory Monitoring","text":"<p>Monitor memory usage during execution:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.memory=true\" job.json\n</code></pre>"},{"location":"debug/#common-debug-scenarios","title":"Common Debug Scenarios","text":""},{"location":"debug/#connection-issues","title":"Connection Issues","text":"<p>If you're experiencing connection problems:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.connection=true\" job.json\n</code></pre>"},{"location":"debug/#performance-issues","title":"Performance Issues","text":"<p>For performance debugging:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.performance=true\" job.json\n</code></pre>"},{"location":"debug/#data-type-issues","title":"Data Type Issues","text":"<p>For data type conversion problems:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug=true -Daddax.debug.datatype=true\" job.json\n</code></pre>"},{"location":"debug/#log-levels","title":"Log Levels","text":"<p>Set different log levels for various components:</p> <pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-debug.xml\" job.json\n</code></pre>"},{"location":"debug/#debug-output-examples","title":"Debug Output Examples","text":""},{"location":"debug/#sql-execution-debug","title":"SQL Execution Debug","text":"<pre><code>DEBUG [Reader-0] - Executing SQL: SELECT id, name, age FROM users WHERE id BETWEEN ? AND ?\nDEBUG [Reader-0] - SQL Parameters: [1, 1000]\nDEBUG [Reader-0] - Fetched 856 records in 1.23 seconds\n</code></pre>"},{"location":"debug/#data-sample-debug","title":"Data Sample Debug","text":"<pre><code>DEBUG [Channel-0] - Sample record: {\"id\": 1, \"name\": \"John Doe\", \"age\": 30}\nDEBUG [Channel-0] - Sample record: {\"id\": 2, \"name\": \"Jane Smith\", \"age\": 25}\n</code></pre>"},{"location":"debug/#performance-debug","title":"Performance Debug","text":"<pre><code>DEBUG [Job] - Channel statistics:\n  - Channel 0: 1000 records/s, 128KB/s\n  - Channel 1: 950 records/s, 122KB/s\n  - Channel 2: 1050 records/s, 135KB/s\n</code></pre>"},{"location":"debug/#troubleshooting-tips","title":"Troubleshooting Tips","text":""},{"location":"debug/#high-memory-usage","title":"High Memory Usage","text":"<p>Monitor memory usage and adjust heap size:</p> <pre><code>bin/addax.sh -j \"-Xms2g -Xmx8g -Daddax.debug.memory=true\" job.json\n</code></pre>"},{"location":"debug/#slow-performance","title":"Slow Performance","text":"<p>Identify bottlenecks:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug.performance=true -Daddax.debug.channel=true\" job.json\n</code></pre>"},{"location":"debug/#data-quality-issues","title":"Data Quality Issues","text":"<p>Check for data conversion errors:</p> <pre><code>bin/addax.sh -j \"-Daddax.debug.datatype=true -Daddax.debug.sample=100\" job.json\n</code></pre>"},{"location":"debug/#custom-debug-configuration","title":"Custom Debug Configuration","text":"<p>Create a custom logback configuration for specific debug needs:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n    &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder&gt;\n            &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;logger name=\"com.wgzhao.addax\" level=\"DEBUG\"/&gt;\n    &lt;logger name=\"com.wgzhao.addax.core.job\" level=\"TRACE\"/&gt;\n\n    &lt;root level=\"INFO\"&gt;\n        &lt;appender-ref ref=\"STDOUT\"/&gt;\n    &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Save as <code>conf/logback-custom.xml</code> and use:</p> <pre><code>bin/addax.sh -j \"-Dlogback.configurationFile=conf/logback-custom.xml\" job.json\n</code></pre>"},{"location":"debug/#production-debugging","title":"Production Debugging","text":"<p>For production environments, use selective debugging:</p> <pre><code># Only log errors and warnings\nbin/addax.sh -j \"-Daddax.debug.errors=true\" job.json\n\n# Log performance metrics only\nbin/addax.sh -j \"-Daddax.debug.performance=true\" job.json\n</code></pre> <p>This helps identify issues without overwhelming log output.</p>"},{"location":"encrypt_password/","title":"Password Encryption","text":"<p>Addax supports password encryption to enhance security when storing database credentials in configuration files.</p>"},{"location":"encrypt_password/#overview","title":"Overview","text":"<p>Instead of storing passwords in plain text in job configuration files, you can use encrypted passwords. This feature helps protect sensitive credentials, especially in shared environments or version control systems.</p>"},{"location":"encrypt_password/#generating-encrypted-passwords","title":"Generating Encrypted Passwords","text":"<p>Use the provided script to encrypt passwords:</p> <pre><code>bin/encrypt.sh &lt;password&gt;\n</code></pre> <p>Example:</p> <pre><code>bin/encrypt.sh mypassword123\n</code></pre> <p>Output: <pre><code>Encrypted password: addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\n</code></pre></p>"},{"location":"encrypt_password/#using-encrypted-passwords","title":"Using Encrypted Passwords","text":""},{"location":"encrypt_password/#in-job-configuration","title":"In Job Configuration","text":"<p>Replace plain text passwords with encrypted ones in your job configuration:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"dbuser\",\n            \"password\": \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/testdb\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"encrypt_password/#environment-variables","title":"Environment Variables","text":"<p>You can also store encrypted passwords in environment variables:</p> <pre><code>export DB_PASSWORD=\"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\"\n</code></pre> <p>Then reference it in your configuration:</p> <pre><code>{\n  \"parameter\": {\n    \"password\": \"${DB_PASSWORD}\"\n  }\n}\n</code></pre>"},{"location":"encrypt_password/#encryption-algorithm","title":"Encryption Algorithm","text":"<p>Addax uses AES (Advanced Encryption Standard) for password encryption:</p> <ul> <li>Algorithm: AES-128</li> <li>Mode: CBC (Cipher Block Chaining)</li> <li>Padding: PKCS5Padding</li> <li>Key: Generated based on system properties</li> </ul>"},{"location":"encrypt_password/#security-considerations","title":"Security Considerations","text":""},{"location":"encrypt_password/#key-management","title":"Key Management","text":"<p>The encryption key is derived from system properties. For enhanced security:</p> <ol> <li> <p>Set custom encryption key:    <pre><code>export ADDAX_ENCRYPT_KEY=\"your-custom-key-here\"\n</code></pre></p> </li> <li> <p>Use different keys per environment:    <pre><code># Development\nexport ADDAX_ENCRYPT_KEY=\"dev-key-2024\"\n\n# Production  \nexport ADDAX_ENCRYPT_KEY=\"prod-key-2024\"\n</code></pre></p> </li> </ol>"},{"location":"encrypt_password/#best-practices","title":"Best Practices","text":"<ol> <li>Rotate encryption keys regularly</li> <li>Use different keys for different environments</li> <li>Store keys securely (not in source code)</li> <li>Limit access to encryption keys</li> <li>Use encrypted passwords for all sensitive data</li> </ol>"},{"location":"encrypt_password/#advanced-usage","title":"Advanced Usage","text":""},{"location":"encrypt_password/#custom-encryption-provider","title":"Custom Encryption Provider","text":"<p>You can implement a custom encryption provider by implementing the <code>PasswordEncryptor</code> interface:</p> <pre><code>public class CustomPasswordEncryptor implements PasswordEncryptor {\n    @Override\n    public String encrypt(String plainPassword) {\n        // Your custom encryption logic\n        return \"custom:enc:\" + encryptedPassword;\n    }\n\n    @Override\n    public String decrypt(String encryptedPassword) {\n        // Your custom decryption logic\n        return decryptedPassword;\n    }\n}\n</code></pre>"},{"location":"encrypt_password/#batch-encryption","title":"Batch Encryption","text":"<p>For multiple passwords, create a script:</p> <pre><code>#!/bin/bash\npasswords=(\"password1\" \"password2\" \"password3\")\n\nfor pwd in \"${passwords[@]}\"; do\n    echo \"Encrypting: $pwd\"\n    bin/encrypt.sh \"$pwd\"\n    echo \"---\"\ndone\n</code></pre>"},{"location":"encrypt_password/#configuration-examples","title":"Configuration Examples","text":""},{"location":"encrypt_password/#mysql-with-encrypted-password","title":"MySQL with Encrypted Password","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"readonly_user\",\n            \"password\": \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\",\n            \"column\": [\"*\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://prod-db:3306/analytics\",\n                \"table\": [\"user_events\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"analytics_user\",\n            \"password\": \"addax:enc:AES:9nPsrKlMN8xR2vY5aBcDfG==\",\n            \"column\": [\"*\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://warehouse:5432/analytics\",\n                \"table\": [\"user_events\"]\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"encrypt_password/#multiple-environments","title":"Multiple Environments","text":"<p>Development (dev.json): <pre><code>{\n  \"parameter\": {\n    \"password\": \"addax:enc:AES:devKeyEncryptedPassword==\"\n  }\n}\n</code></pre></p> <p>Production (prod.json): <pre><code>{\n  \"parameter\": {\n    \"password\": \"addax:enc:AES:prodKeyEncryptedPassword==\"\n  }\n}\n</code></pre></p>"},{"location":"encrypt_password/#troubleshooting","title":"Troubleshooting","text":""},{"location":"encrypt_password/#decryption-errors","title":"Decryption Errors","text":"<p>If you encounter decryption errors:</p> <ol> <li>Verify encryption key: Ensure the same key is used for encryption and decryption</li> <li>Check password format: Ensure the encrypted password starts with <code>addax:enc:AES:</code></li> <li>Validate environment: Confirm environment variables are set correctly</li> </ol>"},{"location":"encrypt_password/#password-not-recognized","title":"Password Not Recognized","text":"<pre><code># Test decryption\nbin/decrypt.sh \"addax:enc:AES:7kMgvpYVGh2kH5tZ1AxyHQ==\"\n</code></pre>"},{"location":"encrypt_password/#key-management-issues","title":"Key Management Issues","text":"<pre><code># Check current encryption key\necho $ADDAX_ENCRYPT_KEY\n\n# Set temporary key for testing\nexport ADDAX_ENCRYPT_KEY=\"test-key-123\"\n</code></pre>"},{"location":"encrypt_password/#migration-guide","title":"Migration Guide","text":""},{"location":"encrypt_password/#from-plain-text-to-encrypted","title":"From Plain Text to Encrypted","text":"<ol> <li>Identify all passwords in configuration files</li> <li>Encrypt each password using the encrypt script</li> <li>Replace plain text with encrypted values</li> <li>Test the configuration to ensure it works</li> <li>Update documentation with new security procedures</li> </ol>"},{"location":"encrypt_password/#example-migration-script","title":"Example Migration Script","text":"<pre><code>#!/bin/bash\n\n# Backup original files\ncp config/job.json config/job.json.backup\n\n# Replace passwords (adjust patterns as needed)\nsed -i 's/\"password\": \"plainpassword\"/\"password\": \"addax:enc:AES:encryptedvalue\"/g' config/job.json\n\necho \"Migration complete. Test the configuration before deploying.\"\n</code></pre> <p>This encryption feature significantly improves the security posture of your Addax deployments while maintaining ease of use.</p>"},{"location":"plugin_development/","title":"Plugin Development","text":"<p>This guide is primarily for developers who need to develop Addax plugins to meet their specific needs.</p>"},{"location":"plugin_development/#addax-flow","title":"Addax Flow","text":"<p>The general process for running a task in Addax is as follows:</p> <p></p> <p>The startup steps are:</p> <ol> <li>Parse configurations, including <code>job.json</code>, <code>core.json</code>, and <code>plugin.json</code>.</li> <li>Set the <code>jobId</code> in the <code>configuration</code>.</li> <li>Start the Engine via <code>Engine.start()</code> to enter the startup procedure.</li> <li>Set the <code>RUNTIME_MODE</code> in the <code>configuration</code>.</li> <li>Start via the <code>JobContainer</code>'s <code>start()</code> method.</li> <li>Execute the job's <code>preHandler()</code>, <code>init()</code>, <code>prepare()</code>, <code>split()</code>, <code>schedule()</code>, <code>post()</code>, and <code>postHandle()</code> methods in sequence.</li> <li>The <code>init()</code> method involves initializing the reader and writer plugins based on the configuration. This includes hot-loading JAR packages and calling the plugin's <code>init()</code> method, as well as setting the reader and writer's configuration.</li> <li>The <code>prepare()</code> method involves initializing the reader and writer plugins by calling their respective <code>prepare()</code> methods. Each plugin has its own <code>jarLoader</code>, which inherits from <code>URLClassLoader</code>.</li> <li>The <code>split()</code> method adjusts the number of channels via <code>adjustChannelNumber()</code> and performs the most granular splitting for the reader and writer. It's important to note that the writer's split result must reference the reader's split result to ensure an equal number of splits, satisfying the 1:1 channel model.</li> <li>The channel count is mainly determined by byte and record rate limiting, which is calculated as the first step in the <code>split()</code> function.</li> <li>In the <code>split()</code> method, the reader plugin will split based on the channel value, but some reader plugins might not use this value. The writer plugin will always split 1:1 based on the reader's splits.</li> <li>The <code>mergeReaderAndWriterTaskConfigs()</code> method inside <code>split()</code> is responsible for merging the configurations of the reader, writer, and transformer to generate task configurations and override the <code>job.content</code> configuration.</li> <li>The <code>schedule()</code> method allocates and generates <code>TaskGroup</code> objects based on the task configurations generated by <code>split()</code>. The number of task groups is determined by dividing the total number of tasks by the number of tasks a single <code>TaskGroup</code> can support.</li> <li><code>schedule()</code> is executed internally by <code>AbstractScheduler</code>'s <code>schedule()</code> method, which then calls <code>startAllTaskGroup()</code> to create all <code>TaskGroupContainer</code>s to organize the related tasks. <code>TaskGroupContainerRunner</code> is responsible for running the <code>TaskGroupContainer</code> to execute the assigned tasks. The concrete implementation class for the scheduler is <code>ProcessInnerScheduler</code>.</li> <li><code>taskGroupContainerExecutorService</code> starts a fixed thread pool to execute <code>TaskGroupContainerRunner</code> objects. The <code>run()</code> method of <code>TaskGroupContainerRunner</code> calls <code>taskGroupContainer.start()</code>, which creates a <code>TaskExecutor</code> for each channel and starts the task via <code>taskExecutor.doStart()</code>.</li> </ol>"},{"location":"plugin_development/#plugin-mechanism","title":"Plugin Mechanism","text":"<p>To handle the differences between various data sources while providing consistent synchronization primitives and extensibility, <code>Addax</code> uses a <code>Framework</code> + <code>Plugin</code> model:</p> <ul> <li>Plugins only need to care about reading from or writing to the data source itself.</li> <li>Common synchronization issues, such as type conversion, performance, and statistics, are handled by the framework.</li> </ul> <p>As a plugin developer, you need to focus on two issues:</p> <ol> <li>The correctness of reading and writing data from the specific data source.</li> <li>How to communicate with the framework and use it correctly.</li> </ol>"},{"location":"plugin_development/#framework-from-a-plugins-perspective","title":"Framework from a Plugin's Perspective","text":""},{"location":"plugin_development/#logical-execution-model","title":"Logical Execution Model","text":"<p>Plugin developers don't need to worry about too much; they mainly need to focus on reading from and writing to specific systems, and how their code is logically executed\u2014which method is called and when. Before that, you need to understand the following concepts:</p> <ul> <li><code>Job</code>: Describes a synchronization job from a source to a destination, representing the smallest business unit of data synchronization. For example, synchronizing a table from MySQL to a table in PostgreSQL.</li> <li><code>Task</code>: The smallest execution unit, obtained by splitting a <code>Job</code> to maximize performance. For example, a <code>Job</code> reading from a sharded MySQL database with 1024 sub-tables can be split into 1024 read <code>Tasks</code>, executed concurrently.</li> <li><code>TaskGroup</code>: A collection of <code>Tasks</code>. A set of <code>Tasks</code> executed within the same <code>TaskGroupContainer</code> is called a <code>TaskGroup</code>.</li> <li><code>JobContainer</code>: The <code>Job</code> executor, a work unit responsible for the global splitting, scheduling, pre- and post-statements of a <code>Job</code>. Similar to the JobTracker in Yarn.</li> <li><code>TaskGroupContainer</code>: The <code>TaskGroup</code> executor, a work unit responsible for executing a group of <code>Tasks</code>. Similar to the TaskTracker in Yarn.</li> </ul> <p>In short, a <code>Job</code> is split into <code>Tasks</code>, which are executed in containers provided by the framework. The plugin only needs to implement the logic for <code>Job</code> and <code>Task</code>.</p>"},{"location":"plugin_development/#programming-interface","title":"Programming Interface","text":"<p>So, how should the logic of <code>Job</code> and <code>Task</code> be mapped to specific code?</p> <p>First, the entry class of a plugin must extend the <code>Reader</code> or <code>Writer</code> abstract class and implement their respective <code>Job</code> and <code>Task</code> inner abstract classes. The implementations of <code>Job</code> and <code>Task</code> must be in the form of inner classes.</p> <pre><code>public class SomeReader\n        extends Reader\n{\n    public static class Job\n            extends Reader.Job\n    {\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public List&lt;Configuration&gt; split(int adviceNumber)\n        {\n            return null;\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n\n    public static class Task\n            extends Reader.Task\n    {\n\n        @Override\n        public void init()\n        {\n        }\n\n        @Override\n        public void prepare()\n        {\n        }\n\n        @Override\n        public void startRead(RecordSender recordSender)\n        {\n        }\n\n        @Override\n        public void post()\n        {\n        }\n\n        @Override\n        public void destroy()\n        {\n        }\n    }\n}\n</code></pre> <p>The <code>Job</code> interface functions are as follows:</p> <ul> <li><code>init</code>: Initialization work for the Job object. At this point, you can get the plugin-related configuration via <code>super.getPluginJobConf()</code>. The read plugin gets the <code>reader</code> section of the configuration, and the write plugin gets the <code>writer</code> section.</li> <li><code>prepare</code>: Global preparation work, such as clearing the target table in MySQL.</li> <li><code>split</code>: Splits the job into <code>Tasks</code>. The <code>adviceNumber</code> parameter is the number of splits suggested by the framework, usually the concurrency level configured at runtime. The return value is a list of <code>Task</code> configurations.</li> <li><code>post</code>: Global post-processing work, such as the <code>rename</code> operation for a MySQL writer after synchronizing to a shadow table.</li> <li><code>destroy</code>: Destruction work for the Job object itself.</li> </ul> <p>The <code>Task</code> interface functions are as follows:</p> <ul> <li><code>init</code>: Initialization of the Task object. At this point, you can get the <code>Task</code>-related configuration via <code>super.getPluginJobConf()</code>. This configuration is one of the configurations returned by the <code>Job#split</code> method.</li> <li><code>prepare</code>: Local preparation work.</li> <li><code>startRead</code>: Reads data from the data source and writes it to the <code>RecordSender</code>. The <code>RecordSender</code> writes the data to the buffer queue connecting the <code>Reader</code> and <code>Writer</code>.</li> <li><code>startWrite</code>: Reads data from the <code>RecordReceiver</code> and writes it to the target data source. The data in the <code>RecordReceiver</code> comes from the buffer queue between the <code>Reader</code> and <code>Writer</code>.</li> <li><code>post</code>: Local post-processing work.</li> <li><code>destroy</code>: Destruction work for the Task object itself.</li> </ul> <p>Please note:</p> <ul> <li>There must be no shared variables between <code>Job</code> and <code>Task</code>, because in a distributed runtime, there is no guarantee that shared variables will be initialized correctly. They can only depend on each other through configuration files.</li> <li><code>prepare</code> and <code>post</code> exist in both <code>Job</code> and <code>Task</code>. The plugin needs to determine where to perform operations based on the actual situation.</li> </ul> <p>The framework executes the <code>Job</code> and <code>Task</code> interfaces in the following order:</p> <pre><code>stateDiagram-v2\ndirection TB\nInit:::job --&gt; Prepare:::job\nPrepare --&gt; Split:::job\nSplit --&gt; Schedule:::fw\nstate Schedule {\n    direction LR\n    init\\nprepare\\nstartRead\\npost\\ndestroy1 --&gt; init\\nprepare\\nstartRead\\npost\\ndestroy : Channel\n}\nSchedule --&gt; Post:::job\n\nclassDef job fill:yellow\nclassDef fw fill:#c6fac4\nclassDef ctask fill:blue</code></pre> <p>In the diagram above, yellow represents the execution phase of the <code>Job</code> part, gray represents the execution phase of the <code>Task</code> part, and green represents the framework's execution phase.</p> <p>The related class relationships are as follows:</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\n    class Pluginable {\n    + init()\n    + destroy()\n    + others()\n    }\n    class AbstractPlugin {\n        + prepare()\n        + post()\n        + others()\n    }\n    class AbstractJobPlugin {\n        + getJobPluginCollector(): JobPluginCollector\n        + setJobPluginCollector(JobPluginCollector)\n    }\n\n    class AbstractTaskPlugin {\n        + getTaskPluginCollector(): TaskPluginCollector\n        + setTaskPluginCollector(TaskPluginCollector)\n    }\n    class Reader_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Writer_Job {\n        + split(init): List&lt;&lt;Configuration&gt;&gt;\n    }\n\n    class Reader_Task {\n        + startRead(RecordSender)\n    }\n\n    class Writer_Task {\n        + startWrite(RecordReceiver)\n    }\n\n    AbstractJobPlugin &lt;|-- Reader_Job\n    AbstractJobPlugin &lt;|-- Writer_Job\n\n    AbstractTaskPlugin &lt;|-- Reader_Task\n    AbstractTaskPlugin &lt;|-- Writer_Task\n\n    AbstractPlugin &lt;|-- AbstractJobPlugin\n    AbstractPlugin &lt;|-- AbstractTaskPlugin\n\n    Pluginable &lt;|-- AbstractPlugin\n</code></pre>"},{"location":"plugin_development/#plugin-definition","title":"Plugin Definition","text":"<p>In each plugin's project, there is a <code>plugin.json</code> file that defines the plugin's information, including its entry class. For example:</p> <pre><code>{\n  \"name\": \"mysqlwriter\",\n  \"class\": \"com.wgzhao.addax.plugin.writer.mysqlwriter.MysqlWriter\",\n  \"description\": \"Use Jdbc connect to database, execute insert sql.\",\n  \"developer\": \"wgzhao\"\n}\n</code></pre> <ul> <li><code>name</code>: The plugin name, case-sensitive. The framework searches for the plugin based on the name specified by the user in the configuration file. Very important.</li> <li><code>class</code>: The fully qualified name of the entry class. The framework creates an instance of the entry class via reflection. Very important.</li> <li><code>description</code>: Descriptive information.</li> <li><code>developer</code>: The developer.</li> </ul>"},{"location":"plugin_development/#packaging-and-publishing","title":"Packaging and Publishing","text":"<p><code>Addax</code> uses <code>assembly</code> for packaging. The packaging commands are as follows:</p> <pre><code>mvn clean package\nmvn package assembly:single\n</code></pre> <p><code>Addax</code> plugins need to follow a uniform directory structure:</p> <pre><code>${ADDAX_HOME}\n\u251c\u2500\u2500 bin\n\u2502     \u251c\u2500\u2500 addax.sh\n\u251c\u2500\u2500 conf\n\u2502     \u251c\u2500\u2500 core.json\n\u2502     \u2514\u2500\u2500 logback.xml\n\u251c\u2500\u2500 job\n\u251c\u2500\u2500 lib\n\u2502     \u251c\u2500\u2500 addax-common-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-core-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-rdbms-&lt;version&gt;.jar\n\u2502     \u251c\u2500\u2500 addax-storage-&lt;version&gt;.jar\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 plugin\n\u2502     \u251c\u2500\u2500 reader\n\u2502     \u2502     \u251c\u2500\u2500 cassandrareader\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 cassandrareader-&lt;version&gt;.jar\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 libs\n\u2502     \u2502     \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502     \u2502     \u2502     \u251c\u2500\u2500 plugin.json\n\u2502     \u2502     \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u2502     \u2514\u2500\u2500 writer\n\u2502         \u251c\u2500\u2500 cassandrawriter\n\u2502         \u2502     \u251c\u2500\u2500 cassandrawriter-&lt;version&gt;.jar\n\u2502         \u2502     \u251c\u2500\u2500 libs\n\u2502         \u2502     \u2502     \u251c\u2500\u2500 &lt;symbol link to shared folder&gt;\n\u2502         \u2502     \u251c\u2500\u2500 plugin.json\n\u2502         \u2502     \u2514\u2500\u2500 plugin_job_template.json\n\u251c\u2500\u2500 shared\n</code></pre> <ul> <li><code>${ADDAX_HOME}/bin</code>: Executable program directory</li> <li><code>${ADDAX_HOME}/conf</code>: Framework configuration directory</li> <li><code>${ADDAX_HOME}/lib</code>: Framework dependency library directory</li> <li><code>${ADDAX_HOME}/shared</code>: Plugin dependency directory</li> <li><code>${ADDAX_HOME}/plugin</code>: Plugin directory</li> </ul> <p>The plugin directory is divided into <code>reader</code> and <code>writer</code> subdirectories, where read and write plugins are stored respectively. The plugin directory structure is as follows:</p> <ul> <li><code>${PLUGIN_HOME}/libs</code>: The plugin's dependency libraries. To reduce package size, these dependencies are symbolic links to the <code>shared</code> directory.</li> <li><code>${PLUGIN_HOME}/plugin-name-version.jar</code>: The plugin's own jar file.</li> <li><code>${PLUGIN_HOME}/plugin.json</code>: The plugin description file.</li> </ul> <p>Although the framework adds all jar files under <code>${PLUGIN_HOME}</code> to the <code>classpath</code> when loading the plugin, it is recommended to keep dependency library jars and the plugin's own jar separate.</p> <p>Special</p> <p>The plugin's directory name must be consistent with the plugin name defined in <code>plugin.json</code>.</p>"},{"location":"plugin_development/#configuration-file","title":"Configuration File","text":"<p><code>Addax</code> uses <code>json</code> as the format for configuration files. A typical <code>Addax</code> task configuration is as follows:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>Addax</code> framework has a <code>core.json</code> configuration file that specifies the framework's default behavior. A task's configuration can specify configuration items that already exist in the framework, and these will have a higher priority, overriding the settings in <code>core.json</code>.</p> <p>The <code>value</code> part of <code>job.content.reader.parameter</code> in the configuration is passed to <code>Reader.Job</code>; the <code>value</code> part of <code>job.content.writer.parameter</code> is passed to <code>Writer.Job</code>. <code>Reader.Job</code> and <code>Writer.Job</code> can retrieve them using <code>super.getPluginJobConf()</code>.</p>"},{"location":"plugin_development/#how-to-design-configuration-parameters","title":"How to Design Configuration Parameters","text":"<p>Designing the configuration file is the first step in plugin development!</p> <p>The <code>parameter</code> section under <code>reader</code> and <code>writer</code> in the task configuration contains the plugin's configuration parameters. These parameters should follow these principles:</p> <ul> <li>Camel Case: All configuration items should use lower camel case, with the first letter being lowercase.</li> <li>Orthogonality Principle: Configuration items must be orthogonal, with no overlapping functionality or hidden rules.</li> <li>Rich Types: Use JSON types appropriately to reduce unnecessary processing logic and the possibility of errors.<ul> <li>Use the correct data types. For example, for a <code>bool</code> type, use <code>true</code>/<code>false</code> instead of <code>\"yes\"</code>/<code>\"true\"</code>/<code>0</code>.</li> <li>Use collection types reasonably, for example, use an array instead of a delimited string.</li> </ul> </li> <li>Consistency with Similar Plugins: Follow the conventions of similar plugin types. For example, the <code>connection</code> parameter for relational databases usually has the following structure:</li> </ul> <pre><code>{\n  \"connection\": [\n    {\n      \"table\": [\"table_1\", \"table_2\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.1:3306/database_1\",\n        \"jdbc:mysql://127.0.0.2:3306/database_1_slave\"\n      ]\n    },\n    {\n      \"table\": [\"table_3\", \"table_4\"],\n      \"jdbcUrl\": [\n        \"jdbc:mysql://127.0.0.3:3306/database_2\",\n        \"jdbc:mysql://127.0.0.4:3306/database_2_slave\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"plugin_development/#how-to-use-the-configuration-class","title":"How to Use the <code>Configuration</code> Class","text":"<p>To simplify operations on <code>json</code>, <code>Addax</code> provides a simple DSL to be used with the <code>Configuration</code> class.</p> <p><code>Configuration</code> provides common operations for reading and writing configuration items, such as <code>get</code>, <code>get with type</code>, <code>get with default value</code>, and <code>set</code>, as well as methods like <code>clone</code> and <code>toJSON</code>. Read and write operations require a <code>path</code> parameter.</p> <ol> <li>A child map is represented by <code>.key</code>. The first dot in the <code>path</code> is omitted.</li> <li>An array element is represented by <code>[index]</code>.</li> </ol> <p>For example, to operate on the following json:</p> <pre><code>{\n  \"a\": {\n    \"b\": {\n      \"c\": 2\n    },\n    \"f\": [\n      1,\n      2,\n      {\n        \"g\": true,\n        \"h\": false\n      },\n      4\n    ]\n  },\n  \"x\": 4\n}\n</code></pre> <p>When calling the <code>configuration.get(path)</code> method, the results for the following path values are:</p> <ul> <li><code>x</code>: <code>4</code></li> <li><code>a.b.c</code>: <code>2</code></li> <li><code>a.b.c.d</code>: <code>null</code></li> <li><code>a.f[0]</code>: <code>1</code></li> <li><code>a.f[2].g</code>: <code>true</code></li> </ul> <p>Note that because the configuration seen by the plugin is only a part of the whole configuration, you need to be aware of the current root path when using a <code>Configuration</code> object.</p> <p>For more operations on <code>Configuration</code>, please refer to Configuration.java.</p>"},{"location":"plugin_development/#plugin-data-transfer","title":"Plugin Data Transfer","text":"<p>Like the general <code>producer-consumer</code> model, <code>Reader</code> and <code>Writer</code> plugins also transfer data through a <code>channel</code>. The <code>channel</code> can be in-memory or persistent.</p> <p>A piece of data in the <code>channel</code> is a <code>Record</code> object. A <code>Record</code> can contain multiple <code>Column</code> objects, which can be simply understood as records and columns in a database.</p> <p><code>Record</code> has the following methods:</p> <pre><code>public interface Record\n{\n    // Add a column to the end\n    void addColumn(Column column);\n\n    // Set a column at a specific index\n    void setColumn(int i, final Column column);\n\n    // Get a column\n    Column getColumn(int i);\n\n    // Convert to a JSON String\n    String toString();\n\n    // Get the total number of columns\n    int getColumnNumber();\n\n    // Calculate the byte size of the entire record in memory\n    int getByteSize();\n}\n</code></pre> <p>Since <code>Record</code> is an interface, the <code>Reader</code> plugin first calls <code>RecordSender.createRecord()</code> to create a <code>Record</code> instance, and then adds <code>Column</code>s one by one to the <code>Record</code>.</p> <p>The <code>Writer</code> plugin calls the <code>RecordReceiver.getFromReader()</code> method to get a <code>Record</code>, then iterates through the <code>Column</code>s and writes them to the target storage. When the <code>Reader</code> has not yet exited and the transfer is still in progress, if <code>getFromReader()</code> returns <code>null</code>, it means there is currently no data in the channel. If the transfer has already ended, it will return <code>null</code>, and the <code>Writer</code> plugin can use this to determine whether to end the <code>startWrite</code> method.</p>"},{"location":"plugin_development/#type-conversion","title":"Type Conversion","text":"<p>To standardize type conversion operations between the source and destination and ensure data integrity, Addax supports six internal data types:</p> <ul> <li><code>Long</code>: Fixed-point numbers (Int, Short, Long, BigInteger, etc.).</li> <li><code>Double</code>: Floating-point numbers (Float, Double, BigDecimal (infinite precision), etc.).</li> <li><code>String</code>: String type, with no length limit, using a universal character set (Unicode).</li> <li><code>Date</code>: Date type.</li> <li><code>Timestamp</code>: Timestamp type.</li> <li><code>Bool</code>: Boolean value.</li> <li><code>Bytes</code>: Binary data, which can store unstructured data such as MP3s.</li> </ul> <p>Correspondingly, there are seven <code>Column</code> implementations: <code>DateColumn</code>, <code>LongColumn</code>, <code>DoubleColumn</code>, <code>BytesColumn</code>, <code>StringColumn</code>, <code>BoolColumn</code>, and <code>TimestampColumn</code>.</p> <p>In addition to providing data-related methods, <code>Column</code> also provides a series of type conversion methods starting with <code>as</code>.</p> <pre><code>%%{init: {\"theme\": \"neutral\"}}%%\nclassDiagram\ndirection TB\nclass Column {\n    &lt;&lt;interface&gt;&gt;\n    - rawData: Object\n    - type: Type\n    + getRawData(): Object\n    + getType(): Type\n    + getByteSize(): init\n    + asLong(): Long\n    + asDouble(): Doule\n    + asString(): String\n    + asDate(): Date\n    + asBytes(): Bytes\n    + asBigDecimal(): BigDecimal\n    + asBoolean(): Boolean\n}\nColumn &lt;|-- Stringcolumn\nColumn &lt;|-- Doublecolumn\nColumn &lt;|-- Longcolumn\nColumn &lt;|-- Datecolumn\nColumn &lt;|-- Boolcolumn\nColumn &lt;|-- Bytescolumn</code></pre> <p>Addax's internal types use different Java types in their implementation:</p> Internal Type Implementation Type Notes Date java.util.Date Timestamp java.sql.Timestamp Can be precise to the nanosecond Long java.math.BigInteger Uses infinite-precision integers to ensure no loss of precision Double java.lang.String Represented as a String to ensure no loss of precision Bytes byte[] String java.lang.String Bool java.lang.Boolean <p>The relationships for converting between types are as follows:</p> from/to Date Long Double Bytes String Bool Date - Use millisecond timestamp Not supported Not supported Convert according to configured format Not supported Long Construct Date from ms - <code>BigDecimal.doubleValue()</code> Not supported <code>BigInteger.toString()</code> 0 is <code>false</code>, others are <code>true</code> Double Not supported <code>BigDecimal.longValue()</code> - Not supported Return internal String directly Not supported Bytes Not supported Not supported Not supported - Convert to <code>byte[]</code> with UTF-8 encoding Not supported String Parse with configured format <code>BigDecimal.longValue</code> <code>BigDecimal.doubleValue</code><sup>1</sup> Convert to <code>byte[]</code> with UTF-8 encoding<sup>2</sup> - \"true\" is <code>true</code>, \"false\" is <code>false</code> Bool Not supported <code>true</code> is <code>1L</code>, else <code>0L</code> <code>true</code> is <code>1.0</code>, else <code>0.0</code> Not supported <code>Boolean.toString()</code> -"},{"location":"plugin_development/#dirty-data-handling","title":"Dirty Data Handling","text":""},{"location":"plugin_development/#what-is-dirty-data","title":"What is Dirty Data","text":"<p>Currently, there are three main types of dirty data:</p> <ol> <li>The Reader reads an unsupported type or an illegal value.</li> <li>Unsupported type conversion, for example, converting <code>Bytes</code> to <code>Date</code>.</li> <li>Writing to the target fails, for example, an integer value exceeds the length limit when writing to MySQL.</li> </ol>"},{"location":"plugin_development/#how-to-handle-dirty-data","title":"How to Handle Dirty Data","text":"<p>In <code>Reader.Task</code> and <code>Writer.Task</code>, you can get a <code>TaskPluginCollector</code> through <code>AbstractTaskPlugin.getPluginCollector()</code>. It provides a series of <code>collectDirtyRecord</code> methods. When dirty data is encountered, you can call this method and pass in the <code>Record</code> that is considered dirty.</p> <p>Users can specify a limit on the number of dirty data records or a percentage limit in the task configuration. When the dirty data exceeds the limit, the framework will terminate the synchronization task. Plugins need to ensure that all dirty data is collected.</p>"},{"location":"plugin_development/#loading-principle","title":"Loading Principle","text":"<ol> <li>The framework scans the <code>plugin/reader</code> and <code>plugin/writer</code> directories and loads each plugin's <code>plugin.json</code> file.</li> <li>It indexes all plugin configurations using the <code>name</code> from the <code>plugin.json</code> file as the key. If duplicate plugin names are found, the framework will exit with an error.</li> <li>The user specifies the plugin name in the <code>name</code> field of the <code>reader</code>/<code>writer</code> configuration. The framework then scans all jars in the plugin's path based on the plugin type (<code>reader</code>/<code>writer</code>) and name, and adds them to a custom <code>classloader</code>.</li> <li>Based on the entry class defined in the plugin configuration, the framework instantiates the corresponding <code>Job</code> and <code>Task</code> objects via reflection.</li> </ol> <ol> <li> <p>Handles values like <code>NaN</code>, <code>Infinity</code>, <code>-Infinity</code>.\u00a0\u21a9</p> </li> <li> <p>Unless another encoding format is specified.\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#download-and-install","title":"Download and Install","text":""},{"location":"quickstart/#download","title":"Download","text":"<p>You can download the installation package from the release page, or you can build it yourself from source code.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Unzip the downloaded installation package to the directory where you want to install it:</p> <pre><code>tar -xzf addax-{version}.tar.gz\n</code></pre>"},{"location":"quickstart/#environment-requirements","title":"Environment Requirements","text":"<ul> <li>Linux or macOS operating system</li> <li>Java 8 or higher version</li> <li>Python 3.6 or higher version (required for some plugins)</li> </ul>"},{"location":"quickstart/#first-synchronization-job","title":"First Synchronization Job","text":"<p>Let's start with a simple example - synchronizing data from a text file to another text file.</p>"},{"location":"quickstart/#prepare-test-data","title":"Prepare Test Data","text":"<p>Create a test data file:</p> <pre><code>echo -e \"1,zhangsan,20\\n2,lisi,21\\n3,wangwu,22\" &gt; /tmp/test.csv\n</code></pre>"},{"location":"quickstart/#create-job-configuration","title":"Create Job Configuration","text":"<p>Create a job configuration file <code>job.json</code>:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/test.csv\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\n              {\n                \"index\": 0,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 1,\n                \"type\": \"string\"\n              },\n              {\n                \"index\": 2,\n                \"type\": \"long\"\n              }\n            ],\n            \"fieldDelimiter\": \",\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"txtfilewriter\",\n          \"parameter\": {\n            \"path\": \"/tmp/result.csv\",\n            \"fileName\": \"result\",\n            \"writeMode\": \"truncate\",\n            \"encoding\": \"UTF-8\",\n            \"fieldDelimiter\": \",\",\n            \"nullFormat\": \"\\\\N\"\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#execute-job","title":"Execute Job","text":"<p>Run the synchronization job:</p> <pre><code>bin/addax.sh job.json\n</code></pre> <p>If successful, you should see output similar to:</p> <pre><code>2023-12-07 10:30:01.234 [main] INFO  JobContainer - Job ID: 202312071030, Total records: 3, Speed: 3rec/s (30B/s), Error records: 0\n</code></pre> <p>And you should find the result file at <code>/tmp/result.csv</code>.</p>"},{"location":"quickstart/#database-synchronization-example","title":"Database Synchronization Example","text":"<p>Here's a more practical example - synchronizing data from MySQL to PostgreSQL.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>MySQL database with test data</li> <li>PostgreSQL database for destination</li> </ul>"},{"location":"quickstart/#create-job-configuration_1","title":"Create Job Configuration","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"mysql_user\",\n            \"password\": \"mysql_password\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test\",\n                \"table\": [\"user_table\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"postgres_user\",\n            \"password\": \"postgres_password\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/test\",\n                \"table\": [\"user_table\"]\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#execute-job_1","title":"Execute Job","text":"<pre><code>bin/addax.sh mysql_to_postgresql.json\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about job configuration</li> <li>Explore available reader plugins</li> <li>Explore available writer plugins</li> <li>Learn about performance tuning</li> </ul>"},{"location":"setupJob/","title":"Job Configuration","text":"<p>This document details how to configure Addax synchronization jobs. Addax uses JSON format configuration files to describe synchronization tasks.</p>"},{"location":"setupJob/#configuration-structure","title":"Configuration Structure","text":"<p>A complete job configuration file consists of three main parts:</p> <ul> <li>core: Core configuration</li> <li>job: Job configuration </li> <li>setting: Runtime settings</li> </ul> <p>Here's the basic structure:</p> <pre><code>{\n  \"core\": {\n    \"transport\": {\n      \"channel\": {\n        \"speed\": {\n          \"byte\": 1048576\n        }\n      }\n    }\n  },\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {},\n        \"writer\": {}\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"setupJob/#core-configuration","title":"Core Configuration","text":"<p>The <code>core</code> section contains system-level configuration:</p>"},{"location":"setupJob/#transport-configuration","title":"Transport Configuration","text":"<pre><code>{\n  \"core\": {\n    \"transport\": {\n      \"channel\": {\n        \"speed\": {\n          \"byte\": 1048576,\n          \"record\": -1\n        },\n        \"flowControlInterval\": 20,\n        \"capacity\": 512,\n        \"byteCapacity\": 67108864\n      }\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>speed.byte</code>: Byte-level speed limit (bytes per second), -1 means no limit</li> <li><code>speed.record</code>: Record-level speed limit (records per second), -1 means no limit  </li> <li><code>flowControlInterval</code>: Flow control check interval (milliseconds)</li> <li><code>capacity</code>: Channel capacity (number of records)</li> <li><code>byteCapacity</code>: Channel byte capacity</li> </ul>"},{"location":"setupJob/#job-configuration_1","title":"Job Configuration","text":"<p>The <code>job</code> section contains the main synchronization task configuration:</p>"},{"location":"setupJob/#content-array","title":"Content Array","text":"<p>The <code>content</code> array can contain multiple reader-writer pairs for complex synchronization scenarios:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {}\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\", \n          \"parameter\": {}\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"setupJob/#reader-configuration","title":"Reader Configuration","text":"<p>Each reader must specify:</p> <ul> <li><code>name</code>: Reader plugin name</li> <li><code>parameter</code>: Reader-specific parameters</li> </ul> <p>Example MySQL reader:</p> <pre><code>{\n  \"reader\": {\n    \"name\": \"mysqlreader\",\n    \"parameter\": {\n      \"username\": \"root\",\n      \"password\": \"password\",\n      \"column\": [\"id\", \"name\", \"age\"],\n      \"splitPk\": \"id\",\n      \"connection\": [\n        {\n          \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test\",\n          \"table\": [\"user_table\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"setupJob/#writer-configuration","title":"Writer Configuration","text":"<p>Each writer must specify:</p> <ul> <li><code>name</code>: Writer plugin name</li> <li><code>parameter</code>: Writer-specific parameters</li> </ul> <p>Example PostgreSQL writer:</p> <pre><code>{\n  \"writer\": {\n    \"name\": \"postgresqlwriter\",\n    \"parameter\": {\n      \"username\": \"postgres\",\n      \"password\": \"password\", \n      \"column\": [\"id\", \"name\", \"age\"],\n      \"connection\": [\n        {\n          \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/test\",\n          \"table\": [\"user_table\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"setupJob/#setting-configuration","title":"Setting Configuration","text":"<p>The <code>setting</code> section controls job execution behavior:</p>"},{"location":"setupJob/#speed-control","title":"Speed Control","text":"<pre><code>{\n  \"setting\": {\n    \"speed\": {\n      \"channel\": 3,\n      \"byte\": 1048576,\n      \"record\": 10000\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>channel</code>: Number of parallel channels (concurrency level)</li> <li><code>byte</code>: Byte-level speed limit per second</li> <li><code>record</code>: Record-level speed limit per second</li> </ul>"},{"location":"setupJob/#error-control","title":"Error Control","text":"<pre><code>{\n  \"setting\": {\n    \"errorLimit\": {\n      \"record\": 0,\n      \"percentage\": 0.02\n    }\n  }\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>record</code>: Maximum allowed error records</li> <li><code>percentage</code>: Maximum allowed error percentage</li> </ul>"},{"location":"setupJob/#data-type-mapping","title":"Data Type Mapping","text":"<p>Addax uses a unified internal type system for data conversion:</p> Addax Type Description Java Type long Long integer java.lang.Long double Double precision float java.lang.Double string String java.lang.String date Date/time java.util.Date bool Boolean java.lang.Boolean bytes Byte array byte[]"},{"location":"setupJob/#variable-substitution","title":"Variable Substitution","text":"<p>Addax supports variable substitution in configuration files:</p> <pre><code>{\n  \"reader\": {\n    \"parameter\": {\n      \"jdbcUrl\": \"jdbc:mysql://${host}:${port}/${database}\",\n      \"username\": \"${username}\",\n      \"password\": \"${password}\"\n    }\n  }\n}\n</code></pre> <p>Variables can be passed via command line:</p> <pre><code>bin/addax.sh job.json -p \"-Dhost=localhost -Dport=3306\"\n</code></pre>"},{"location":"setupJob/#configuration-examples","title":"Configuration Examples","text":""},{"location":"setupJob/#simple-file-transfer","title":"Simple File Transfer","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/input.txt\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\"*\"],\n            \"fieldDelimiter\": \"\\t\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"txtfilewriter\",\n          \"parameter\": {\n            \"path\": \"/tmp/output\",\n            \"fileName\": \"result.txt\",\n            \"encoding\": \"UTF-8\",\n            \"fieldDelimiter\": \"\\t\"\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"setupJob/#database-to-database","title":"Database to Database","text":"<pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"source_user\",\n            \"password\": \"source_pass\",\n            \"column\": [\"id\", \"name\", \"email\", \"created_time\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://source-host:3306/source_db\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"postgresqlwriter\",\n          \"parameter\": {\n            \"username\": \"target_user\",\n            \"password\": \"target_pass\",\n            \"column\": [\"id\", \"name\", \"email\", \"created_time\"],\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://target-host:5432/target_db\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"byte\": 10485760\n      },\n      \"errorLimit\": {\n        \"record\": 10,\n        \"percentage\": 0.1\n      }\n    }\n  }\n}\n</code></pre> <p>For plugin-specific configuration details, please refer to the respective plugin documentation in the reader and writer sections.</p>"},{"location":"statsreport/","title":"Statistics Report","text":"<p>Addax provides detailed statistics and monitoring during job execution to help you understand performance and troubleshoot issues.</p>"},{"location":"statsreport/#overview","title":"Overview","text":"<p>During execution, Addax reports various metrics including:</p> <ul> <li>Records processed per second</li> <li>Bytes transferred per second  </li> <li>Error counts and percentages</li> <li>Channel performance</li> <li>Memory usage</li> <li>Job progress</li> </ul>"},{"location":"statsreport/#statistics-output","title":"Statistics Output","text":""},{"location":"statsreport/#console-output","title":"Console Output","text":"<p>During job execution, Addax displays real-time statistics:</p> <pre><code>2023-12-07 10:30:15.123 [Statistics] INFO - Total records: 15000, Speed: 1500 rec/s (1.2 MB/s), Errors: 0 (0.00%)\n2023-12-07 10:30:25.124 [Statistics] INFO - Total records: 30000, Speed: 1500 rec/s (1.2 MB/s), Errors: 2 (0.01%)\n2023-12-07 10:30:35.125 [Statistics] INFO - Total records: 45000, Speed: 1500 rec/s (1.2 MB/s), Errors: 2 (0.00%)\n</code></pre>"},{"location":"statsreport/#final-report","title":"Final Report","text":"<p>At job completion, a comprehensive report is displayed:</p> <pre><code>====================Job Statistics====================\nJob ID: 202312071030001\nStart Time: 2023-12-07 10:30:00\nEnd Time: 2023-12-07 10:35:30\nTotal Time: 5m30s\n\nRecords:\n  - Total Read: 100000\n  - Total Written: 99998\n  - Errors: 2\n  - Error Rate: 0.002%\n\nPerformance:\n  - Average Speed: 303 rec/s\n  - Peak Speed: 450 rec/s\n  - Total Bytes: 12.5 MB\n  - Throughput: 38.5 KB/s\n\nChannels:\n  - Channel 0: 25000 records, 310 rec/s\n  - Channel 1: 25000 records, 305 rec/s  \n  - Channel 2: 24999 records, 298 rec/s\n  - Channel 3: 24999 records, 295 rec/s\n========================================================\n</code></pre>"},{"location":"statsreport/#enabling-detailed-statistics","title":"Enabling Detailed Statistics","text":""},{"location":"statsreport/#command-line-option","title":"Command Line Option","text":"<p>Enable detailed statistics reporting:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.detailed=true\" job.json\n</code></pre>"},{"location":"statsreport/#configuration-setting","title":"Configuration Setting","text":"<p>Add to job configuration:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"detailed\": true,\n        \"interval\": 10000\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"statsreport/#statistics-components","title":"Statistics Components","text":""},{"location":"statsreport/#performance-metrics","title":"Performance Metrics","text":"Metric Description Unit Records/sec Number of records processed per second rec/s Bytes/sec Amount of data transferred per second B/s Error Rate Percentage of failed records % Channel Utilization Performance of each parallel channel rec/s"},{"location":"statsreport/#memory-statistics","title":"Memory Statistics","text":"<pre><code>bin/addax.sh -j \"-Daddax.stats.memory=true\" job.json\n</code></pre> <p>Output includes: - JVM heap usage - Memory allocation rates - Garbage collection statistics</p>"},{"location":"statsreport/#io-statistics","title":"I/O Statistics","text":"<p>Track input/output performance:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.io=true\" job.json\n</code></pre> <p>Includes: - Network I/O rates - Disk read/write speeds - Connection pool statistics</p>"},{"location":"statsreport/#custom-statistics-interval","title":"Custom Statistics Interval","text":""},{"location":"statsreport/#set-update-frequency","title":"Set Update Frequency","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"interval\": 5000\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"statsreport/#disable-statistics","title":"Disable Statistics","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"enabled\": false\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"statsreport/#export-statistics","title":"Export Statistics","text":""},{"location":"statsreport/#json-format","title":"JSON Format","text":"<p>Export statistics to JSON file:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.export=/tmp/job_stats.json\" job.json\n</code></pre>"},{"location":"statsreport/#csv-format","title":"CSV Format","text":"<p>Export to CSV for analysis:</p> <pre><code>bin/addax.sh -j \"-Daddax.stats.export=/tmp/job_stats.csv -Daddax.stats.format=csv\" job.json\n</code></pre>"},{"location":"statsreport/#monitoring-integration","title":"Monitoring Integration","text":""},{"location":"statsreport/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Enable Prometheus endpoint:</p> <pre><code>bin/addax.sh -j \"-Daddax.metrics.prometheus.enabled=true -Daddax.metrics.prometheus.port=9090\" job.json\n</code></pre>"},{"location":"statsreport/#jmx-monitoring","title":"JMX Monitoring","text":"<p>Enable JMX for external monitoring tools:</p> <pre><code>bin/addax.sh -j \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999\" job.json\n</code></pre>"},{"location":"statsreport/#performance-analysis","title":"Performance Analysis","text":""},{"location":"statsreport/#identifying-bottlenecks","title":"Identifying Bottlenecks","text":"<p>Look for these patterns in statistics:</p> <p>Reader Bottleneck: <pre><code>Channel 0: 1000 rec/s (reader working hard)\nChannel 1: 100 rec/s (waiting for reader)\nChannel 2: 100 rec/s (waiting for reader)\n</code></pre></p> <p>Writer Bottleneck: <pre><code>Channel 0: 200 rec/s (waiting for writer)\nChannel 1: 200 rec/s (waiting for writer)  \nChannel 2: 200 rec/s (waiting for writer)\n</code></pre></p> <p>Network Bottleneck: <pre><code>High record rate but low byte rate indicates small records\nLow record rate but high byte rate indicates large records\n</code></pre></p>"},{"location":"statsreport/#optimization-recommendations","title":"Optimization Recommendations","text":"<p>Based on statistics, consider:</p> <ol> <li>Low Channel Utilization: Increase channel count</li> <li>High Error Rate: Check data quality and mappings</li> <li>Memory Issues: Adjust JVM heap size</li> <li>Uneven Channel Performance: Check data distribution</li> </ol>"},{"location":"statsreport/#alerting","title":"Alerting","text":""},{"location":"statsreport/#error-threshold-alerts","title":"Error Threshold Alerts","text":"<p>Set up alerts for error rates:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"errorLimit\": {\n        \"record\": 100,\n        \"percentage\": 0.1\n      },\n      \"statistics\": {\n        \"alerts\": {\n          \"errorRate\": 0.05\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"statsreport/#performance-alerts","title":"Performance Alerts","text":"<p>Alert on low performance:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"statistics\": {\n        \"alerts\": {\n          \"minSpeed\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"statsreport/#troubleshooting-with-statistics","title":"Troubleshooting with Statistics","text":""},{"location":"statsreport/#common-issues","title":"Common Issues","text":"<p>Job Running Slowly: - Check channel utilization - Monitor memory usage - Verify network connectivity</p> <p>High Error Rate: - Review data type mappings - Check source data quality - Verify target constraints</p> <p>Memory Errors: - Monitor heap usage trends - Check for memory leaks - Adjust JVM settings</p>"},{"location":"statsreport/#debug-commands","title":"Debug Commands","text":"<p>Get real-time statistics:</p> <pre><code># Monitor job progress\ntail -f $ADDAX_HOME/log/addax.log | grep Statistics\n\n# Check JVM status\njstat -gc &lt;pid&gt;\n\n# Monitor network connections\nnetstat -an | grep &lt;port&gt;\n</code></pre> <p>This comprehensive statistics system helps ensure your data synchronization jobs run efficiently and provides the visibility needed for troubleshooting and optimization.</p>"},{"location":"transformer/","title":"Data Transformation","text":""},{"location":"transformer/#transformer-definition","title":"Transformer Definition","text":"<p>During data synchronization and transmission, users may have customized requirements for data processing, such as trimming columns or transforming column values. This can be achieved through the T (Transformer) process in ETL. Addax includes a Transformer module that allows for flexible data transformation by defining a series of UDFs (User-Defined Functions).</p>"},{"location":"transformer/#execution-model","title":"Execution Model","text":"<pre><code>graph LR\nsource((\"source\"))\nsubgraph fr[\"Addax Framework\"]\n    direction LR\n    Reader ==&gt; Transformer ==&gt;Writer\nend\ntarget((\"target\"))\nsource ==&gt; fr ==&gt; target</code></pre>"},{"location":"transformer/#udf-functions","title":"UDF Functions","text":""},{"location":"transformer/#dx_substr","title":"dx_substr","text":"<p><code>dx_substr(idx, pos, length) -&gt; str</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>pos</code>: The starting position within the field's value.</li> <li><code>length</code>: The length of the target substring.</li> </ul> <p>Returns: A substring of the specified length from the specified starting position (inclusive). An exception is thrown if the starting position is invalid. If the field is null, it is returned directly (i.e., this transformer does not process it).</p>"},{"location":"transformer/#dx_pad","title":"dx_pad","text":"<p><code>dx_pad(idx, flag, length, chr)</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>flag</code>: \"l\" or \"r\", indicating whether to pad at the beginning (left) or the end (right).</li> <li><code>length</code>: The target length of the field.</li> <li><code>chr</code>: The character to use for padding.</li> </ul> <p>Returns: If the source string's length is less than the target length, it returns the string after padding. If it's longer, it is truncated (always from the right). If the field is null, it is converted to an empty string before padding.</p> <p>Examples:</p> <ul> <li><code>dx_pad(1, \"l\", \"4\", \"A\")</code>: If <code>column 1</code>'s value is <code>xyz</code>, the transformed value is <code>Axyz</code>. If the value is <code>xyzzzzz</code>, it becomes <code>xyzz</code>.</li> <li><code>dx_pad(1, \"r\", \"4\", \"A\")</code>: If <code>column 1</code>'s value is <code>xyz</code>, the transformed value is <code>xyzA</code>. If the value is <code>xyzzzzz</code>, it becomes <code>xyzz</code>.</li> </ul>"},{"location":"transformer/#dx_replace","title":"dx_replace","text":"<p><code>dx_replace(idx, pos, length, str) -&gt; str</code></p> <p>Parameters</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>pos</code>: The starting position within the field's value.</li> <li><code>length</code>: The length of the substring to be replaced.</li> <li><code>str</code>: The string to replace with.</li> </ul> <p>Returns: Replaces a substring of a specified length from a specified starting position (inclusive). An exception is thrown if the starting position is invalid. If the field is null, it is returned directly (i.e., this transformer does not process it).</p> <p>Examples:</p> <ul> <li><code>dx_replace(1, \"2\", \"4\", \"****\")</code>: If <code>column 1</code>'s value is <code>addaxTest</code>, it is transformed to <code>da****est</code>.</li> <li><code>dx_replace(1, \"5\", \"10\", \"****\")</code>: If <code>column 1</code>'s value is <code>addaxTest</code>, it is transformed to <code>data****</code>.</li> </ul>"},{"location":"transformer/#dx_filter","title":"dx_filter","text":"<p><code>dx_filter(idx, operator, expr) -&gt; str</code></p> <p>Parameters:</p> <ul> <li><code>idx</code>: The index of the field in the record.</li> <li><code>operator</code>: The operator. Supported operators are <code>like</code>, <code>not like</code>, <code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code>.</li> <li><code>expr</code>: A regular expression (Java-style) or a value.</li> </ul> <p>Returns:</p> <ul> <li>If the condition is met, it returns <code>null</code>, which filters out the entire row. If the condition is not met, the row is kept.</li> <li><code>like</code> and <code>not like</code>: The field is converted to a string and then fully matched against the target regular expression.</li> <li><code>&gt;</code>, <code>=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>!=</code>, <code>&lt;=</code>: Comparison is performed based on the data type. Numeric types are compared by value; string and boolean types are compared lexicographically.</li> <li>If the target field is <code>null</code>, it will satisfy the <code>= null</code> filter condition and be filtered out. For the <code>!= null</code> condition, <code>null</code> does not satisfy the filter condition and is not filtered. For <code>like</code>, if the field is <code>null</code>, it is not filtered.</li> </ul> <p>Examples:</p> <ul> <li><code>dx_filter(1, \"like\", \"dataTest\")</code></li> <li><code>dx_filter(1, \"&gt;=\", \"10\")</code></li> </ul> <p>Compound filters (i.e., conditions involving multiple fields) are not currently supported as the function parameters would be too complex for users.</p>"},{"location":"transformer/#dx_groovy","title":"dx_groovy","text":"<p><code>dx_groovy(code, package) -&gt; record</code></p> <p>Parameters</p> <ul> <li><code>code</code>: Code that conforms to Groovy syntax.</li> <li><code>package</code>: <code>extraPackage</code>, which can be a list or empty.</li> </ul> <p>Returns</p> <p>A <code>Record</code> data type.</p> <p>Notes:</p> <ul> <li><code>dx_groovy</code> can only be called once per transformer configuration. Multiple calls are not allowed.</li> <li>The <code>groovy code</code> supports packages from <code>java.lang</code> and <code>java.util</code>. Objects that can be directly referenced include <code>record</code> and various column types under <code>element</code> (BoolColumn.class, BytesColumn.class, DateColumn.class, DoubleColumn.class, LongColumn.class, StringColumn.class). Other packages are not supported by default. If you need to use other packages, you can set <code>extraPackage</code>. Note that <code>extraPackage</code> does not support third-party JARs.</li> <li>In the <code>groovy code</code>, you must return the updated <code>Record</code> (e.g., <code>record.setColumn(columnIndex, new StringColumn(newValue));</code>) or <code>null</code>. Returning <code>null</code> filters out the current row.</li> <li>You can directly call static utility methods (GroovyTransformerStaticUtil).</li> </ul> <p>Examples:</p> <p>Groovy implementation of <code>subStr</code>:</p> <pre><code>String code=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = oriValue.substring(0, 3);\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\ndx_groovy(code); // Note: The original doc had `dx_groovy(record)` which is incorrect. It should be the code string.\n</code></pre> <p>Groovy implementation of <code>replace</code>:</p> <pre><code>String code2=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String newValue = \\\"****\\\" + oriValue.substring(3, oriValue.length());\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>Groovy implementation of <code>pad</code>:</p> <pre><code>String code3=\"Column column = record.getColumn(1);\\n\"+\n        \" String oriValue = column.asString();\\n\"+\n        \" String padString = \\\"12345\\\";\\n\"+\n        \" String finalPad = \\\"\\\";\\n\"+\n        \" int NeedLength = 8 - oriValue.length();\\n\"+\n        \"        while (NeedLength &gt; 0) {\\n\"+\n        \"\\n\"+\n        \"            if (NeedLength &gt;= padString.length()) {\\n\"+\n        \"                finalPad += padString;\\n\"+\n        \"                NeedLength -= padString.length();\\n\"+\n        \"            } else {\\n\"+\n        \"                finalPad += padString.substring(0, NeedLength);\\n\"+\n        \"                NeedLength = 0;\\n\"+\n        \"            }\\n\"+\n        \"        }\\n\"+\n        \" String newValue= finalPad + oriValue;\\n\"+\n        \" record.setColumn(1, new StringColumn(newValue));\\n\"+\n        \" return record;\";\n</code></pre> <p>Starting from version <code>4.1.2</code>, <code>dx_groovy</code> supports loading Groovy code from an external file. The file is read relative to the <code>$ADDAX_HOME</code> directory, which is the installation directory of Addax.</p> <p>For example, to implement <code>subStr</code>, you can create a file <code>job/substr.groovy</code> with the following content:</p> <pre><code>Column column = record.getColumn(1)\nString oriValue = column.asString()\nString newValue = oriValue.substring(0, 3)\nrecord.setColumn(1, new StringColumn(newValue))\nreturn record\n</code></pre> <p>Then, define it in the <code>job</code> file like this:</p> <pre><code>{\n  \"transformer\": [\n    {\n      \"name\": \"dx_groovy\",\n      \"parameter\": {\n        \"codeFile\": \"job/substr.groovy\"\n      }\n    }\n  ]\n}\n</code></pre> <p>You can also specify an absolute path for the file.</p>"},{"location":"transformer/#job-definition","title":"Job Definition","text":"<p>In this example, four UDFs are configured.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"My name is xxxx\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"password is Passw0rd\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"random\": \"0,10\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      },\n      \"transformer\": [\n        {\n          \"name\": \"dx_replace\",\n          \"parameter\": {\n            \"columnIndex\": 0,\n            \"paras\": [\n              \"11\",\n              \"6\",\n              \"wgzhao\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_substr\",\n          \"parameter\": {\n            \"columnIndex\": 1,\n            \"paras\": [\n              \"0\",\n              \"12\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_map\",\n          \"parameter\": {\n            \"columnIndex\": 2,\n            \"paras\": [\n              \"^\",\n              \"2\"\n            ]\n          }\n        },\n        {\n          \"name\": \"dx_filter\",\n          \"parameter\": {\n            \"columnIndex\": 6,\n            \"paras\": [\n              \"&lt;\",\n              \"5\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"transformer/#custom-functions","title":"Custom Functions","text":"<p>If the built-in functions do not meet your data transformation requirements, you can write code that conforms to Groovy specifications within the <code>transformer</code>. Here is a complete example:</p> <pre><code>\n</code></pre> <p>The <code>transformer</code> code above modifies the first two fields of each record. It adds the prefix <code>Header_</code> to the first string field and doubles the value of the second integer field. The execution result is as follows:</p> <pre><code>$ bin/addax.sh job/transformer_demo.json \n\n  ___      _     _            \n / _ \\    | |   | |           \n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt; \n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-04 15:45:56.421 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-04 15:45:56.443 [        main] INFO  Engine               - \n\n.....\n\n2021-08-04 15:45:56.458 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-04 15:45:56.459 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-04 15:45:56.460 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-04 15:45:56.470 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-04 15:45:56.471 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-08-04 15:45:56.472 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-04 15:45:56.498 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-04 15:45:56.505 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-04 15:45:56.517 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-08-04 15:45:56.520 [ taskGroup-0] INFO  TransformerUtil      -  user config transformers [[dx_groovy]], loading...\n2021-08-04 15:45:56.531 [ taskGroup-0] INFO  TransformerUtil      -  1 of transformer init success. name=dx_groovy, isNative=true parameter = \n  {\"code\":\"record.setColumn(0, new StringColumn('Header_' + record.getColumn(0).asString()));record.setColumn(1, new LongColumn(record.getColumn(1).asLong() * 2));return record;\"}\n\nHeader_Addax    2       1989-06-04 00:00:01     true    test\nHeader_Addax    4       1989-06-03 00:00:01     true    test\nHeader_Addax    6       1989-06-02 00:00:01     true    test\nHeader_Addax    8       1989-06-01 00:00:01     true    test\nHeader_Addax    10      1989-05-31 00:00:01     true    test\nHeader_Addax    12      1989-05-30 00:00:01     true    test\nHeader_Addax    14      1989-05-29 00:00:01     true    test\nHeader_Addax    16      1989-05-28 00:00:01     true    test\nHeader_Addax    18      1989-05-27 00:00:01     true    test\nHeader_Addax    20      1989-05-26 00:00:01     true    test\n\n2021-08-04 15:45:59.515 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-04 15:45:59.517 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-04 15:45:59.518 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-08-04 15:45:59.521 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-04 15:45:59.524 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 330 bytes | Speed 110B/s, 3 records/s | Error 0 records, 0 bytes |  \n  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Transformer Success 10 records | Transformer Error 0 records | Transformer Filter 0 records \n  | Transformer usedTime 0.383s | Percentage 100.00%\n2021-08-04 15:45:59.527 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-04 15:45:56\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-04 15:45:59\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              110B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n\n2021-08-04 15:45:59.528 [       job-0] INFO  JobContainer         - \nTransformer\u6210\u529f\u8bb0\u5f55\u603b\u6570         :                  10\nTransformer\u5931\u8d25\u8bb0\u5f55\u603b\u6570         :                   0\nTransformer\u8fc7\u6ee4\u8bb0\u5f55\u603b\u6570         :                   0\n</code></pre>"},{"location":"transformer/#metrics-and-dirty-data","title":"Metrics and Dirty Data","text":"<p>The Transform process involves data conversion, which may increase or decrease the amount of data. Therefore, precise metrics are needed, including:</p> <ul> <li>Number of input records and bytes for the Transform.</li> <li>Number of output records and bytes from the Transform.</li> <li>Number of dirty data records and bytes from the Transform.</li> <li>If there are multiple Transforms, and one of them generates dirty data, subsequent transforms will not be executed for that record, and it will be directly counted as dirty data.</li> <li>Currently, only overall metrics for all Transforms are provided (success, failure, filtered counts, and time consumed by the transform).</li> </ul> <p>The metrics displayed during the process are defined as follows:</p> <pre><code>Total 1000000 records, 22000000 bytes | Transform 100000 records(in), 10000 records(out) | Speed 2.10MB/s, 100000 records/s | Error 0 records, 0 bytes | Percentage 100.00%\n</code></pre> <p>Note: This mainly records the input and output of the transformation, which requires monitoring changes in the number of data records.</p> <p>The final job metrics are displayed as follows:</p> <pre><code>Job start  at             : 2025-07-23 09:08:26\nJob end    at             : 2025-07-23 09:08:29\nJob took secs             :                  3s\nAverage   bps             :              110B/s\nAverage   rps             :              3rec/s\nNumber of rec             :                  10\nFailed record             :                   0\nTransformer success records:                  10\nTransformer failed  records:                   0\nTransformer filter  records:                   0\n</code></pre> <p>Note: This mainly records the input and output of the transformation, which requires monitoring changes in the number of data records.</p>"},{"location":"reader/accessreader/","title":"Access Reader","text":"<p>AccessReader implements the ability to read data from Access databases, based on Addax RDBMS Reader.</p>"},{"location":"reader/accessreader/#example","title":"Example","text":"<p>We download the test file AccessThemeDemo.zip, extract it to get the <code>AccessThemeDemo.mdb</code> file, which contains a <code>tbl_Users</code> table. We will synchronize the data from this table to the terminal.</p> <p>The following configuration reads the table to the terminal:</p> job/access2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"accessreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"tbl_Users\"\n            ],\n            \"jdbcUrl\": \"jdbc:ucanaccess:///Users/wgzhao/Downloads/AccessThemeDemo.mdb\"\n          },\n          \"where\": \"\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"encoding\": \"utf-8\",\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/access2stream.json</code></p>"},{"location":"reader/accessreader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/access2stream.json\n</code></pre>"},{"location":"reader/accessreader/#parameters","title":"Parameters","text":"<p>AccessReader is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/cassandrareader/","title":"Cassandra Reader","text":"<p><code>CassandraReader</code> plugin implements the ability to read data from Cassandra.</p>"},{"location":"reader/cassandrareader/#configuration","title":"Configuration","text":"<p>Below is an example configuration for reading data from Cassandra to terminal</p> job/cassandra2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"cassandrareader\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"test\",\n          \"table\": \"addax_src\",\n          \"column\": [\n            \"textCol\",\n            \"blobCol\",\n            \"writetime(blobCol)\",\n            \"boolCol\",\n            \"smallintCol\",\n            \"tinyintCol\",\n            \"intCol\",\n            \"bigintCol\",\n            \"varintCol\",\n            \"floatCol\",\n            \"doubleCol\",\n            \"decimalCol\",\n            \"dateCol\",\n            \"timeCol\",\n            \"timeStampCol\",\n            \"uuidCol\",\n            \"inetCol\",\n            \"durationCol\",\n            \"listCol\",\n            \"mapCol\",\n            \"setCol\",\n            \"tupleCol\",\n            \"udtCol\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/cassandrareader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description host Yes list None Connection domain or IP, multiple nodes separated by commas port Yes int 9042 Port username No string None Username password No string None Password useSSL No boolean false Whether to use SSL connection keyspace Yes string None Keyspace where the table to be synchronized is located table Yes string None Selected table to be synchronized column Yes list None Collection of columns to be synchronized in the configured table, elements can specify column name or <code>writetime(column_name)</code>, the latter form reads timestamp of <code>column_name</code> column instead of data where No string None Data filtering condition <code>cql</code> expression allowFiltering No boolean None Whether to filter data on server side, refer to official documentation for detailed description consistencyLevel No string LOCAL_QUORUM Data consistency level, options: <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code>"},{"location":"reader/clickhousereader/","title":"ClickHouse Reader","text":"<p><code>ClickHouseReader</code> plugin supports reading data from ClickHouse database.</p>"},{"location":"reader/clickhousereader/#example","title":"Example","text":""},{"location":"reader/clickhousereader/#table-structure-and-data-information","title":"Table Structure and Data Information","text":"<p>Assume the table structure and data to be read are as follows:</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre>"},{"location":"reader/clickhousereader/#configure-json-file","title":"Configure JSON File","text":"<p>The following configuration file reads specified table data from ClickHouse database and prints to terminal</p> job/clickhouse2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/clickhouse2stream.json</code></p>"},{"location":"reader/clickhousereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/clickhouse2stream.json\n</code></pre> <p>The output information is as follows (non-critical information removed):</p> <pre><code>021-01-06 14:39:35.742 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n\n2021-01-06 14:39:35.767 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        \"*\"\n                    ],\n                    \"connection\":[\n                        {\n                            \"jdbcUrl\":[\n                                \"jdbc:clickhouse://127.0.0.1:8123/\"\n                            ],\n                            \"table\":[\n                                \"ck_addax\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"default\"\n                },\n                \"name\":\"clickhousereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n127 -32768  2147483647  -9223372036854775808    255 65535   4294967295  18446744073709551615    1   1   1234567891234567891234567891.1234567891Hello String 2c:16:db:a3:3a:4f   \n5f042a36-5b0c-4f71-adfd-4df4fca1b863    2021-01-01  2021-01-01 00:00:00 2021-01-01 00:00:00 hello\n\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-06 14:39:35\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-06 14:39:39\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               77B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              0rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   1\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/clickhousereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"reader/clickhousereader/#supported-data-types","title":"Supported Data Types","text":"Addax Internal Type ClickHouse Data Type Long Uint8, Uint16, Uint32, Uint64, Int8, Int16, Int32, Int64, Enum8, Enum16 Double Float32, Float64, Decimal String String, FixedString(N) Date Date, DateTime, DateTime64 Boolean UInt8 Bytes String"},{"location":"reader/clickhousereader/#limitations","title":"Limitations","text":"<p>Except for the above listed field types, other types are not supported, such as Array, Nested, etc.</p>"},{"location":"reader/databendreader/","title":"Databend Reader","text":"<p>DatabendReader plugin implements reading data from Databend.</p> <p>Note that Databend has MySQL client protocol compatible implementation, so you can directly use MySQL Reader to read Databend data.</p>"},{"location":"reader/databendreader/#example","title":"Example","text":"<p>We can start Databend database in the following way</p> <pre><code>docker run  -tid  --rm  -p 8000:8000 \\\n   -e QUERY_DEFAULT_USER=databend \\\n   -e QUERY_DEFAULT_PASSWORD=databend \\\n   datafuselabs/databend\n</code></pre> <p>Then create a table to read</p> <pre><code>(\n    id int,\n    name varchar(255),\n    salary float,\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>And populate necessary data</p> <p>The following configuration reads this table to terminal:</p> job/databend2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"databendreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://127.0.0.1:8000/default\",\n            \"table\": [\n              \"addax_reader\"\n            ]\n          },\n          \"username\": \"databend\",\n          \"password\": \"databend\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/databend2stream.json</code></p>"},{"location":"reader/databendreader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/databend2stream.json\n</code></pre>"},{"location":"reader/databendreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"reader/databendreader/#limitations","title":"Limitations","text":"<p>None at the moment</p>"},{"location":"reader/datareader/","title":"Data Reader","text":"<p><code>DataReader</code> plugin is specifically designed to generate data that meets certain rule requirements in development and testing environments.</p> <p>In actual development and testing, we need to generate test data according to certain business rules, not just random content, such as ID card numbers, bank account numbers, stock codes, etc.</p>"},{"location":"reader/datareader/#why-reinvent-the-wheel","title":"Why Reinvent the Wheel","text":"<p>Indeed, there are quite a few specialized data generation tools on the Internet, many of which are powerful and performant. However, most of these tools consider the data generation part but ignore the problem of writing data to the target end, or some consider it but only consider one or a limited number of databases.</p> <p>It happens that the Addax tool can provide enough target-end writing capabilities, plus the existing Stream Reader is already a simple version of data generation tool. Therefore, adding some specific rules to this functionality and leveraging the diversity of the writing end naturally makes it a good data generation tool.</p>"},{"location":"reader/datareader/#configuration-example","title":"Configuration Example","text":"<p>Here I list all the rules currently supported by the plugin in the example below</p> datareader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"datareader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"1,100,\",\n              \"rule\": \"random\",\n              \"type\": \"double\"\n            },\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"1\",\n              \"rule\": \"incr\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989/06/04 00:00:01,-1\",\n              \"rule\": \"incr\",\n              \"type\": \"date\",\n              \"dateFormat\": \"yyyy/MM/dd hh:mm:ss\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"rule\": \"address\"\n            },\n            {\n              \"rule\": \"bank\"\n            },\n            {\n              \"rule\": \"company\"\n            },\n            {\n              \"rule\": \"creditCard\"\n            },\n            {\n              \"rule\": \"debitCard\"\n            },\n            {\n              \"rule\": \"idCard\"\n            },\n            {\n              \"rule\": \"lat\"\n            },\n            {\n              \"rule\": \"lng\"\n            },\n            {\n              \"rule\": \"name\"\n            },\n            {\n              \"rule\": \"job\"\n            },\n            {\n              \"rule\": \"phone\"\n            },\n            {\n              \"rule\": \"stockCode\"\n            },\n            {\n              \"rule\": \"stockAccount\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content to <code>job/datareader2stream.json</code></p> <p>Then execute this task, the output result is similar to the following:</p> <pre><code>$ bin/addax.sh job/datareader2stream.json\n\n  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.2-SNAPSHOT)\n\n2021-08-13 17:02:00.888 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-08-13 17:02:00.910 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"rule\":\"random\",\n                            \"type\":\"double\",\n                            \"scale\": \"2\",\n                            \"value\":\"1,100,\"\n                        },\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"rule\":\"incr\",\n                            \"type\":\"long\",\n                            \"value\":\"1\"\n                        },\n                        {\n                            \"dateFormat\":\"yyyy/MM/dd hh:mm:ss\",\n                            \"rule\":\"incr\",\n                            \"type\":\"date\",\n                            \"value\":\"1989/06/04 00:00:01,-1\"\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        },\n                        {\n                            \"rule\":\"address\"\n                        },\n                        {\n                            \"rule\":\"bank\"\n                        },\n                        {\n                            \"rule\":\"company\"\n                        },\n                        {\n                            \"rule\":\"creditCard\"\n                        },\n                        {\n                            \"rule\":\"debitCard\"\n                        },\n                        {\n                            \"rule\":\"idCard\"\n                        },\n                        {\n                            \"rule\":\"lat\"\n                        },\n                        {\n                            \"rule\":\"lng\"\n                        },\n                        {\n                            \"rule\":\"name\"\n                        },\n                        {\n                            \"rule\":\"job\"\n                        },\n                        {\n                            \"rule\":\"phone\"\n                        },\n                        {\n                            \"rule\":\"stockCode\"\n                        },\n                        {\n                            \"rule\":\"stockAccount\"\n                        }\n                    ],\n                    \"sliceRecordCount\":10\n                },\n                \"name\":\"datareader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-08-13 17:02:00.937 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-08-13 17:02:00.938 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-08-13 17:02:00.940 [        main] INFO  JobContainer         - Set jobId = 0\n2021-08-13 17:02:00.976 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do prepare work .\n2021-08-13 17:02:00.977 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-08-13 17:02:00.978 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-08-13 17:02:00.979 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] splits to [1] tasks.\n2021-08-13 17:02:00.980 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [1] tasks.\n2021-08-13 17:02:01.002 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-08-13 17:02:01.009 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-08-13 17:02:01.017 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n7.65    DataX   1   1989-06-04 00:00:01 test    \u5929\u6d25\u5e02\u5357\u4eac\u53bf\u957f\u5bff\u533a\u5149\u660e\u8def263\u53f7    \u4ea4\u901a\u94f6\u884c    \u6613\u52a8\u529b\u4fe1\u606f\u6709\u9650\u516c\u53f8   6227894836568607    6235712610856305437 450304194808316766  31.3732613  -125.3507716    \u9f9a\u519b  \u673a\u7535\u5de5\u7a0b\u5e08   13438631667 726929  8741848665\n18.58   DataX   2   1989-06-03 00:00:01 test    \u6c5f\u82cf\u7701\u592a\u539f\u5e02\u6d54\u9633\u533a\u4e1c\u5c71\u8def33\u53f7 \u4e2d\u56fd\u94f6\u884c    \u65f6\u7a7a\u76d2\u6570\u5b57\u4fe1\u606f\u6709\u9650\u516c\u53f8 4096666711928233    6217419359154239015 220301200008188547  48.6648764  104.8567048 \u5321\u98de  \u5316\u5986\u5e08 18093137306 006845  1815787371\n16.16   DataX   3   1989-06-02 00:00:01 test    \u53f0\u6e7e\u7701\u90af\u90f8\u5e02\u6e05\u6cb3\u533a\u4e07\u987a\u8def10\u53f7 \u5927\u540c\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u79d1\u6280\u6709\u9650\u516c\u53f8 4096713966912225    6212977716107080594 150223196408276322  29.0134395  142.6426842 \u652f\u6ce2  \u5ba1\u6838\u5458 13013458079 020695  3545552026\n63.89   DataX   4   1989-06-01 00:00:01 test    \u4e0a\u6d77\u5e02\u8f9b\u96c6\u53bf\u516d\u679d\u7279\u533a\u7518\u56ed\u8def119\u53f7   \u4e2d\u56fd\u519c\u4e1a\u94f6\u884c  \u6cf0\u9e92\u9e9f\u4f20\u5a92\u6709\u9650\u516c\u53f8   6227893481508780    6215686558778997167 220822196208286838  -71.6484635 111.8181273 \u656c\u5764  \u623f\u5730\u4ea7\u5ba2\u670d   13384928291 174445  0799668655\n79.18   DataX   5   1989-05-31 00:00:01 test    \u9655\u897f\u7701\u5357\u4eac\u5e02\u671d\u9633\u533a\u5927\u80dc\u8def170\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u6656\u6765\u8ba1\u7b97\u673a\u4fe1\u606f\u6709\u9650\u516c\u53f8 6227535683896707    6217255315590053833 350600198508222018  -24.9783587 78.017024   \u848b\u6768  \u56fa\u5b9a\u8d44\u4ea7\u4f1a\u8ba1  18766298716 402188  9633759917\n14.97   DataX   6   1989-05-30 00:00:01 test    \u6d77\u5357\u7701\u957f\u6625\u53bf\u74a7\u5c71\u533a\u78a7\u6d77\u8857147\u53f7    \u534e\u590f\u94f6\u884c    \u6d59\u5927\u4e07\u670b\u79d1\u6280\u6709\u9650\u516c\u53f8  6224797475369912    6215680436662199846 220122199608190275  -3.5088667  -40.2634359 \u8fb9\u6768  \u7763\u5bfc/\u5de1\u5e97   13278765923 092780  2408887582\n45.49   DataX   7   1989-05-29 00:00:01 test    \u53f0\u6e7e\u7701\u6f5c\u6c5f\u53bf\u6881\u5e73\u533a\u4e03\u661f\u8857201\u53f7    \u664b\u57ce\u5546\u884c    \u5f00\u53d1\u533a\u4e16\u521b\u4fe1\u606f\u6709\u9650\u516c\u53f8 5257468530819766    6213336008535546044 141082197908244004  -72.9200596 120.6018163 \u6851\u660e  \u7cfb\u7edf\u5de5\u7a0b\u5e08   13853379719 175864  8303448618\n8.45    DataX   8   1989-05-28 00:00:01 test    \u6d77\u5357\u7701\u676d\u5dde\u53bf\u57ce\u5317\u533a\u5929\u5174\u8def11\u53f7 \u5927\u540c\u5546\u884c    \u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8  6227639043120062    6270259717880740332 430405198908214042  -16.5115338 -39.336119  \u8983\u5065  \u4eba\u4e8b\u603b\u76d1    13950216061 687461  0216734574\n15.01   DataX   9   1989-05-27 00:00:01 test    \u4e91\u5357\u7701\u60e0\u5dde\u5e02\u548c\u5e73\u533a\u6d77\u9e25\u8857201\u53f7    \u5185\u8499\u53e4\u94f6\u884c   \u9ec4\u77f3\u91d1\u627f\u4fe1\u606f\u6709\u9650\u516c\u53f8  6200358843233005    6235730928871528500 130300195008312067  -61.646097  163.0882369 \u536b\u5efa\u534e \u7535\u8bdd\u91c7\u7f16    15292600492 001658  1045093445\n55.14   DataX   10  1989-05-26 00:00:01 test    \u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7    \u5eca\u574a\u94f6\u884c    \u521b\u6c47\u79d1\u6280\u6709\u9650\u516c\u53f8    6227605280751588    6270262330691012025 341822200908168063  77.2165746  139.5431377 \u6c60\u6d69  \u591a\u5a92\u4f53\u8bbe\u8ba1   18693948216 201678  0692522928\n\n2021-08-13 17:02:04.020 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-08-13 17:02:04.021 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-08-13 17:02:04.022 [       job-0] INFO  JobContainer         - Addax Reader.Job [datareader] do post work.\n2021-08-13 17:02:04.025 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-08-13 17:02:04.028 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 10 records, 1817 bytes | Speed 605B/s, 3 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-08-13 17:02:04.030 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-08-13 17:02:00\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-08-13 17:02:04\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              605B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              3rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  10\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/datareader/#configuration-description","title":"Configuration Description","text":"<p>The configuration of <code>column</code> is slightly different from other plugins. A field consists of the following configuration items:</p> Configuration Required Default Value Example Description value No None <code>Addax</code> Data value, required in some cases rule No <code>constant</code> <code>idCard</code> Data generation rule, detailed description below type No <code>string</code> <code>double</code> Data value type dateFormat No <code>yyyy-MM-dd HH:mm:ss</code> <code>yyyy/MM/dd HH:mm:ss</code> Date format, only valid when <code>type</code> is <code>date</code>"},{"location":"reader/datareader/#rule-description","title":"rule Description","text":"<p>The core of this plugin's field configuration is the <code>rule</code> field, which is used to indicate what kind of data should be generated, and according to different rules, combined with other configuration options to generate data that meets expectations. Currently, the configurations of <code>rule</code> are all built-in supported rules, custom rules are not supported yet. Detailed description follows:</p>"},{"location":"reader/datareader/#constant","title":"constant","text":"<p><code>constant</code> is the default configuration of <code>rule</code>. This rule means that the data value to be generated is determined by the <code>value</code> configuration item, with no changes. For example:</p> <pre><code>{\n  \"value\": \"Addax\",\n  \"type\": \"string\",\n  \"rule\": \"constant\"\n}\n</code></pre> <p>This means the data value generated by this field is all <code>Addax</code></p>"},{"location":"reader/datareader/#incr","title":"incr","text":"<p>The meaning of <code>incr</code> configuration item is consistent with the <code>incr</code> meaning in the <code>streamreader</code> plugin, indicating this is an incremental data generation rule. For example:</p> <pre><code>{\n  \"value\": \"1,2\",\n  \"rule\": \"incr\",\n  \"type\": \"long\"\n}\n</code></pre> <p>This means the field data is a long integer, starting from 1, incrementing by 2 each time, forming an incremental sequence starting from 1 with step size 2.</p> <p>For more detailed configuration rules and precautions for this field, refer to the <code>incr</code> description in streamreader.</p>"},{"location":"reader/datareader/#random","title":"random","text":"<p>The meaning of <code>random</code> configuration item is consistent with the <code>random</code> meaning in the <code>streamreader</code> plugin, indicating this is an incremental data generation rule. For example:</p> <pre><code>{\n  \"value\": \"1,10\",\n  \"rule\": \"random\",\n  \"type\": \"string\"\n}\n</code></pre> <p>This means the field data is a random string with length 1 to 10 (both 1 and 10 included).</p> <p>For more detailed configuration rules and precautions for this field, refer to the <code>random</code> description in streamreader.</p> Rule Name Meaning Example Data Type Description <code>address</code> Randomly generate address information that basically meets domestic actual conditions <code>\u8fbd\u5b81\u7701\u5170\u5dde\u5e02\u5f90\u6c47\u533a\u4e1c\u5c71\u8857176\u53f7</code> string <code>bank</code> Randomly generate a domestic bank name <code>\u534e\u590f\u94f6\u884c</code> string <code>company</code> Randomly generate a company name <code>\u4e07\u8fc5\u7535\u8111\u79d1\u6280\u6709\u9650\u516c\u53f8</code> string <code>creditCard</code> Randomly generate a credit card number <code>430405198908214042</code> string 16 digits <code>debitCard</code> Randomly generate a debit card number <code>6227894836568607</code> string 19 digits <code>email</code> Randomly generate an email address <code>ok2a@gmail.com</code> string <code>idCard</code> Randomly generate a domestic ID card number <code>350600198508222018</code> string 18 digits, follows validation rules, first 6 digits meet administrative division requirements <code>lat</code> Randomly generate latitude data <code>48.6648764</code> double Fixed 7 decimal places, can also use <code>latitude</code> <code>lng</code> Randomly generate longitude data <code>120.6018163</code> double Fixed 7 decimal places, can also use <code>longitude</code> <code>name</code> Randomly generate a domestic name <code>\u6c60\u6d69</code> string Currently doesn't consider surname distribution in China <code>job</code> Randomly generate a domestic job title <code>\u7cfb\u7edf\u5de5\u7a0b\u5e08</code> string Data source from recruitment websites <code>phone</code> Randomly generate a domestic mobile phone number <code>15292600492</code> string Currently doesn't consider virtual phone numbers <code>stockCode</code> Randomly generate a 6-digit stock code <code>687461</code> string First two digits meet domestic stock code standards <code>stockAccount</code> Randomly generate a 10-digit stock trading account <code>0692522928</code> string Completely random, doesn't meet account standards <code>uuid</code> Randomly generate a UUID string <code>bc1cf125-929b-43b7-b324-d7c4cc5a75d2</code> string Completely random <code>zipCode</code> Randomly generate a domestic postal code <code>411105</code> long Doesn't fully meet domestic postal code standards <p>Note: The data types returned by the rules in the above table are fixed and cannot be modified, so <code>type</code> doesn't need to be configured, and the configured type will be ignored. Since data generation comes from internal rules, <code>value</code> also doesn't need to be configured, and the configured content will be ignored.</p>"},{"location":"reader/dbfreader/","title":"Dbf Reader","text":"<p><code>DbfReader</code> plugin supports reading DBF format files.</p>"},{"location":"reader/dbfreader/#configuration","title":"Configuration","text":"<p>The following is a configuration example for reading DBF files and printing to terminal</p> jobs/dbf2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"dbfreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"dbf\",\n              \"type\": \"string\"\n            }\n          ],\n          \"path\": [\n            \"/tmp/out\"\n          ],\n          \"encoding\": \"GBK\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/dbfreader/#parameters","title":"Parameters","text":"<p><code>parameter</code> configuration supports the following configurations:</p> Configuration Required Default Value Description path Yes None DBF file path, supports writing multiple paths, detailed description below column Yes None Collection of columns to be synchronized in the configured table, is a collection of <code>{type: value}</code> or <code>{type: index}</code>, detailed configuration below encoding No GBK DBF file encoding, such as <code>GBK</code>, <code>UTF-8</code> nullFormat No <code>\\N</code> Define which string can represent null"},{"location":"reader/dbfreader/#path","title":"path","text":"<p>Description: Path information of local file system, note that multiple paths can be supported here.</p> <ul> <li>When specifying a single local file, DbfFileReader can currently only use single-threaded data extraction. Phase 2 considers multi-threaded concurrent reading for single files in uncompressed file situations.</li> <li>When specifying multiple local files, DbfFileReader supports using multi-threaded data extraction. Thread concurrency is specified by the number of channels.</li> <li>When specifying wildcards, DbfFileReader attempts to traverse multiple file information. For example: specifying <code>/*</code> represents reading all files under the / directory, specifying <code>/foo/*</code> represents reading all files under the <code>foo</code> directory. dbfFileReader currently only supports <code>*</code> as a file wildcard.</li> </ul> <p>It is particularly important to note that Addax treats all dbf files synchronized under one job as the same data table. Users must ensure that all files can adapt to the same set of schema information. Users must ensure that the read files are in dbf-like format and provide Addax with read permissions.</p> <p>It is particularly important to note that if there are no matching files for extraction under the path specified by Path, Addax will report an error.</p>"},{"location":"reader/dbfreader/#column","title":"column","text":"<p>List of fields to read, <code>type</code> specifies the type of source data, <code>name</code> is the field name with maximum length of 8, <code>value</code> specifies that the current type is constant, not reading data from source file, but automatically generating corresponding columns based on <code>value</code> value.</p> <p>By default, users can read all data as <code>String</code> type, configured as follows:</p> <pre><code>{\n  \"column\": [\"*\"]\n}\n</code></pre> <p>Users can specify Column field information, configured as follows:</p> <pre><code>[\n  {\n    \"type\": \"long\",\n    \"index\": 0\n  },\n  {\n    \"type\": \"string\",\n    \"value\": \"addax\"\n  }\n]\n</code></pre> <ul> <li><code>\"index\": 0</code> means getting int field from the first column of local DBF file</li> <li><code>\"value\": \"addax\"</code> means generating string field <code>addax</code> internally from dbfFileReader as the current field. For user-specified <code>column</code> information, <code>type</code> must be filled, and <code>index</code> and <code>value</code> must choose one.</li> </ul>"},{"location":"reader/dbfreader/#supported-data-types","title":"Supported Data Types","text":"<p>Local files provide data types themselves, this type is defined by Addax dbfFileReader:</p> Addax Internal Type Local File Data Type Long Long Double Double String String Boolean Boolean Date Date <p>Where:</p> <ul> <li>Long refers to string representation of integer form in local file text, such as <code>19901219</code>.</li> <li>Double refers to string representation of Double form in local file text, such as <code>3.1415</code>.</li> <li>Boolean refers to string representation of Boolean form in local file text, such as <code>true</code>, <code>false</code>. Case insensitive.</li> <li>Date refers to string representation of Date form in local file text, such as <code>2014-12-31</code>, can configure <code>dateFormat</code> to specify format.</li> </ul>"},{"location":"reader/elasticsearchreader/","title":"ElasticSearchReader","text":"<p>ElasticSearchReader plugin implements the functionality of reading indexes from Elasticsearch. It uses the Rest API provided by Elasticsearch (default port 9200) to execute specified query statements and batch retrieve data.</p>"},{"location":"reader/elasticsearchreader/#example","title":"Example","text":"<p>Assume the index content to be retrieved is as follows</p> <pre><code>{\n  \"took\": 14,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 1,\n    \"hits\": [\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"38\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      },\n      {\n        \"_index\": \"test-1\",\n        \"_type\": \"default\",\n        \"_id\": \"103\",\n        \"_score\": 1,\n        \"_source\": {\n          \"col_date\": \"2017-05-25T11:22:33.000+08:00\",\n          \"col_integer\": 19890604,\n          \"col_keyword\": \"hello world\",\n          \"col_ip\": \"1.1.1.1\",\n          \"col_text\": \"long text\",\n          \"col_double\": 19890604,\n          \"col_long\": 19890604,\n          \"col_geo_point\": \"41.12,-71.34\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>Configure a task to read data from Elasticsearch and print to terminal</p> job/es2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"elasticsearchreader\",\n        \"parameter\": {\n          \"endpoint\": \"http://127.0.0.1:9200\",\n          \"accessId\": \"\",\n          \"accesskey\": \"\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"searchType\": \"dfs_query_then_fetch\",\n          \"headers\": {},\n          \"scroll\": \"3m\",\n          \"search\": [\n            {\n              \"query\": {\n                \"match\": {\n                  \"col_ip\": \"1.1.1.1\"\n                }\n              },\n              \"aggregations\": {\n                \"top_10_states\": {\n                  \"terms\": {\n                    \"field\": \"col_date\",\n                    \"size\": 10\n                  }\n                }\n              }\n            }\n          ],\n          \"column\": [\n            \"col_ip\",\n            \"col_double\",\n            \"col_long\",\n            \"col_integer\",\n            \"col_keyword\",\n            \"col_text\",\n            \"col_geo_point\",\n            \"col_date\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/es2stream.json</code></p> <p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/es2stream.json\n</code></pre> <p>The output result is similar to the following (output records are reduced):</p> <pre><code>2021-02-19 13:38:15.860 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-02-19 13:38:15.895 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"accessId\":\"\",\n                    \"headers\":{},\n                    \"endpoint\":\"http://127.0.0.1:9200\",\n                    \"search\":[\n                      {\n                        \"query\": {\n                          \"match\": {\n                            \"col_ip\": \"1.1.1.1\"\n                          }\n                        },\n                        \"aggregations\": {\n                          \"top_10_states\": {\n                            \"terms\": {\n                              \"field\": \"col_date\",\n                              \"size\": 10\n                            }\n                          }\n                        }\n                      }\n                    ],\n                    \"accesskey\":\"*****\",\n                    \"searchType\":\"dfs_query_then_fetch\",\n                    \"scroll\":\"3m\",\n                    \"column\":[\n                        \"col_ip\",\n                        \"col_double\",\n                        \"col_long\",\n                        \"col_integer\",\n                        \"col_keyword\",\n                        \"col_text\",\n                        \"col_geo_point\",\n                        \"col_date\"\n                    ],\n                    \"index\":\"test-1\",\n                    \"type\":\"default\"\n                },\n                \"name\":\"elasticsearchreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true,\n                    \"encoding\":\"UTF-8\"\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-02-19 13:38:15.934 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-19 13:38:15.934 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-19 13:38:15.937 [main] INFO  JobContainer - Set jobId = 0\n\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n2017-05-25T11:22:33.000+08:00   19890604    hello world 1.1.1.1 long text   19890604    19890604    41.12,-71.34\n\n2021-02-19 13:38:19.845 [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.\n2021-02-19 13:38:19.848 [job-0] INFO  JobContainer - Addax Writer.Job [streamwriter] do post work.\n2021-02-19 13:38:19.849 [job-0] INFO  JobContainer - Addax Reader.Job [elasticsearchreader] do post work.\n2021-02-19 13:38:19.855 [job-0] INFO  JobContainer - PerfTrace not enable!\n2021-02-19 13:38:19.858 [job-0] INFO  StandAloneJobContainerCommunicator - Total 95 records, 8740 bytes | Speed 2.84KB/s, 31 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.103s | Percentage 100.00%\n2021-02-19 13:38:19.861 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-19 13:38:15\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-19 13:38:19\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.84KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             31rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   2\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/elasticsearchreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description endpoint Yes string None ElasticSearch connection address accessId No string <code>\"\"</code> User in http auth accessKey No string <code>\"\"</code> Password in http auth index Yes string None Index name in elasticsearch type No string index name Type name of index in elasticsearch search Yes list <code>[]</code> JSON format API search data body column Yes list None Fields to be read timeout No int 60 Client timeout (unit: seconds) discovery No boolean false Enable node discovery (polling) and periodically update server list in client compression No boolean true HTTP request, enable compression multiThread No boolean true HTTP request, whether multi-threaded searchType No string <code>dfs_query_then_fetch</code> Search type headers No map <code>{}</code> HTTP request headers scroll No string <code>\"\"</code> Scroll pagination configuration"},{"location":"reader/elasticsearchreader/#search","title":"search","text":"<p>The search configuration item allows configuration of content that meets Elasticsearch API query requirements, like this:</p> <pre><code>{\n  \"query\": {\n    \"match\": {\n      \"message\": \"myProduct\"\n    }\n  },\n  \"aggregations\": {\n    \"top_10_states\": {\n      \"terms\": {\n        \"field\": \"state\",\n        \"size\": 10\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/elasticsearchreader/#searchtype","title":"searchType","text":"<p>searchType currently supports the following types:</p> <ul> <li>dfs_query_then_fetch</li> <li>query_then_fetch</li> <li>count</li> <li>scan</li> </ul>"},{"location":"reader/excelreader/","title":"Excel Reader","text":"<p><code>Excel Reader</code> plugin implements the ability to read data from Microsoft Excel files.</p>"},{"location":"reader/excelreader/#configuration","title":"Configuration","text":""},{"location":"reader/excelreader/#get-sample-files","title":"Get Sample Files","text":"<p>Download the Excel compressed file for demonstration from here and extract it to the <code>/tmp/in</code> directory. The three folders have the same content, where:</p> <ul> <li><code>demo.xlsx</code> is the new Excel format</li> <li><code>demo.xls</code> is the old Excel format</li> <li><code>demo_gbk.xlsx</code> is created under Windows and stored with GBK encoding</li> </ul> <p>File content is shown in the following table:</p> No. Integer Type Float Type String Type Date Type Formula Calculation Cell Formatting 1 11 1102.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/10 5544.17 \u00a51,102.23 2 12 1103.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/11 5552.17 \u00a51,103.23 3 13 1104.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/12 5560.17 \u00a51,104.23 4 14 1105.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/13 5568.17 \u00a51,105.23 5 15 1106.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/14 5576.17 \u00a51,106.23 6 16 1107.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/15 5584.17 \u00a51,107.23 7 17 1108.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/16 5592.17 \u00a51,108.23 8 18 1109.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/17 5600.17 \u00a51,109.23 9 19 1110.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/18 5608.17 \u00a51,110.23 10 20 1111.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/19 5616.17 \u00a51,111.23 11 21 1112.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/20 5624.17 \u00a51,112.23 12 22 1113.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/21 5632.17 \u00a51,113.23 13 23 1114.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/22 5640.17 \u00a51,114.23 14 24 1115.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/23 5648.17 \u00a51,115.23 15 25 1116.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/24 5656.17 \u00a51,116.23 16 26 1117.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/25 5664.17 \u00a51,117.23 17 27 1118.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/26 5672.17 \u00a51,118.23 18 28 1119.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/27 5680.17 \u00a51,119.23 19 29 1120.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/28 5688.17 \u00a51,120.23 20 30 1121.234 Addax \u52a0\u4e0a\u4e2d\u6587 2021/9/29 5696.17 \u00a51,121.23 <p>The table headers roughly describe the characteristics of cell data.</p>"},{"location":"reader/excelreader/#create-job-file","title":"Create Job File","text":"<p>Create the following JSON file:</p> excel2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"excelreader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/in\"\n          ],\n          \"header\": true,\n          \"skipRows\": 0\n        }\n      },\n      \"writer\": {\n        \"parameter\": {\n          \"print\": true\n        },\n        \"name\": \"streamwriter\"\n      }\n    }\n  }\n}\n</code></pre> <p>Save the output content to the <code>job/excel2stream.json</code> file and execute the collection command:</p> <pre><code>$ bin/addax.sh job/excel2stream.json\n</code></pre> <p>If there are no errors, you should get the following output:</p> Click to expand <pre><code> ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3)\n\n2021-09-09 14:43:42.579 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-09 14:43:42.621 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"path\":[\n                        \"/tmp/in\"\n                    ],\n                    \"column\":[\n                        {\n                            \"name\":\"no\",\n                            \"type\":\"long\"\n                        },\n                        {\n                            \"name\":\"birth\",\n                            \"format\":\"yyyy-MM-dd HH:mm:ss\",\n                            \"type\":\"date\"\n                        },\n                        {\n                            \"name\":\"kk\",\n                            \"type\":\"string\"\n                        }\n                    ],\n                    \"header\":true,\n                    \"skipHeader\":true,\n                    \"encoding\":\"UTF-8\",\n                    \"fieldDelimiter\":\",\"\n                },\n                \"name\":\"excelreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":2\n        }\n    }\n}\n\n2021-09-09 14:43:42.653 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-09 14:43:42.653 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-09 14:43:42.655 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_old.xls] as a candidate to be read.\n2021-09-09 14:43:42.669 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo_gbk.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - add file [/tmp/in/demo.xlsx] as a candidate to be read.\n2021-09-09 14:43:42.670 [       job-0] INFO  ExcelReader$Job      - The number of files to read is: [3]\n2021-09-09 14:43:42.677 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do prepare work .\n2021-09-09 14:43:42.678 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do prepare work .\n2021-09-09 14:43:42.679 [       job-0] INFO  JobContainer         - Job set Channel-Number to 2 channels.\n2021-09-09 14:43:42.681 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] splits to [3] tasks.\n2021-09-09 14:43:42.682 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] splits to [3] tasks.\n2021-09-09 14:43:42.727 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-09 14:43:42.736 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [2] channels for [3] tasks.\n2021-09-09 14:43:42.741 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-09 14:43:42.742 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.755 [0-0-1-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo.xlsx\n2021-09-09 14:43:42.757 [0-0-0-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:42.758 [0-0-0-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_gbk.xlsx\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n1   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n1   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n1   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n1   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n1   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n1   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n1   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n1   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n1   20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n1   21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n1   22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n1   23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n1   24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n1   25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n1   26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n1   27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n1   28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n1   29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n1   30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - The first row is skipped as a table header\n2021-09-09 14:43:43.894 [0-0-2-reader] INFO  ExcelReader$Task     - begin read file /tmp/in/demo_old.xls\n1   11  1102.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-10 00:00:00 5544.17 1102.234\n2   12  1103.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-11 00:00:00 5552.17 1103.234\n3   13  1104.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-12 00:00:00 5560.17 1104.234\n4   14  1105.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-13 00:00:00 5568.17 1105.234\n5   15  1106.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-14 00:00:00 5576.17 1106.234\n6   16  1107.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-15 00:00:00 5584.17 1107.234\n7   17  1108.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-16 00:00:00 5592.17 1108.234\n8   18  1109.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-17 00:00:00 5600.17 1109.234\n9   19  1110.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-18 00:00:00 5608.17 1110.234\n10  20  1111.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-19 00:00:00 5616.17 1111.234\n11  21  1112.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-20 00:00:00 5624.17 1112.234\n12  22  1113.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-21 00:00:00 5632.17 1113.234\n13  23  1114.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-22 00:00:00 5640.17 1114.234\n14  24  1115.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-23 00:00:00 5648.17 1115.234\n15  25  1116.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-24 00:00:00 5656.17 1116.234\n16  26  1117.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-25 00:00:00 5664.17 1117.234\n17  27  1118.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-26 00:00:00 5672.17 1118.234\n18  28  1119.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-27 00:00:00 5680.17 1119.234\n19  29  1120.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-28 00:00:00 5688.17 1120.234\n20  30  1121.234    Addax\u52a0\u4e0a\u4e2d\u6587   2021-09-29 00:00:00 5696.17 1121.234\n2021-09-09 14:43:45.753 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-09 14:43:45.754 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-09-09 14:43:45.756 [       job-0] INFO  JobContainer         - Addax Reader.Job [excelreader] do post work.\n2021-09-09 14:43:45.761 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-09 14:43:45.762 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 60 records, 3360 bytes | Speed 1.09KB/s, 20 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.993s | Percentage 100.00%\n2021-09-09 14:43:45.764 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-09 14:43:42\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-09 14:43:45\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            1.09KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             20rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                  60\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/excelreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description path Yes string/list None Specify the folder to read, multiple can be specified header No boolean false Whether the file contains headers skipRows No int 0 How many rows to skip at the beginning"},{"location":"reader/excelreader/#header","title":"header","text":"<p>Whether the Excel file contains headers, if so, skip them.</p>"},{"location":"reader/excelreader/#skiprows","title":"skipRows","text":"<p>Specify the number of rows to skip, default is 0, meaning no skipping. Note that if <code>header</code> is set to true and <code>skipRows</code> is set to 2, it means the first three rows are all skipped. If <code>header</code> is false, it means skipping the first two rows.</p>"},{"location":"reader/excelreader/#supported-data-types","title":"Supported Data Types","text":"<p>The Excel reading functionality implementation depends on the Apache POI project, which has a very broad definition of cell data types. It only defines three types: Boolean, Double (numeric), and String. Among them, numeric type includes all integers, decimals, and dates. Currently, simple distinction is made for numeric types:</p> <ol> <li>Use library utility classes to detect if it's a date type, if so, determine it as date type</li> <li>Convert the numeric value to long integer and compare with the original value, if equal, determine as Long type</li> <li>Otherwise determine as Double type</li> </ol>"},{"location":"reader/excelreader/#limitations","title":"Limitations","text":"<ol> <li>Currently only reads the first Sheet of the file and ignores other Sheets</li> <li>Does not support specifying column reading</li> <li>Does not support skipping trailing rows (for example, summary tail rows may not meet requirements)</li> <li>Does not check if the number of columns in each row is equal, Excel must ensure this</li> <li>Only reads files with <code>xlsx</code> or <code>xls</code> file extensions in the specified directory, other extension files will be ignored with warning messages</li> </ol>"},{"location":"reader/ftpreader/","title":"FTP Reader","text":"<p>FTP Reader provides the ability to read data storage from remote FTP/SFTP file systems.</p>"},{"location":"reader/ftpreader/#functionality","title":"Functionality","text":""},{"location":"reader/ftpreader/#configuration-example","title":"Configuration Example","text":"job/ftp2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"ftpreader\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"127.0.0.1\",\n          \"port\": 22,\n          \"username\": \"xx\",\n          \"password\": \"xxx\",\n          \"path\": [\n            \"/var/ftp/test.txt\",\n            \"/var/tmp/*.txt\",\n            \"/public/ftp\",\n            \"/public/a??.txt\"\n          ],\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"ftpWriter\",\n        \"parameter\": {\n          \"path\": \"/var/ftp/FtpWriter/result\",\n          \"fileName\": \"shihf\",\n          \"writeMode\": \"truncate\",\n          \"format\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/ftpreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description protocol Yes string None Server protocol, currently supports <code>ftp</code> and <code>sftp</code> transport protocols host Yes string None Server address port No int 22/21 If transport protocol is <code>sftp</code>, default is 22; if standard ftp protocol, default is 21 timeout No int 60000 Connection timeout for ftp server, in milliseconds (ms) connectPattern No string PASV Connection mode, only supports <code>PORT</code>, <code>PASV</code> modes. This parameter is only used for ftp protocol username Yes string None FTP server access username password No string None FTP server access password useKey No boolean false Whether to use private key login, only valid for sftp login keyPath No string <code>~/.ssh/id_rsa</code> Private key address keyPass No string None Private key password, if no private key password is set, no need to configure this path Yes list None Remote FTP file system path information, note that multiple paths can be supported, detailed description below column Yes <code>list&lt;map&gt;</code> None List of fields to read, type specifies the type of source data, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, default empty means no compression. Supports <code>zip</code>, <code>gz</code>, <code>bzip2</code> encoding No string <code>utf-8</code> File encoding configuration for reading skipHeader No boolean false CSV format files may have header titles that need to be skipped. Default is not to skip nullFormat No char <code>\\N</code> Define which strings can represent null maxTraversalLevel No int 100 Maximum number of folder levels allowed for traversal csvReaderConfig No map None CSV file reading parameter configuration, Map type. Default values used if not configured, see below"},{"location":"reader/hanareader/","title":"HANA Reader","text":"<p>HANA Reader plugin implements the ability to read data from SAP HANA.</p>"},{"location":"reader/hanareader/#example","title":"Example","text":"<p>The following configuration reads this table to terminal:</p> job/hanareader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hanareader\",\n          \"parameter\": {\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": {\n              \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n              \"table\": [\n                \"addax_tbl\"\n              ]\n            },\n            \"username\": \"system\",\n            \"password\": \"HXEHana1\"\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hana2stream.json</code></p>"},{"location":"reader/hanareader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"reader/hanareader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all parameters of RDBMS Reader.</p>"},{"location":"reader/hbase11xreader/","title":"HBase11X Reader","text":"<p>HBase11X Reader plugin supports reading data from HBase 1.x version. Its implementation method is to connect to remote HBase service through HBase's Java client and read data within the <code>rowkey</code> range you specify through Scan method.</p>"},{"location":"reader/hbase11xreader/#configuration","title":"Configuration","text":""},{"location":"reader/hbase11xreader/#table-creation-and-data-population","title":"Table Creation and Data Population","text":"<p>The following demonstration is based on the table and data created below:</p> <pre><code>create 'users', 'address','info'\nput 'users', 'lisi', 'address:country', 'china'\nput 'users', 'lisi', 'address:province',    'beijing'\nput 'users', 'lisi', 'info:age',        27\nput 'users', 'lisi', 'info:birthday',   '1987-06-17'\nput 'users', 'lisi', 'info:company',    'baidu'\nput 'users', 'xiaoming', 'address:city',    'hangzhou'\nput 'users', 'xiaoming', 'address:country', 'china'\nput 'users', 'xiaoming', 'address:province',    'zhejiang'\nput 'users', 'xiaoming', 'info:age',        29\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17'\nput 'users', 'xiaoming', 'info:company',    'alibaba'\n</code></pre>"},{"location":"reader/hbase11xreader/#normal-mode","title":"normal Mode","text":"<p>Read HBase table as a normal two-dimensional table (horizontal table), reading the latest version data. For example:</p> <pre><code>hbase(main):017:0&gt; scan 'users'\nROW           COLUMN+CELL\n lisi         column=address:city, timestamp=1457101972764, value=beijing\n lisi         column=address:country, timestamp=1457102773908, value=china\n lisi         column=address:province, timestamp=1457101972736, value=beijing\n lisi         column=info:age, timestamp=1457101972548, value=27\n lisi         column=info:birthday, timestamp=1457101972604, value=1987-06-17\n lisi         column=info:company, timestamp=1457101972653, value=baidu\n xiaoming     column=address:city, timestamp=1457082196082, value=hangzhou\n xiaoming     column=address:country, timestamp=1457082195729, value=china\n</code></pre> <p>For detailed configuration and parameters, please refer to the original HBase11X Reader documentation.</p>"},{"location":"reader/hbase11xsqlreader/","title":"HBase11x SQL Reader","text":"<p>HBase11x SQL Reader plugin implements reading data from Phoenix(HBase SQL), supporting HBase version 1.x.</p>"},{"location":"reader/hbase11xsqlreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Phoenix to local:</p> <pre><code>{\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"byte\":-1,\n              \"channel\": 1\n            }\n        },\n        \"content\": [ {\n                \"reader\": {\n                    \"name\": \"hbase11xsqlreader\",\n                    \"parameter\": {\n                        \"queryServerAddress\": \"http://127.0.0.1:8765\",\n                        \"serialization\": \"PROTOBUF\",\n                        \"table\": \"TEST\",\n                        \"column\": [\"ID\", \"NAME\"]\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"reader/hbase11xsqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/hbase20xreader/","title":"HBase20 Reader","text":"<p>HBase20 Reader plugin supports reading data from HBase 2.x version. Its implementation method is to connect to remote HBase service through HBase's Java client and read data within the <code>rowkey</code> range you specify through Scan method.</p>"},{"location":"reader/hbase20xreader/#configuration","title":"Configuration","text":"<p>The following demonstration is based on the table and data created below:</p> <pre><code>create 'users', {NAME=&gt;'address', VERSIONS=&gt;100},{NAME=&gt;'info',VERSIONS=&gt;1000}\nput 'users', 'lisi', 'address:country', 'china1', 20200101\nput 'users', 'lisi', 'address:province',    'beijing1', 20200101\nput 'users', 'lisi', 'info:age',        27, 20200101\nput 'users', 'lisi', 'info:birthday',   '1987-06-17', 20200101\nput 'users', 'lisi', 'info:company',    'baidu1', 20200101\nput 'users', 'xiaoming', 'address:city',    'hangzhou1', 20200101\nput 'users', 'xiaoming', 'address:country', 'china1', 20200101\nput 'users', 'xiaoming', 'address:province',    'zhejiang1',20200101\nput 'users', 'xiaoming', 'info:age',        29, 20200101\nput 'users', 'xiaoming', 'info:birthday',   '1987-06-17',20200101\n</code></pre> <p>For detailed configuration and parameters, please refer to the original HBase20 Reader documentation.</p>"},{"location":"reader/hbase20xsqlreader/","title":"HBase20 SQL Reader","text":"<p>HBase20 SQL Reader plugin implements reading data from Phoenix(HBase SQL), corresponding to HBase2.X and Phoenix5.X versions.</p>"},{"location":"reader/hbase20xsqlreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Phoenix to local:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"hbase20xsqlreader\",\n          \"parameter\": {\n            \"queryServerAddress\": \"http://127.0.0.1:8765\",\n            \"serialization\": \"PROTOBUF\",\n            \"table\": \"TEST\",\n            \"column\": [\"ID\", \"NAME\"]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"reader/hbase20xsqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/hdfsreader/","title":"HDFS Reader","text":"<p>HDFS Reader provides the ability to read data storage from distributed file system Hadoop HDFS.</p> <p>Currently HdfsReader supports the following file formats:</p> <ul> <li>textfile\uff08text\uff09</li> <li>orcfile\uff08orc\uff09</li> <li>rcfile\uff08rc\uff09</li> <li>sequence file\uff08seq\uff09</li> <li>Csv(csv)</li> <li>parquet</li> </ul>"},{"location":"reader/hdfsreader/#features-and-limitations","title":"Features and Limitations","text":"<ol> <li>Supports textfile, orcfile, parquet, rcfile, sequence file and csv format files, and requires that the file content stores a logically two-dimensional table.</li> <li>Supports reading multiple types of data (represented using String), supports column pruning, supports column constants</li> <li>Supports recursive reading, supports regular expressions (<code>*</code> and <code>?</code>).</li> <li>Supports common compression algorithms, including GZIP, SNAPPY, ZLIB, etc.</li> <li>Multiple Files can support concurrent reading.</li> <li>Supports sequence file data compression, currently supports lzo compression method.</li> <li>csv type supports compression formats: gzip, bz2, zip, lzo, lzo_deflate, snappy.</li> <li>Currently the Hive version in the plugin is <code>3.1.1</code>, Hadoop version is <code>3.1.1</code>, writes normally in Hadoop <code>2.7.x</code>, Hadoop <code>3.1.x</code> and Hive <code>2.x</code>, hive <code>3.1.x</code> test environments; other versions are theoretically supported, but please test further before using in production environments;</li> <li>Supports <code>kerberos</code> authentication</li> </ol>"},{"location":"reader/hdfsreader/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hdfsreader\",\n        \"parameter\": {\n          \"path\": \"/user/hive/warehouse/mytable01/*\",\n          \"defaultFS\": \"hdfs://xxx:port\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"hello\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            }\n          ],\n          \"fileType\": \"orc\",\n          \"encoding\": \"UTF-8\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/hdfsreader/#configuration-parameters","title":"Configuration Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File path to read defaultFS Yes string None HDFS <code>NAMENODE</code> node address, if HA mode is configured, it is the value of <code>defaultFS</code> fileType Yes string None File type column Yes <code>list&lt;map&gt;</code> None List of fields to read fieldDelimiter No char <code>,</code> Specify text file field delimiter, binary files do not need to specify this encoding No string <code>utf-8</code> File encoding configuration, currently only supports <code>utf-8</code> nullFormat No string None Characters that can represent null, if user configures: <code>\"\\\\N\"</code>, then if source data is <code>\"\\N\"</code>, it's treated as <code>null</code> field haveKerberos No boolean None Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Kerberos authentication credential file path, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Kerberos authentication credential principal, e.g. <code>addax/node1@WGZHAO.COM</code> compress No string None Specify compression format of files to read hadoopConfig No map None Can configure some advanced parameters related to Hadoop, such as HA configuration hdfsSitePath No string None Path to <code>hdfs-site.xml</code>, detailed explanation below"},{"location":"reader/hivereader/","title":"Hive Reader","text":"<p>Hive Reader plugin implements the ability to read data from Apache Hive database.</p> <p>The main purpose of adding this plugin is to solve the problem of Kerberos authentication when using RDBMS Reader plugin to read Hive database. If your Hive database does not have Kerberos authentication enabled, you can directly use RDBMS Reader. If Kerberos authentication is enabled, you can use this plugin.</p>"},{"location":"reader/hivereader/#example","title":"Example","text":"<p>We create the following table in Hive's test database and insert a record:</p> <pre><code>create table default.hive_reader\n(\n    col1 int,\n    col2 string,\n    col3 timestamp\n)\nstored as orc;\n\n\ninsert into hive_reader values(1, 'hello', current_timestamp()), (2, 'world', current_timestamp());\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/hive2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"hivereader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:hive2://localhost:10000/default;principal=hive/_HOST@EXAMPLE.COM\",\n            \"table\": [\n              \"hive_reader\"\n            ]\n          },\n          \"where\": \"logdate='20211013'\",\n          \"haveKerberos\": true,\n          \"kerberosKeytabFilePath\": \"/etc/security/keytabs/hive.headless.keytab\",\n          \"kerberosPrincipal\": \"hive@EXAMPLE.COM\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hive2stream.json</code></p>"},{"location":"reader/hivereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hive2stream.json\n</code></pre>"},{"location":"reader/hivereader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description jdbcUrl Yes list None JDBC connection information of target database driver No string None Custom driver class name to solve compatibility issues, see description below username Yes string None Username of data source password No string None Password for specified username of data source, can be omitted if no password"},{"location":"reader/httpreader/","title":"HTTP Reader","text":"<p>HTTP Reader plugin implements the ability to read Restful API data.</p>"},{"location":"reader/httpreader/#example","title":"Example","text":""},{"location":"reader/httpreader/#sample-interface-and-data","title":"Sample Interface and Data","text":"<p>The following configuration demonstrates how to get data from a specified API, assuming the accessed interface is:</p> <p>http://127.0.0.1:9090/mock/17/LDJSC/ASSET</p> <p>The interface accepts GET requests with the following parameters:</p> Parameter Name Example Value CURR_DATE 2021-01-17 DEPT 9400 USERNAME andi <p>The following is a sample of accessed data (actual returned data may vary slightly):</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": \"9700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1581.03,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 36.75,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.009448781026677719,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015153586011995693,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": 0.0652347643813081,\n      \"TMMARKET_VALUE_SHARECOM\": 0.024853621341525287,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.005242133578517903,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1645.1193961136973,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.16690149257388515,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.017886267801303465,\n      \"POTENTIAL_LOST_ASSETS\": 56.76,\n      \"TOTAL_LIABILITIES\": 57.81,\n      \"TOTAL_ASSETS\": 1306.33,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 4.79,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.006797058194980485,\n      \"NEW_ASSETS_DAY\": 14.92,\n      \"NEW_ASSETS_MON\": 90.29,\n      \"NEW_ASSETS_YEAR\": 297.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.04015576541561927,\n      \"NEW_FUNDS_DAY\": 18.16,\n      \"INFLOW_FUNDS_DAY\": 2.12,\n      \"OUTFLOW_FUNDS_DAY\": 9.73,\n      \"OVERALL_POSITION\": 0.810298404938773,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.03521615634095476,\n      \"NEW_CUST_FUNDS_MON\": 69.44,\n      \"INFLOW_FUNDS_MONTH\": 62.26,\n      \"OUTFLOW_FUNDS_MONTH\": 32.59\n    },\n    {\n      \"CURR_DATE\": \"2019-08-30\",\n      \"DEPT\": \"8700\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1596.74,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 41.86,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": 0.03470208565515685,\n      \"TMMARKET_VALUE_GROWTH_MON\": 0.07818120801111743,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.05440250244736409,\n      \"TMMARKET_VALUE_SHARECOM\": 0.09997733019626448,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.019726478499825697,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1007.9314679742108,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.15123738798885086,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.04694052069678048,\n      \"POTENTIAL_LOST_ASSETS\": 52.48,\n      \"TOTAL_LIABILITIES\": 55.28,\n      \"TOTAL_ASSETS\": 1366.72,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 10.12,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.009708491982487952,\n      \"NEW_ASSETS_DAY\": 12.42,\n      \"NEW_ASSETS_MON\": 41.14,\n      \"NEW_ASSETS_YEAR\": 279.32,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": -0.025878627161898062,\n      \"NEW_FUNDS_DAY\": 3.65,\n      \"INFLOW_FUNDS_DAY\": 14.15,\n      \"OUTFLOW_FUNDS_DAY\": 17.08,\n      \"OVERALL_POSITION\": 0.9098432997243932,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02111922282868306,\n      \"NEW_CUST_FUNDS_MON\": 57.21,\n      \"INFLOW_FUNDS_MONTH\": 61.16,\n      \"OUTFLOW_FUNDS_MONTH\": 15.83\n    },\n    {\n      \"CURR_DATE\": \"2019-06-30\",\n      \"DEPT\": \"6501\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1506.72,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": -13.23,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.0024973354204176554,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.015530793150701896,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.08556724628979398,\n      \"TMMARKET_VALUE_SHARECOM\": 0.15000077963967678,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.049629446804825755,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1250.1040863177336,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19098445630488178,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": -0.007881179708853471,\n      \"POTENTIAL_LOST_ASSETS\": 50.53,\n      \"TOTAL_LIABILITIES\": 56.62,\n      \"TOTAL_ASSETS\": 1499.53,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 29.56,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": -0.02599813232345556,\n      \"NEW_ASSETS_DAY\": 28.81,\n      \"NEW_ASSETS_MON\": 123.24,\n      \"NEW_ASSETS_YEAR\": 263.63,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.0073986669331394875,\n      \"NEW_FUNDS_DAY\": 18.52,\n      \"INFLOW_FUNDS_DAY\": 3.26,\n      \"OUTFLOW_FUNDS_DAY\": 6.92,\n      \"OVERALL_POSITION\": 0.8713692113306709,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": 0.02977644553289545,\n      \"NEW_CUST_FUNDS_MON\": 85.14,\n      \"INFLOW_FUNDS_MONTH\": 23.35,\n      \"OUTFLOW_FUNDS_MONTH\": 92.95\n    },\n    {\n      \"CURR_DATE\": \"2019-12-07\",\n      \"DEPT\": \"8705\",\n      \"TOTAL_MANAGED_MARKET_VALUE\": 1575.85,\n      \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\": 8.94,\n      \"TMMARKET_VALUE_DOD_GROWTH_RATE\": -0.04384846980627058,\n      \"TMMARKET_VALUE_GROWTH_MON\": -0.022962456288549656,\n      \"TMMARKET_VALUE_GROWTH_YEAR\": -0.005047009316021089,\n      \"TMMARKET_VALUE_SHARECOM\": 0.07819484815809447,\n      \"TMMARKET_VALUE_SHARE_GROWTH_RATE\": -0.008534369960890256,\n      \"AVERAGE_NEW_ASSETS_DAYINMON\": 1340.0339240689955,\n      \"YEAR_NEW_ASSET_SSHARECOM\": 0.19019952857677042,\n      \"YN_ASSET_SSHARECOM_GROWTH_RATE\": 0.01272353909992914,\n      \"POTENTIAL_LOST_ASSETS\": 54.63,\n      \"TOTAL_LIABILITIES\": 53.17,\n      \"TOTAL_ASSETS\": 1315.08,\n      \"TOTAL_ASSETS_DOD_GROWTH\": 49.31,\n      \"TOTAL_ASSETS_DOD_GROWTH_RATE\": 0.0016538407028265922,\n      \"NEW_ASSETS_DAY\": 29.17,\n      \"NEW_ASSETS_MON\": 44.75,\n      \"NEW_ASSETS_YEAR\": 172.87,\n      \"NEW_ASSETS_DOD_GROWTH_RATE\": 0.045388692595736746,\n      \"NEW_FUNDS_DAY\": 18.46,\n      \"INFLOW_FUNDS_DAY\": 12.93,\n      \"OUTFLOW_FUNDS_DAY\": 10.38,\n      \"OVERALL_POSITION\": 0.8083127036694828,\n      \"OVERALL_POSITION_DOD_GROWTH_RATE\": -0.02847453515632541,\n      \"NEW_CUST_FUNDS_MON\": 49.74,\n      \"INFLOW_FUNDS_MONTH\": 81.93,\n      \"OUTFLOW_FUNDS_MONTH\": 18.17\n    }\n  ]\n}\n</code></pre> <p>We need to get partial key value data from the <code>result</code> results.</p>"},{"location":"reader/httpreader/#configuration","title":"Configuration","text":"<p>The following configuration implements getting data from the interface and printing to terminal</p> job/httpreader2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"httpreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"url\": \"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\",\n              \"proxy\": {\n                \"host\": \"http://127.0.0.1:3128\",\n                \"auth\": \"user:pass\"\n              }\n            }\n          ],\n          \"reqParams\": {\n            \"CURR_DATE\": \"2021-01-18\",\n            \"DEPT\": \"9700\"\n          },\n          \"resultKey\": \"result\",\n          \"method\": \"GET\",\n          \"column\": [\n            \"CURR_DATE\",\n            \"DEPT\",\n            \"TOTAL_MANAGED_MARKET_VALUE\",\n            \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n          ],\n          \"username\": \"user\",\n          \"password\": \"passw0rd\",\n          \"headers\": {\n            \"X-Powered-by\": \"Addax\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/httpreader2stream.json</code> file.</p>"},{"location":"reader/httpreader/#execution","title":"Execution","text":"<p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/httpreader2stream.json\n</code></pre> <p>The output of the above command is roughly as follows:</p> <pre><code>2021-01-20 09:07:41.864 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; com.sun.management.internal.OperatingSystemImpl\n2021-01-20 09:07:41.877 [main] INFO  Engine - the machine info  =&gt;\n\n    osInfo:     Mac OS X x86_64 10.15.1\n    jvmInfo:    AdoptOpenJDK 14 14.0.2+12\n    cpu num:    8\n\n    totalPhysicalMemory:    -0.00G\n    freePhysicalMemory: -0.00G\n    maxFileDescriptorCount: -1\n    currentOpenFileDescriptorCount: -1\n\n    GC Names    [G1 Young Generation, G1 Old Generation]\n\n    MEMORY_NAME                    | allocation_size                | init_size\n    CodeHeap 'profiled nmethods'   | 117.21MB                       | 2.44MB\n    G1 Old Gen                     | 2,048.00MB                     | 39.00MB\n    G1 Survivor Space              | -0.00MB                        | 0.00MB\n    CodeHeap 'non-profiled nmethods' | 117.21MB                       | 2.44MB\n    Compressed Class Space         | 1,024.00MB                     | 0.00MB\n    Metaspace                      | -0.00MB                        | 0.00MB\n    G1 Eden Space                  | -0.00MB                        | 25.00MB\n    CodeHeap 'non-nmethods'        | 5.57MB                         | 2.44MB\n\n\n2021-01-20 09:07:41.903 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"reqParams\":{\n                        \"CURR_DATE\":\"2021-01-18\",\n                        \"DEPT\":\"9700\"\n                    },\n                    \"method\":\"GET\",\n                    \"column\":[\n                        \"CURR_DATE\",\n                        \"DEPT\",\n                        \"TOTAL_MANAGED_MARKET_VALUE\",\n                        \"TOTAL_MANAGED_MARKET_VALUE_GROWTH\"\n                    ],\n                    \"resultKey\":\"result\",\n                    \"connection\":[\n                        {\n                            \"url\":\"http://127.0.0.1:9090/mock/17/LDJSC/ASSET\"\n                        }\n                    ]\n                },\n                \"name\":\"httpreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":\"true\"\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"bytes\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-01-20 09:07:41.926 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-01-20 09:07:41.927 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-01-20 09:07:41.928 [main] INFO  JobContainer - Set jobId = 0\n2021-01-20 09:07:42.002 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started\n\n2019-08-30  9700    1539.85 -14.78\n2019-10-01  9700    1531.71 47.66\n2020-12-03  9700    1574.38 7.34\n2020-11-31  9700    1528.13 41.62\n2019-03-01  9700    1554.28 -9.29\n\n2021-01-20 09:07:45.006 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-01-20 09:07:41\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-01-20 09:07:44\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :               42B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :              1rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                   5\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/httpreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description url Yes string None HTTP address to access reqParams No map None Interface request parameters resultKey No string None Key value to get results, if getting entire return value, no need to fill method No string get Request mode, only supports GET and POST, case insensitive column Yes list None Keys to get, configure as <code>\"*\"</code> to get all key values username No string None Authentication account required for interface request (if any) password No string None Password required for interface request (if any) proxy No map None Proxy address, see description below headers No map None Custom request header information isPage No boolean None Whether interface supports pagination pageParams No map None Pagination parameters"},{"location":"reader/httpreader/#reqparams","title":"reqParams","text":"<p>reqParams are request parameters. If the request is <code>GET</code> method, it will be appended to the <code>url</code> in <code>k=v</code> format. If the request is <code>POST</code> mode, <code>reqParams</code> will be sent as JSON content in the request body. In particular, in <code>POST</code> mode, if your request body is not a <code>k-v</code> structure, you can set the <code>key</code> to empty string, like:</p> <pre><code>{\n  \"reqParams\": {\n    \"\": [123,3456]\n  }\n}\n</code></pre> <p>The program will handle this case specially.</p>"},{"location":"reader/httpreader/#proxy","title":"proxy","text":"<p>If the accessed interface needs to go through a proxy, you can configure the <code>proxy</code> configuration item, which is a json dictionary containing a required <code>host</code> field and an optional <code>auth</code> field.</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"http://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p>For <code>socks</code> proxy (V4, V5), you can write:</p> <pre><code>{\n  \"proxy\": {\n    \"host\": \"socks://127.0.0.1:8080\",\n    \"auth\": \"user:pass\"\n  }\n}\n</code></pre> <p><code>host</code> is the proxy address, including proxy type. Currently only supports <code>http</code> proxy and <code>socks</code> (both V4 and V5) proxy. If the proxy requires authentication, you can configure <code>auth</code>, which consists of username and password separated by colon (<code>:</code>).</p>"},{"location":"reader/httpreader/#column","title":"column","text":"<p>Besides directly specifying keys, <code>column</code> also allows using JSON Xpath style to specify key values to get. Suppose you want to read the following JSON file:</p> <pre><code>{\n  \"result\": [\n    {\n      \"CURR_DATE\": \"2019-12-09\",\n      \"DEPT\": {\n        \"ID\": \"9700\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 1\n        },\n        {\n          \"COL2\": 2\n        }\n      ]\n    },\n    {\n      \"CURR_DATE\": \"2021-11-09\",\n      \"DEPT\": {\n        \"ID\": \"6500\"\n      },\n      \"KK\": [\n        {\n          \"COL1\": 3\n        },\n        {\n          \"COL2\": 4\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>If we want to read <code>CURR_DATE</code>, <code>ID</code>, <code>COL1</code>, <code>COL2</code> as four fields, your <code>column</code> can be configured like this:</p> <pre><code>{\n  \"column\": [\n    \"CURR_DATE\",\n    \"DEPT.ID\",\n    \"KK[0].COL1\",\n    \"KK[1].COL2\"\n  ]\n}\n</code></pre> <p>The execution result is as follows:</p> <pre><code>...\n2021-10-30 14:01:50.273 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n\n2019-12-09  9700    1   2\n2021-11-09  6500    3   4\n\n2021-10-30 14:01:53.283 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Writer.Job [streamwriter] do post work.\n2021-10-30 14:01:53.284 [       job-0] INFO  JobContainer         - Addax Reader.Job [httpreader] do post work.\n2021-10-30 14:01:53.286 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-10-30 14:01:53.289 [       job-0] INFO  JobContainer         -\nTask start time                    : 2021-10-30 14:01:50\nTask end time                      : 2021-10-30 14:01:53\nTask total duration                :                  3s\nTask average throughput            :               10B/s\nRecord write speed                 :              0rec/s\nTotal records read                 :                   2\nTotal read/write failures          :                   0\n</code></pre> <p>Note: If you specify a non-existent key, it returns NULL value directly.</p>"},{"location":"reader/httpreader/#ispage","title":"isPage","text":"<p>The <code>isPage</code> parameter is used to specify whether the interface supports pagination. It is a boolean value. If <code>true</code>, it means the interface supports pagination, otherwise it doesn't.</p> <p>When the interface supports pagination, it will automatically paginate reading until the number of records returned by the interface's last return is less than the number of records per page.</p>"},{"location":"reader/httpreader/#pageparams","title":"pageParams","text":"<p>The <code>pageParams</code> parameter only takes effect when the <code>isPage</code> parameter is <code>true</code>. It is a JSON dictionary containing two optional fields <code>pageIndex</code> and <code>pageSize</code>.</p> <p><code>pageIndex</code> is used to indicate the current page for pagination. It is a JSON field containing two optional fields <code>key</code> and <code>value</code>, where <code>key</code> specifies the parameter name for page number, and <code>value</code> specifies the current page number value.</p> <p><code>pageSize</code> is used to indicate the page size for pagination. It is a JSON field containing two optional fields <code>key</code> and <code>value</code>, where <code>key</code> specifies the parameter name for page size, and <code>value</code> specifies the page size value.</p> <p>The default values for these two parameters are:</p> <pre><code>{\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"pageIndex\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"pageSize\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>If your interface pagination parameters are not <code>pageIndex</code> and <code>pageSize</code>, you can specify them through the <code>pageParams</code> parameter. For example:</p> <pre><code>{\n  \"isPage\": true,\n  \"pageParams\": {\n    \"pageIndex\": {\n      \"key\": \"page\",\n      \"value\": 1\n    },\n    \"pageSize\": {\n      \"key\": \"size\",\n      \"value\": 100\n    }\n  }\n}\n</code></pre> <p>This means the pagination parameters passed to the interface are <code>page=1&amp;size=100</code>.</p>"},{"location":"reader/httpreader/#limitations","title":"Limitations","text":"<ol> <li>The returned result must be JSON type</li> <li>Currently all key values are treated as string type</li> <li>Interface Token authentication mode not yet supported</li> <li>Pagination retrieval not yet supported</li> <li>Proxy only supports <code>http</code> mode</li> </ol>"},{"location":"reader/influxdb2reader/","title":"InfluxDB2 Reader","text":"<p>InfluxDB2 Reader plugin implements reading data from InfluxDB version 2.0 and above.</p> <p>Note: If your InfluxDB is version 1.8 and below, you should use the InfluxDBReader plugin.</p>"},{"location":"reader/influxdb2reader/#example","title":"Example","text":"<p>The following example demonstrates how this plugin reads data from specified tables (i.e., metrics) and outputs to terminal.</p>"},{"location":"reader/influxdb2reader/#create-job-file","title":"Create Job File","text":"<p>Create <code>job/influx2stream.json</code> file with the following content:</p> job/influx2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdb2reader\",\n        \"parameter\": {\n          \"column\": [\n            \"location\",\n            \"height\",\n            \"wet\"\n          ],\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"bucket\": \"test\",\n            \"table\": [\n              \"temperature\"\n            ],\n            \"org\": \"com.wgzhao\"\n          },\n          \"token\": \"YOUR_SECURE_TOKEN\",\n          \"range\": [\n            \"-1h\",\n            \"-5m\"\n          ],\n          \"limit\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/influxdb2reader/#run","title":"Run","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/influx2stream.json\n</code></pre>"},{"location":"reader/influxdb2reader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None InfluxDB connection string token Yes string None Token for accessing database table No list None Selected table names (i.e., metrics) to be synchronized org Yes string None Specify InfluxDB org name bucket Yes string None Specify InfluxDB bucket name column No list None Collection of column names to be synchronized in configured table, detailed description see rdbmreader range Yes list None Time range for reading data limit No int None Limit number of records to get"},{"location":"reader/influxdb2reader/#column","title":"column","text":"<p>If <code>column</code> is not specified, or <code>column</code> is specified as <code>[\"*\"]</code>, all valid <code>_field</code> fields and <code>_time</code> field will be read. Otherwise, read according to specified fields.</p>"},{"location":"reader/influxdb2reader/#range","title":"range","text":"<p><code>range</code> is used to specify the time range for reading metrics, with the following format:</p> <pre><code>{\n  \"range\": [\"start_time\", \"end_time\"]\n}\n</code></pre> <p><code>range</code> consists of a list of two strings, the first string represents start time, the second represents end time. The time expression format must comply with Flux format requirements, like this:</p> <pre><code>{\n  \"range\": [\"-15h\", \"-2h\"]\n}\n</code></pre> <p>If you don't want to specify the second end time, you can omit it, like this:</p> <pre><code>{\n  \"range\": [\"-15h\"]\n}\n</code></pre>"},{"location":"reader/influxdb2reader/#type-conversion","title":"Type Conversion","text":"<p>Current implementation treats all fields as strings.</p>"},{"location":"reader/influxdb2reader/#limitations","title":"Limitations","text":"<ol> <li>Current plugin only supports version 2.0 and above</li> </ol>"},{"location":"reader/influxdbreader/","title":"InfluxDB Reader","text":"<p>InfluxDBReader plugin implements reading data from InfluxDB. The underlying implementation calls InfluxQL language to query tables and get returned data.</p>"},{"location":"reader/influxdbreader/#example","title":"Example","text":"<p>The following example demonstrates how this plugin reads data from specified tables (i.e., metrics) and outputs to terminal</p>"},{"location":"reader/influxdbreader/#create-required-database-tables-and-data","title":"Create Required Database Tables and Data","text":"<p>Use the following commands to create tables and data to be read:</p> <pre><code># create database\ninflux --execute \"CREATE DATABASE NOAA_water_database\"\n# download sample data\ncurl https://s3.amazonaws.com/noaa.water-database/NOAA_data.txt -o NOAA_data.txt\n# import data via influx-cli\ninflux -import -path=NOAA_data.txt -precision=s -database=NOAA_water_database\n</code></pre>"},{"location":"reader/influxdbreader/#create-job-file","title":"Create Job File","text":"<p>Create <code>job/influxdb2stream.json</code> file with the following content:</p> job/influxdb2stream.json <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdbreader\",\n        \"parameter\": {\n          \"column\": [\n            \"*\"\n          ],\n          \"where\": \"1=1\",\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"NOAA_water_database\",\n            \"table\": \"h2o_feet\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/influxdbreader/#run","title":"Run","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/influxdb2stream.json\n</code></pre>"},{"location":"reader/influxdbreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None InfluxDB connection string username Yes string None Username of data source password No string None Password for specified username of data source database Yes string None Database specified by data source table Yes string None Selected table name to be synchronized column Yes list None Collection of column names to be synchronized in configured table, detailed description see rdbmreader connTimeout No int 15 Set connection timeout value, in seconds readTimeout No int 20 Set read timeout value, in seconds writeTimeout No int 20 Set write timeout value, in seconds where No string None Filtering conditions for the table querySql No string None Use SQL query to get data, if configured, <code>table</code> and <code>column</code> configuration items are invalid"},{"location":"reader/influxdbreader/#type-conversion","title":"Type Conversion","text":"<p>Current implementation treats all fields as strings.</p>"},{"location":"reader/influxdbreader/#limitations","title":"Limitations","text":"<ol> <li>Current plugin only supports version 1.x, version 2.0 and above are not supported</li> </ol>"},{"location":"reader/jsonfilereader/","title":"JSON File Reader","text":"<p>JSON File Reader provides the ability to read data from local file system storage.</p>"},{"location":"reader/jsonfilereader/#configuration-example","title":"Configuration Example","text":"job/json2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      },\n      \"reader\": {\n        \"name\": \"jsonfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/test*.json\"\n          ],\n          \"column\": [\n            {\n              \"index\": \"$.id\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.name\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": \"$.age\",\n              \"type\": \"long\"\n            },\n            {\n              \"index\": \"$.score.math\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.score.english\",\n              \"type\": \"double\"\n            },\n            {\n              \"index\": \"$.pubdate\",\n              \"type\": \"date\"\n            },\n            {\n              \"type\": \"string\",\n              \"value\": \"constant string\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Where <code>/tmp/test*.json</code> are multiple copies of the same JSON file, with content as follows:</p> <pre><code>{\"name\": \"zhangshan\",\"id\": 19890604,\"age\": 12,\"score\": {\"math\": 92.5,\"english\": 97.5,\"chinese\": 95},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"lisi\",\"id\": 19890605,\"age\": 12,\"score\": {\"math\": 90.5,\"english\": 77.5,\"chinese\": 90},\"pubdate\": \"2020-09-05\"}\n{\"name\": \"wangwu\",\"id\": 19890606,\"age\": 12,\"score\": {\"math\": 89,\"english\": 100,\"chinese\": 92},\"pubdate\": \"2020-09-05\"}\n</code></pre>"},{"location":"reader/jsonfilereader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes list None Local file system path information, note that multiple paths can be supported column Yes list None List of fields to read, type specifies the type of source data fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, default empty means no compression. Supports zip, gzip, bzip2 encoding No string utf-8 Encoding configuration for reading files singleLine No boolean true Whether each data record is on one line"},{"location":"reader/jsonfilereader/#path","title":"path","text":"<p>Local file system path information, note that multiple paths can be supported, for example:</p> <pre><code>{\n  \"path\": [\n    \"/var/ftp/test.json\", // Read test.json file in /var/ftp directory\n    \"/var/tmp/*.json\", // Read all json files in /var/tmp directory\n    \"/public/ftp\", // Read all files in /public/ftp directory, if ftp is a file, read directly\n    \"/public/a??.json\" // Read all files in /public directory starting with 'a', followed by two characters, ending with json\n  ]\n}\n</code></pre> <p>It is particularly important to note that if there are no matching files for extraction under the path specified by Path, Addax will report an error.</p>"},{"location":"reader/jsonfilereader/#column","title":"column","text":"<p>List of fields to read, type specifies the type of source data, index specifies the current column from json specification using Jayway JsonPath syntax, value specifies that the current type is constant, not reading data from source file, but automatically generating corresponding columns based on value. Users must specify Column field information.</p> <p>For user-specified Column information, type must be filled, and index/value must choose one.</p>"},{"location":"reader/jsonfilereader/#singleline","title":"singleLine","text":"<p>There are two ways to store data in JSON format in the industry: one is one JSON object per line, which is <code>Single Line JSON (aka. JSONL or JSON Lines)</code>; the other is that the entire file is a JSON array, and each element is a JSON object, which is <code>Multiline JSON</code>.</p> <p>Addax supports one JSON object per line by default, i.e., <code>singleLine = true</code>. In this case, note that:</p> <ol> <li>There should be no comma at the end of each line's JSON object, otherwise parsing will fail.</li> <li>A JSON object cannot span multiple lines, otherwise parsing will fail.</li> </ol> <p>If the data is an entire file as a JSON array with each element being a JSON object, you need to set <code>singleLine</code> to <code>false</code>. Suppose the data in the above example is represented in the following format:</p> <pre><code>{\n  \"result\": [\n    {\n      \"name\": \"zhangshan\",\n      \"id\": 19890604,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 92.5,\n        \"english\": 97.5,\n        \"chinese\": 95\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"lisi\",\n      \"id\": 19890605,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 90.5,\n        \"english\": 77.5,\n        \"chinese\": 90\n      },\n      \"pubdate\": \"2020-09-05\"\n    },\n    {\n      \"name\": \"wangwu\",\n      \"id\": 19890606,\n      \"age\": 12,\n      \"score\": {\n        \"math\": 89,\n        \"english\": 100,\n        \"chinese\": 92\n      },\n      \"pubdate\": \"2020-09-05\"\n    }\n  ]\n}\n</code></pre> <p>Because this format is valid JSON format, each JSON object can span multiple lines. Correspondingly, when reading such data, its <code>path</code> configuration should be filled as follows:</p> <pre><code>{\n  \"singleLine\": false,\n  \"column\": [\n    {\n      \"index\": \"$.result[*].id\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].name\",\n      \"type\": \"string\"\n    },\n    {\n      \"index\": \"$.result[*].age\",\n      \"type\": \"long\"\n    },\n    {\n      \"index\": \"$.result[*].score.math\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$.result[*].score.english\",\n      \"type\": \"double\"\n    },\n    {\n      \"index\": \"$..result[*].pubdate\",\n      \"type\": \"date\"\n    },\n    {\n      \"type\": \"string\",\n      \"value\": \"constant string\"\n    }\n  ]\n}\n</code></pre> <p>For more detailed usage instructions, please refer to Jayway JsonPath syntax.</p> <p>Note: When this type of data is in a JSON array, the program can only read the entire file into memory and then parse it, so it is not suitable for reading large files. For reading large files, it is recommended to use the format of one JSON object per line, which is the <code>Single Line JSON</code> format. This format can be read line by line without taking up too much memory.</p>"},{"location":"reader/jsonfilereader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Local File Data Type Long Long Double Double String String Boolean Boolean Date Date"},{"location":"reader/kafkareader/","title":"Kafka Reader","text":"<p>Kafka Reader plugin implements the functionality of reading JSON format messages from Kafka queues. This plugin was introduced in version <code>4.0.10</code>.</p>"},{"location":"reader/kafkareader/#example","title":"Example","text":"<p>The following configuration demonstrates how to read from specified topics in Kafka and output to terminal.</p>"},{"location":"reader/kafkareader/#create-task-file","title":"Create Task File","text":"<p>First create a task file <code>kafka2stream.json</code>, with the following content:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": [\n      {\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"print\": true\n          }\n        },\n        \"reader\": {\n          \"name\": \"kafkareader\",\n          \"parameter\": {\n            \"brokerList\": \"wgzhao-laptop:9092\",\n            \"topic\": \"test-1\",\n            \"column\": [\n              \"col1\",\n              \"col3\",\n              \"col0\",\n              \"col9\"\n            ],\n            \"missingKeyValue\": \"\\\\N\",\n            \"properties\": {\n              \"auto.offset.reset\": \"earliest\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"reader/kafkareader/#run","title":"Run","text":"<p>Execute the <code>bin/addax.sh kafka2stream.json</code> command.</p>"},{"location":"reader/kafkareader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description brokerList Yes string None Broker configuration for connecting to kafka service, like <code>localhost:9092</code>, multiple brokers separated by commas (<code>,</code>) topic Yes string None Topic to write to column Yes list None Collection of column names to be synchronized in the configured table, detailed below missingKeyValue No string None What value to fill when field does not exist, detailed below properties No map None Other kafka connection parameters to be set"},{"location":"reader/kafkareader/#column","title":"column","text":"<p><code>column</code> is used to specify the keys to read from JSON messages. If set to <code>*</code>, it means reading all keys in the message. Note that in this case, the output will not be sorted, meaning the output order of keys for each record is not guaranteed to be consistent.</p> <p>You can also specify keys to read, for example:</p> <pre><code>{\n  \"column\": [\"col1\", \"col2\", \"col3\"]\n}\n</code></pre> <p>This way, the plugin will try to read the corresponding keys in the given order. If a key to be read does not exist in a message, the plugin will report an error and exit. If you want it not to exit, you can set <code>missingKeyValue</code>, which represents the value to fill when the key to be read does not exist.</p> <p>Additionally, the plugin will automatically guess the type of the key value being read. If the type cannot be guessed, it will be treated as String type.</p>"},{"location":"reader/kafkareader/#limitations","title":"Limitations","text":"<ol> <li>Only supports Kafka <code>1.0</code> and above versions, versions below this cannot be guaranteed to work</li> <li>Currently does not support kafka services with <code>kerberos</code> authentication enabled</li> </ol>"},{"location":"reader/kudureader/","title":"Kudu Reader","text":"<p>Kudu Reader plugin uses Kudu's Java client KuduClient to perform Kudu read operations.</p>"},{"location":"reader/kudureader/#configuration-example","title":"Configuration Example","text":"<p>We connect to kudu service through Trino's <code>kudu connector</code>, then perform table creation and data insertion.</p>"},{"location":"reader/kudureader/#table-creation-and-data-insertion-statements","title":"Table Creation and Data Insertion Statements","text":"<pre><code>CREATE TABLE kudu.default.users (\n  user_id int WITH (primary_key = true),\n  user_name varchar with (nullable=true),\n  age int with (nullable=true),\n  salary double with (nullable=true),\n  longtitue decimal(18,6) with (nullable=true),\n  latitude decimal(18,6) with (nullable=true),\n  p decimal(21,20) with (nullable=true),\n  mtime timestamp with (nullable=true)\n) WITH (\n  partition_by_hash_columns = ARRAY['user_id'],\n  partition_by_hash_buckets = 2\n);\n\ninsert into kudu.default.users \nvalues \n(1, cast('wgzhao' as varchar), 18, cast(18888.88 as double), \n cast(123.282424 as decimal(18,6)), cast(23.123456 as decimal(18,6)),\n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 14:40:41'),\n(2, cast('anglina' as varchar), 16, cast(23456.12 as double), \n cast(33.192123 as decimal(18,6)), cast(56.654321 as decimal(18,6)), \n cast(1.12345678912345678912 as decimal(21,20)), \n timestamp '2021-01-10 03:40:41');\n-- ONLY insert primary key value\n insert into kudu.default.users(user_id) values  (3);\n</code></pre>"},{"location":"reader/kudureader/#configuration","title":"Configuration","text":"<p>The following is the configuration for reading kudu table and outputting to terminal:</p> job/kudu2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"kudureader\",\n        \"parameter\": {\n          \"masterAddress\": \"localhost:7051,localhost:7151,localhost:7251\",\n          \"table\": \"users\",\n          \"splitPk\": \"user_id\",\n          \"lowerBound\": 1,\n          \"upperBound\": 100,\n          \"readTimeout\": 5,\n          \"scanTimeout\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/kudu2stream.json</code></p>"},{"location":"reader/kudureader/#execution","title":"Execution","text":"<p>Execute the following command for collection</p> <pre><code>bin/addax.sh job/kudu2stream.json\n</code></pre>"},{"location":"reader/kudureader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description masterAddress Yes string None Kudu Master cluster RPC address, multiple addresses separated by comma (,) table Yes string None Kudu table name splitPk No string None Parallel reading data shard field lowerBound No string None Lower bound of parallel reading data shard range upperBound No string None Upper bound of parallel reading data shard range readTimeout No int 10 Read data timeout (seconds) scanTimeout No int 20 Data scan request timeout (seconds) column No list None Specify fields to get where No list None Specify other filter conditions, see description below haveKerberos No boolean false Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Credential file path for Kerberos authentication, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Credential principal for Kerberos authentication, e.g. <code>addax/node1@WGZHAO.COM</code>"},{"location":"reader/kudureader/#where","title":"where","text":"<p><code>where</code> is used to define more filter conditions. It is an array type, where each element of the array is a filter condition, for example:</p> <pre><code>{\n  \"where\": [\"age &gt; 1\", \"user_name = 'wgzhao'\"] \n}\n</code></pre> <p>The above defines two filter conditions. Each filter condition consists of three parts in the format <code>column operator value</code>:</p> <ul> <li><code>column</code>: Field to filter</li> <li><code>operator</code>: Comparison symbol, currently only supports <code>=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>!=</code>. Other operators are not currently supported</li> <li><code>value</code>: Comparison value</li> </ul> <p>There are other limitations here that need special attention when using:</p> <ol> <li>Multiple filter conditions have logical AND relationship between them, logical OR relationship is not supported yet</li> </ol>"},{"location":"reader/kudureader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Kudu Data Type Long byte, short, int, long Double float, double, decimal String string Date timestamp Boolean boolean Bytes binary"},{"location":"reader/mongodbreader/","title":"MongoDB Reader","text":"<p>MongoDBReader plugin uses MongoDB's Java client MongoClient to perform MongoDB read operations.</p>"},{"location":"reader/mongodbreader/#configuration-example","title":"Configuration Example","text":"<p>This example reads a table from MongoDB and prints to terminal</p> job/mongo2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"mongodbreader\",\n        \"parameter\": {\n          \"username\": \"\",\n          \"password\": \"\",\n          \"column\": [\n            \"unique_id\",\n            \"sid\",\n            \"user_id\",\n            \"auction_id\",\n            \"content_type\",\n            \"pool_type\",\n            \"frontcat_id\",\n            \"catagoryid\",\n            \"gmt_create\",\n            \"taglist\",\n            \"property\",\n            \"scorea\",\n            \"scoreb\",\n            \"scorec\"\n          ],\n          \"connection\": {\n            \"address\": [\n              \"127.0.0.1:27017\"\n            ],\n            \"database\": \"tag_per_data\",\n            \"collection\": \"tag_data\",\n            \"authDb\": \"admin\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/mongodbreader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description address Yes list None MongoDB data address information, multiple can be written username No string None MongoDB username password No string None MongoDB password database Yes string None MongoDB database collection Yes string None MongoDB collection name column Yes list None MongoDB document column names, does not support <code>[\"*\"]</code> to get all columns query No string None Custom query conditions fetchSize No int 2048 Batch size for retrieving records"},{"location":"reader/mongodbreader/#collection","title":"collection","text":"<p>The <code>collection</code> here currently only supports a single collection, so the type is set to string rather than the array type common in other plugins. This is particularly noteworthy.</p>"},{"location":"reader/mongodbreader/#column","title":"column","text":"<p><code>column</code> is used to specify the field names to be read. Here we make two assumptions about field name composition:</p> <ul> <li>Cannot start with single quote (<code>'</code>)</li> <li>Cannot consist entirely of numbers and dots (<code>.</code>)</li> </ul> <p>Based on the above assumptions, we can simplify the <code>column</code> configuration while also specifying some constants as supplementary fields. For example, when collecting a table, we generally need to add collection time, collection source and other constants, which can be configured like this:</p> <pre><code>{\n  \"column\": [\n    \"col1\",\n    \"col2\",\n    \"col3\",\n    \"'source_mongodb'\",\n    \"20211026\",\n    \"123.12\"\n  ]\n}\n</code></pre> <p>The last three fields in the above configuration are constants, treated as string type, integer type, and floating point type respectively.</p>"},{"location":"reader/mongodbreader/#query","title":"query","text":"<p><code>query</code> is a BSON string that conforms to MongoDB query format, for example:</p> <pre><code>{\n  \"query\": \"{amount: {$gt: 140900}, oc_date: {$gt: 20190110}}\"\n}\n</code></pre> <p>The above query is similar to <code>where amount &gt; 140900 and oc_date &gt; 20190110</code> in SQL.</p>"},{"location":"reader/mongodbreader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type MongoDB Data Type Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes"},{"location":"reader/mysqlreader/","title":"MySQL Reader","text":"<p>The MySQLReader plugin enables reading data from MySQL databases.</p>"},{"location":"reader/mysqlreader/#example","title":"Example","text":"<p>Let's create a table in MySQL's test database and insert a record:</p> <pre><code>CREATE TABLE IF NOT EXISTS test_table (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    age INT,\n    salary DECIMAL(10,2),\n    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO test_table (name, age, salary) VALUES \n('John Doe', 30, 50000.00),\n('Jane Smith', 25, 45000.00);\n</code></pre> <p>Here's a configuration to read from this table to the console:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"password\",\n            \"column\": [\"id\", \"name\", \"age\", \"salary\", \"created_time\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?useSSL=false&amp;serverTimezone=UTC\",\n                \"table\": [\"test_table\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre> <p>Save this configuration as <code>job/mysql2stream.json</code>.</p>"},{"location":"reader/mysqlreader/#execute-the-job","title":"Execute the Job","text":"<p>Run the following command to execute the data collection:</p> <pre><code>bin/addax.sh job/mysql2stream.json\n</code></pre>"},{"location":"reader/mysqlreader/#parameter-description","title":"Parameter Description","text":"<p>This plugin is based on the RDBMS Reader implementation, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/mysqlreader/#driver","title":"driver","text":"<p>The current Addax uses MySQL JDBC driver version 8.0 or higher, with the driver class name <code>com.mysql.cj.jdbc.Driver</code>, not <code>com.mysql.jdbc.Driver</code>. If you need to collect from a MySQL server lower than version <code>5.6</code> and need to use the <code>Connector/J 5.1</code> driver, you can follow these steps:</p> <p>Replace the built-in driver</p> <pre><code>rm -f plugin/reader/mysqlreader/libs/mysql-connector-java-*.jar\n</code></pre> <p>Copy the old driver to the plugin directory</p> <pre><code>cp mysql-connector-java-5.1.48.jar plugin/reader/mysqlreader/libs/\n</code></pre> <p>Specify the driver class name</p> <p>In your JSON file, configure <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p>"},{"location":"reader/mysqlreader/#required-parameters","title":"Required Parameters","text":"Parameter Description Required Default jdbcUrl JDBC connection URL Yes None username Database username Yes None password Database password Yes None table List of tables to read from Yes None column List of columns to read Yes None"},{"location":"reader/mysqlreader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Required Default splitPk Primary key for data splitting No None where WHERE clause for filtering No None querySql Custom SQL query No None fetchSize JDBC fetch size No 1024 driver JDBC driver class name No com.mysql.cj.jdbc.Driver"},{"location":"reader/mysqlreader/#data-type-mapping","title":"Data Type Mapping","text":"MySQL Type Addax Type Notes TINYINT, SMALLINT, MEDIUMINT, INT long BIGINT long FLOAT, DOUBLE, DECIMAL double VARCHAR, CHAR, TEXT string DATE, TIME, DATETIME, TIMESTAMP date BIT bool BINARY, VARBINARY, BLOB bytes"},{"location":"reader/mysqlreader/#performance-tuning","title":"Performance Tuning","text":""},{"location":"reader/mysqlreader/#split-key-configuration","title":"Split Key Configuration","text":"<p>For large tables, configure <code>splitPk</code> to enable parallel reading:</p> <pre><code>{\n  \"parameter\": {\n    \"splitPk\": \"id\",\n    \"channel\": 4\n  }\n}\n</code></pre>"},{"location":"reader/mysqlreader/#fetch-size-optimization","title":"Fetch Size Optimization","text":"<p>Adjust <code>fetchSize</code> based on your memory and network conditions:</p> <pre><code>{\n  \"parameter\": {\n    \"fetchSize\": 2048\n  }\n}\n</code></pre>"},{"location":"reader/mysqlreader/#query-optimization","title":"Query Optimization","text":"<p>Use <code>where</code> clause to filter data at the source:</p> <pre><code>{\n  \"parameter\": {\n    \"where\": \"created_time &gt;= '2023-01-01' AND status = 'active'\"\n  }\n}\n</code></pre>"},{"location":"reader/mysqlreader/#error-handling","title":"Error Handling","text":"<p>Common issues and solutions:</p>"},{"location":"reader/mysqlreader/#connection-timeout","title":"Connection Timeout","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?connectTimeout=60000&amp;socketTimeout=60000\"\n  }\n}\n</code></pre>"},{"location":"reader/mysqlreader/#ssl-connection-issues","title":"SSL Connection Issues","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?useSSL=false\"\n  }\n}\n</code></pre>"},{"location":"reader/mysqlreader/#timezone-issues","title":"Timezone Issues","text":"<pre><code>{\n  \"parameter\": {\n    \"jdbcUrl\": \"jdbc:mysql://localhost:3306/test?serverTimezone=UTC\"\n  }\n}\n</code></pre> <p>For more detailed configuration options, please refer to the RDBMS Reader documentation.</p>"},{"location":"reader/oraclereader/","title":"Oracle Reader","text":"<p>Oracle Reader plugin is used to read data from Oracle.</p>"},{"location":"reader/oraclereader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from Oracle database to local:</p> job/oracle2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"oraclereader\",\n        \"parameter\": {\n          \"username\": \"oracle\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:oracle:thin:@127.0.0.1:5432/orcl\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/oraclereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/oraclereader/#support-for-geometry-type","title":"Support for GEOMETRY Type","text":"<p>Starting from Addax <code>4.0.13</code>, experimental support for Oracle GEOMETRY type is provided. This plugin converts this type of data to JSON array strings.</p> <p>Suppose you have such a table and data:</p> <pre><code>\n</code></pre> <p>The final output result of reading this table data is similar to the following:</p> <pre><code>\n</code></pre> <p>Note: This data type is currently in experimental support stage. The author's understanding of this data type is not deep, and it has not been comprehensively tested. Please do not use it directly in production environments.</p>"},{"location":"reader/postgresqlreader/","title":"PostgreSQL Reader","text":"<p>The PostgreSQLReader plugin enables reading data from PostgreSQL databases.</p>"},{"location":"reader/postgresqlreader/#example","title":"Example","text":"<p>Create a sample table in PostgreSQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    email VARCHAR(100),\n    age INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO users (username, email, age) VALUES \n('alice', 'alice@example.com', 28),\n('bob', 'bob@example.com', 32),\n('charlie', 'charlie@example.com', 25);\n</code></pre> <p>Configuration to read from PostgreSQL:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"postgresqlreader\",\n          \"parameter\": {\n            \"username\": \"postgres\",\n            \"password\": \"password\",\n            \"column\": [\"id\", \"username\", \"email\", \"age\", \"created_at\"],\n            \"splitPk\": \"id\",\n            \"connection\": [\n              {\n                \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/testdb\",\n                \"table\": [\"users\"]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/postgresqlreader/#parameters","title":"Parameters","text":"<p>This plugin is based on the RDBMS Reader implementation.</p>"},{"location":"reader/postgresqlreader/#required-parameters","title":"Required Parameters","text":"Parameter Description Required Default jdbcUrl PostgreSQL JDBC connection URL Yes None username Database username Yes None password Database password Yes None table List of tables to read from Yes None column List of columns to read Yes None"},{"location":"reader/postgresqlreader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Required Default splitPk Primary key for data splitting No None where WHERE clause for filtering No None fetchSize JDBC fetch size No 1024"},{"location":"reader/postgresqlreader/#data-type-mapping","title":"Data Type Mapping","text":"PostgreSQL Type Addax Type Notes SMALLINT, INTEGER, BIGINT long REAL, DOUBLE PRECISION, NUMERIC double VARCHAR, CHAR, TEXT string DATE, TIME, TIMESTAMP date BOOLEAN bool BYTEA bytes"},{"location":"reader/postgresqlreader/#performance-tips","title":"Performance Tips","text":""},{"location":"reader/postgresqlreader/#use-split-key-for-large-tables","title":"Use Split Key for Large Tables","text":"<pre><code>{\n  \"parameter\": {\n    \"splitPk\": \"id\",\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 4\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/postgresqlreader/#optimize-with-where-clause","title":"Optimize with WHERE Clause","text":"<pre><code>{\n  \"parameter\": {\n    \"where\": \"created_at &gt;= '2023-01-01' AND status = 'active'\"\n  }\n}\n</code></pre>"},{"location":"reader/postgresqlreader/#connection-examples","title":"Connection Examples","text":""},{"location":"reader/postgresqlreader/#standard-connection","title":"Standard Connection","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb\"\n}\n</code></pre>"},{"location":"reader/postgresqlreader/#ssl-connection","title":"SSL Connection","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb?ssl=true&amp;sslmode=require\"\n}\n</code></pre>"},{"location":"reader/postgresqlreader/#connection-pool-settings","title":"Connection Pool Settings","text":"<pre><code>{\n  \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/mydb?prepareThreshold=0&amp;preparedStatementCacheQueries=0\"\n}\n</code></pre>"},{"location":"reader/rdbmsreader/","title":"RDBMS Reader","text":"<p>RDBMS Reader plugin supports reading data from traditional RDBMS. This is a generic relational database reading plugin that can support more relational database reading by registering database drivers.</p> <p>At the same time, RDBMS Reader is also the base class for other relational database reading plugins. The following reading plugins all depend on this plugin:</p> <ul> <li>Oracle Reader</li> <li>MySQL Reader</li> <li>PostgreSQL Reader</li> <li>ClickHouse Reader</li> <li>SQLServer Reader</li> <li>Access Reader</li> <li>Databend Reader</li> </ul> <p>Note: If a dedicated database reading plugin is already provided, it is recommended to use the dedicated plugin. If the database you need to read does not have a dedicated plugin, consider using this generic plugin. Before use, you need to perform the following operations to run normally, otherwise exceptions will occur.</p>"},{"location":"reader/rdbmsreader/#configure-driver","title":"Configure Driver","text":"<p>Suppose you need to read data from IBM DB2. Since no dedicated reading plugin is provided, we can use this plugin to implement it. Before use, you need to download the corresponding JDBC driver and copy it to the <code>plugin/reader/rdbmsreader/libs</code> directory. If your driver class name is special, you need to find the <code>driver</code> item in the task configuration file and fill in the correct JDBC driver name, such as DB2's driver name <code>com.ibm.db2.jcc.DB2Driver</code>. If not filled, the plugin will automatically guess the driver name.</p> <p>The following lists common databases and their corresponding driver names:</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"reader/rdbmsreader/#configuration","title":"Configuration","text":"<p>The following configuration shows how to read data from Presto database to terminal</p> job/rdbms2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": 1048576,\n        \"channel\": 1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"rdbmsreader\",\n        \"parameter\": {\n          \"username\": \"hive\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"driver\": \"io.prestosql.jdbc.PrestoDriver\",\n          \"connection\": {\n            \"table\": [\n              \"default.table\"\n            ],\n            \"jdbcUrl\": \"jdbc:presto://127.0.0.1:8080/hive\"\n          },\n          \"fetchSize\": 1024,\n          \"where\": \"1 = 1\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/rdbmsreader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description jdbcUrl Yes list None JDBC connection information of the target database, jdbcUrl follows RDBMS official specifications and can include connection attachment control information driver No string None Custom driver class name to solve compatibility issues, see description below username Yes string None Username of the data source password No string None Password for the specified username of the data source table Yes list None Selected table names to be synchronized, using JSON data format. When configured for multiple tables, users need to ensure that multiple tables have the same table structure column Yes list None Collection of column names to be synchronized in the configured table, detailed description below splitPk No string None Use the field represented by splitPk for data sharding, which can greatly improve data synchronization efficiency, see notes below autoPk No boolean false Whether to automatically guess the sharding primary key, introduced in version <code>3.2.6</code>, see description below where No string None Filtering conditions for the table session No list None For local connections, modify session configuration, see below querySql No string None Use custom SQL instead of specified table to get data. When this item is configured, <code>table</code> and <code>column</code> configuration items are ignored fetchSize No int 1024 Defines the number of batch data fetched between plugin and database server each time. Increasing this value may cause Addax OOM excludeColumn No list None Column name fields to be excluded, only valid when <code>column</code> is configured as <code>*</code>"},{"location":"reader/rdbmsreader/#jdbcurl","title":"jdbcUrl","text":"<p>In addition to configuring necessary information, <code>jdbcUrl</code> configuration can also add specific configuration properties for each specific driver. Here we particularly mention that we can use configuration properties to support proxies to access databases through proxies. For example, for PrestoSQL database JDBC driver, it supports the <code>socksProxy</code> parameter, so the above configured <code>jdbcUrl</code> can be modified to:</p> <p><code>jdbc:presto://127.0.0.1:8080/hive?socksProxy=192.168.1.101:1081</code></p> <p>Most relational database JDBC drivers support <code>socksProxyHost,socksProxyPort</code> parameters for proxy access. There are also some special cases.</p> <p>The following are the proxy types and configuration methods supported by various database JDBC drivers:</p> Database Proxy Type Proxy Configuration Example MySQL socks socksProxyHost,socksProxyPort <code>socksProxyHost=192.168.1.101&amp;socksProxyPort=1081</code> Presto socks socksProxy <code>socksProxy=192.168.1.101:1081</code> Presto http httpProxy <code>httpProxy=192.168.1.101:3128</code>"},{"location":"reader/rdbmsreader/#driver","title":"driver","text":"<p>In most cases, the JDBC driver for a database is fixed, but some have different recommended driver class names due to different versions, such as MySQL. The new MySQL JDBC driver type recommends using <code>com.mysql.cj.jdbc.Driver</code> instead of the previous <code>com.mysql.jdbc.Driver</code>. If you want to use the old driver name, you can configure the <code>driver</code> configuration item. Otherwise, the plugin will automatically guess the driver name based on the string in <code>jdbcUrl</code>.</p>"},{"location":"reader/rdbmsreader/#column","title":"column","text":"<p>Collection of column names to be synchronized in the configured table, using JSON array to describe field information. Users use <code>*</code> to represent default use of all column configurations, such as <code>[\"*\"]</code>.</p> <p>Supports column pruning, i.e., columns can be selected for partial export.</p> <p>Supports column reordering, i.e., columns can be exported not according to table schema information.</p> <p>Supports constant configuration, users need to follow JSON format:</p> <p><code>[\"id\", \"`table`\", \"1\", \"'bazhen.csy'\", \"null\", \"to_char(a + 1)\", \"2.3\" , \"true\"]</code></p> <ul> <li><code>id</code> is ordinary column name</li> <li><code>`table`</code> is column name containing reserved words</li> <li><code>1</code> is integer constant</li> <li><code>'bazhen.csy'</code> is string constant</li> <li><code>null</code> is null pointer. Note that <code>null</code> here must appear as a string, i.e., quoted with double quotes</li> <li><code>to_char(a + 1)</code> is expression</li> <li><code>2.3</code> is floating point number</li> <li><code>true</code> is boolean value. Similarly, boolean values here must also be quoted with double quotes</li> </ul> <p>Column must be explicitly filled and cannot be empty!</p>"},{"location":"reader/rdbmsreader/#excludecolumn","title":"excludeColumn","text":"<p>There is a situation where we need to read most fields of a table. If the table has many fields, configuring <code>column</code> is obviously time-consuming. In particular, when we collect business data to big data platforms, we generally add some additional fields including partition fields and collection information. When we need to write back to business data tables, we need to exclude these fields. Under this consideration, we introduced the <code>excludeColumn</code> configuration item. When <code>column</code> is configured as <code>*</code>, the <code>excludeColumn</code> configuration item takes effect to exclude some fields.</p> <p>For example:</p> <pre><code>{\n  \"column\": [\"*\"],\n  \"excludeColumn\": [\"partition_col\", \"etl_time\"]\n}\n</code></pre>"},{"location":"reader/redisreader/","title":"Redis Reader","text":"<p>Redis Reader plugin is used to read Redis RDB data.</p>"},{"location":"reader/redisreader/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": [\"tcp://127.0.0.1:6379\", \"file:///data/dump.rdb\", \"http://localhost/dump.rdb\"],\n            \"auth\": \"password\"\n          },\n          \"include\": [\n            \"^user\"\n          ],\n          \"exclude\": [\n            \"^password\"\n          ],\n          \"db\": [\n            0,\n            1\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"timeout\": 60000\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/redisreader/#parameters","title":"Parameters","text":"Configuration Required Default Value Description uri Yes No Redis connection, supports multiple local rdb files/network rdb files, if cluster, fill in all master node addresses db No None Database index to read, if not filled, read all databases include No None Keys to include, supports regular expressions exclude No None Keys to exclude, supports regular expressions"},{"location":"reader/redisreader/#constraints","title":"Constraints","text":"<ol> <li>Does not support directly reading any redis server that does not support the <code>sync</code> command. If needed, please read from backed up rdb files.</li> <li>If it's a native redis cluster, please fill in all master node TCP addresses. The <code>redisreader</code> plugin will automatically dump rdb files from all nodes.</li> <li>Only parses <code>String</code> data type, other composite types (<code>Sets</code>, <code>List</code>, etc. will be ignored)</li> </ol>"},{"location":"reader/s3reader/","title":"S3 Reader","text":"<p>S3 Reader plugin is used to read data on Amazon AWS S3 storage. In implementation, this plugin is written based on S3's official SDK 2.0.</p> <p>This plugin also supports reading storage services compatible with S3 protocol, such as MinIO.</p>"},{"location":"reader/s3reader/#configuration-example","title":"Configuration Example","text":"<p>The following sample configuration is used to read two files from S3 storage and print them out</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"s3reader\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": [\n            \"1.csv\",\n            \"aa.csv\",\n            \"upload_*.csv\",\n            \"bb_??.csv\"\n          ],\n          \"column\": [\n            \"*\"\n          ],\n          \"region\": \"ap-northeast-1\",\n          \"fileFormat\": \"csv\",\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/s3reader/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None S3 Server EndPoint address, e.g. <code>s3.xx.amazonaws.com</code> region Yes string None S3 Server Region address, e.g. <code>ap-southeast-1</code> accessId Yes string None Access ID accessKey Yes string None Access Key bucket Yes string None Bucket to read object Yes list None Objects to read, can specify multiple and wildcard patterns, see description below column Yes list None Column information of objects to read, refer to <code>column</code> description in RDBMS Reader fieldDelimiter No string <code>,</code> Field delimiter for reading, only supports single character compress No string None File compression format, default is no compression encoding No string <code>utf8</code> File encoding format writeMode No string <code>nonConflict</code> pathStyleAccessEnabled No boolean false Whether to enable path-style access mode"},{"location":"reader/s3reader/#object","title":"object","text":"<p>When specifying a single object, the plugin can currently only use single-threaded data extraction.</p>"},{"location":"reader/sqlitereader/","title":"SQLite Reader","text":"<p>SQLite Reader plugin is used to read sqlite files in a specified directory. It inherits from RDBMS Reader.</p>"},{"location":"reader/sqlitereader/#example","title":"Example","text":"<p>We create an example file:</p> <pre><code>$ sqlite3  /tmp/test.sqlite3\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter \".help\" for instructions\nEnter SQL statements terminated with a \";\"\nsqlite&gt; create table test(id int, name varchar(10), salary double);\nsqlite&gt; insert into test values(1,'foo', 12.13),(2,'bar',202.22);\nsqlite&gt; .q\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/sqlite2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlitereader\",\n        \"parameter\": {\n          \"username\": \"fakeuser\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite:/tmp/test.sqlite3\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/sqlite2stream.json</code></p>"},{"location":"reader/sqlitereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/sqlite2stream.json\n</code></pre>"},{"location":"reader/sqlitereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/sqlserverreader/","title":"SQLServer Reader","text":"<p>SqlServerReader plugin is used to read data from SQLServer.</p>"},{"location":"reader/sqlserverreader/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to synchronize and extract data from SQLServer database to local:</p> job/sqlserver2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sqlserverreader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"*\"\n          ],\n          \"splitPk\": \"db_id\",\n          \"connection\": {\n            \"table\": [\n              \"table\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://localhost:3433;DatabaseName=dbname\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/sqlserverreader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/streamreader/","title":"Stream Reader","text":"<p>Stream Reader is a plugin that reads data from memory, mainly used to quickly generate expected data and test write plugins.</p> <p>A complete StreamReader configuration file is as follows:</p> <pre><code>{\n  \"reader\": {\n    \"name\": \"streamreader\",\n    \"parameter\": {\n      \"column\": [\n        {\n          \"value\": \"unique_id\",\n          \"type\": \"string\"\n        },\n        {\n          \"value\": \"1989-06-04 08:12:13\",\n          \"type\": \"date\",\n          \"dateFormat\": \"yyyy-MM-dd HH:mm:ss\"\n        },\n        {\n          \"value\": 1984,\n          \"type\": \"long\"\n        },\n        {\n          \"value\": 1989.64,\n          \"type\": \"double\"\n        },\n        {\n          \"value\": true,\n          \"type\": \"bool\"\n        },\n        {\n          \"value\": \"a long text\",\n          \"type\": \"bytes\"\n        }\n      ],\n      \"sliceRecordCount\": 10\n    }\n  }\n}\n</code></pre> <p>The above configuration file will generate 10 records (assuming channel is 1), with each record containing:</p> <p><code>unique_id,'1989-06-04 08:12:13',1984,1989.64,true,'a long text'</code></p> <p>Currently StreamReader supports all output data types listed above:</p> <ul> <li><code>string</code> String type</li> <li><code>date</code> Date type  </li> <li><code>long</code> All integer types</li> <li><code>double</code> All floating point numbers</li> <li><code>bool</code> Boolean type</li> <li><code>bytes</code> Byte type</li> </ul> <p>The <code>date</code> type also supports <code>dateFormat</code> configuration to specify the format of input dates, default is <code>yyyy-MM-dd HH:mm:ss</code>. For example, your input can be like this:</p> <pre><code>{\n  \"value\": \"1989/06/04 12:13:14\",\n  \"type\": \"date\",\n  \"dateFormat\": \"yyyy/MM/dd HH:mm:ss\"\n}\n</code></pre> <p>Note that regardless of the input format for date type, it is internally converted to <code>yyyy-MM-dd HH:mm:ss</code> format.</p> <p>StreamReader also supports random input functionality. For example, to randomly get any integer between 0-10, we can configure the column like this:</p> <pre><code>{\n  \"random\": \"0,10\",\n  \"type\": \"long\"\n}\n</code></pre> <p>To get a random floating point number between 0 and 100, configure like this:</p> <pre><code>{\n  \"random\": \"0,100\",\n  \"type\": \"double\"\n}\n</code></pre> <p>To specify decimal places for floating point numbers, e.g., 2 decimal places, configure like this:</p> <pre><code>{\n  \"random\": \"0,100,2\",\n  \"type\": \"double\"\n}\n</code></pre> <p>Note: It cannot guarantee that the generated decimal always has exactly 2 places. If the decimal part is 0, the decimal places will be fewer than specified.</p> <p>Here we use the <code>random</code> keyword to indicate its value is random, with the range being a closed interval.</p> <p>Other random type configurations are as follows:</p> <ul> <li><code>long</code>: random 0, 10 - random number between 0 and 10</li> <li><code>string</code>: random 0, 10 - random string of length between 0 and 10</li> <li><code>bool</code>: random 0, 10 - ratio of false and true occurrences</li> <li><code>double</code>: random 0, 10 - random floating point between 0 and 10</li> <li><code>double</code>: random 0, 10, 2 - random floating point between 0 and 10 with 2 decimal places</li> <li><code>date</code>: random '2014-07-07 00:00:00', '2016-07-07 00:00:00' - random time between start time and end time, default date format (commas not supported) yyyy-MM-dd HH:mm:ss</li> <li><code>BYTES</code>: random 0, 10 - random string of length between 0 and 10, get its UTF-8 encoded binary string</li> </ul> <p>StreamReader also supports increment functions. For example, to get an arithmetic sequence starting from 1 with increment of 5, configure like this:</p> <pre><code>{\n  \"incr\": \"1,5\",\n  \"type\": \"long\"\n}\n</code></pre> <p>To get a decreasing sequence, change the step size (5 in the above example) to negative. Default step size is 1.</p> <p>Increment also supports date type (introduced in version <code>4.0.1</code>), for example:</p> <pre><code>{\n  \"incr\": \"1989-06-04 09:01:02,2,d\",\n  \"type\": \"date\"\n}\n</code></pre> <p><code>incr</code> consists of three parts: start date, step size, and step unit, separated by English commas (,).</p> <ul> <li>Start date: Correct date string, default format is <code>yyyy-MM-dd hh:mm:ss</code>. If time format is different, need to configure <code>dateFormat</code> to specify date format. This is mandatory.</li> <li>Step size: Length to increase each time, default is 1. For decreasing, fill in negative number. This is optional.</li> <li>Step unit: What time unit to increment/decrement by, default is by day. This is optional. Available units:</li> <li>d/day</li> <li>M/month</li> <li>y/year</li> <li>h/hour</li> <li>m/minute</li> <li>s/second</li> <li>w/week</li> </ul> <p>Configuration item <code>sliceRecordCount</code> specifies the number of data records to generate. If <code>channel</code> is specified, actual generated records = <code>sliceRecordCount * channel</code></p>"},{"location":"reader/sybasereader/","title":"Sybase Reader","text":"<p>SybaseReader plugin implements reading data from Sybase.</p>"},{"location":"reader/sybasereader/#example","title":"Example","text":"<p>We can use Docker container to start a Sybase database</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>The following configuration reads this table to terminal:</p> job/sybasereader.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"sybasereader\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.ijdbc_function_escapes\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": \"true\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/sybase2stream.json</code></p>"},{"location":"reader/sybasereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/sybase2stream.json\n</code></pre>"},{"location":"reader/sybasereader/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Reader, so you can refer to all configuration items of RDBMS Reader.</p>"},{"location":"reader/tdenginereader/","title":"TDengine Reader","text":"<p>TDengine Reader plugin is used to read data from TDengine by TaosData TDengine.</p>"},{"location":"reader/tdenginereader/#prerequisites","title":"Prerequisites","text":"<p>Considering performance issues, this plugin uses TDengine's JDBC-JNI driver, which directly calls the client API (<code>libtaos.so</code> or <code>taos.dll</code>) to send write and query requests to taosd instances. Therefore, dynamic library link files need to be configured before use.</p> <p>First copy <code>plugin/reader/tdenginereader/libs/libtaos.so.2.0.16.0</code> to <code>/usr/lib64</code> directory, then execute the following commands to create soft links:</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"reader/tdenginereader/#example","title":"Example","text":"<p>TDengine comes with a demo database taosdemo. We read some data from the demo database and print to terminal.</p> <p>The following is the configuration file:</p> job/tdengine2stream.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0.02\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"tdenginereader\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"taosdata\",\n          \"beginDateTime\": \"2017-07-14 10:40:00\",\n          \"endDateTime\": \"2017-08-14 10:40:00\",\n          \"splitInterval\": \"1d\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:TAOS://127.0.0.1:6030/test\",\n            \"querySql\": [\n              \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 10\"\n            ]\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"streamwriter\",\n        \"parameter\": {\n          \"print\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/tdengine2stream.json</code></p>"},{"location":"reader/tdenginereader/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/tdengine2stream.json\n</code></pre> <p>Command output is similar to the following:</p> <pre><code>2021-02-20 15:32:23.161 [main] INFO  VMInfo - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-02-20 15:32:23.229 [main] INFO  Engine -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"password\":\"*****\",\n                    \"connection\":[\n                        {\n                            \"querySql\":[\n                                \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n                            ],\n                            \"jdbcUrl\":[\n                                \"jdbc:TAOS://127.0.0.1:6030/test\"\n                            ]\n                        }\n                    ],\n                    \"username\":\"root\"\n                },\n                \"name\":\"tdenginereader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"print\":true\n                },\n                \"name\":\"streamwriter\"\n            }\n    },\n    \"setting\":{\n        \"errorLimit\":{\n            \"record\":0,\n            \"percentage\":0.02\n        },\n        \"speed\":{\n            \"channel\":3\n        }\n    }\n}\n\n2021-02-20 15:32:23.277 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-02-20 15:32:23.278 [main] INFO  JobContainer - Addax jobContainer starts job.\n2021-02-20 15:32:23.281 [main] INFO  JobContainer - Set jobId = 0\njava.library.path:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n....\n2021-02-20 15:32:23.687 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Begin to read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n2021-02-20 15:32:23.692 [0-0-0-reader] WARN  DBUtil - current database does not supoort TYPE_FORWARD_ONLY/CONCUR_READ_ONLY\n2021-02-20 15:32:23.740 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Finished read record by Sql: [select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\n] jdbcUrl:[jdbc:TAOS://127.0.0.1:6030/test].\n\n1500000001000   5   5   0   1   beijing\n1500000001000   0   6   2   1   beijing\n1500000001000   7   0   0   1   beijing\n1500000001000   8   9   6   1   beijing\n1500000001000   9   9   1   1   beijing\n1500000001000   8   2   0   1   beijing\n1500000001000   4   5   5   3   beijing\n1500000001000   3   3   3   3   beijing\n1500000001000   5   4   8   3   beijing\n1500000001000   9   4   6   3   beijing\n\n2021-02-20 15:32:26.689 [job-0] INFO  JobContainer -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-02-20 15:32:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-02-20 15:32:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :              800B/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"reader/tdenginereader/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description jdbcUrl Yes list None JDBC connection information of target database, note that <code>TAOS</code> here must be uppercase username Yes string None Username of data source password No string None Password for specified username of data source table Yes list None Selected table names to be synchronized, using JSON data format. When configured for multiple tables, users need to ensure multiple tables have the same structure column Yes list None Collection of column names to be synchronized in configured table, detailed description rdbmreader where No string None Filtering conditions for the table querySql No list None Use custom SQL instead of specified table to get data. When this item is configured, Addax system will ignore <code>table</code>, <code>column</code> configuration items beginDateTime Yes string None Data start time, Job migrates data from <code>beginDateTime</code> to <code>endDateTime</code>, format is <code>yyyy-MM-dd HH:mm:ss</code> endDateTime Yes string None Data end time, Job migrates data from <code>beginDateTime</code> to <code>endDateTime</code>, format is <code>yyyy-MM-dd HH:mm:ss</code> splitInterval Yes string None Divide <code>task</code> according to <code>splitInterval</code>, create one <code>task</code> per <code>splitInterval</code>"},{"location":"reader/tdenginereader/#splitinterval","title":"splitInterval","text":"<p>Used to divide <code>task</code>. For example, <code>20d</code> represents dividing data into 1 <code>task</code> every 20 days. Configurable time units:</p> <ul> <li><code>d</code> (day)</li> <li><code>h</code> (hour)</li> <li><code>m</code> (minute)</li> <li><code>s</code> (second)</li> </ul>"},{"location":"reader/tdenginereader/#using-jdbc-restful-interface","title":"Using JDBC-RESTful Interface","text":"<p>If you don't want to depend on local libraries or don't have permissions, you can use the <code>JDBC-RESTful</code> interface to write to tables. Compared to JDBC-JNI, the configuration differences are:</p> <ul> <li>driverClass specified as <code>com.taosdata.jdbc.rs.RestfulDriver</code></li> <li>jdbcUrl starts with <code>jdbc:TAOS-RS://</code></li> <li>Use <code>6041</code> as connection port</li> </ul> <p>So the <code>connection</code> in the above configuration should be modified as follows:</p> <pre><code>{\n  \"connection\": [\n    {\n      \"querySql\": [\n        \"select * from test.meters where ts &lt;'2017-07-14 10:40:02' and  loc='beijing' limit 100\"\n      ],\n      \"jdbcUrl\": [\n        \"jdbc:TAOS-RS://127.0.0.1:6041/test\"\n      ],\n      \"driver\": \"com.taosdata.jdbc.rs.RestfulDriver\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reader/tdenginereader/#type-conversion","title":"Type Conversion","text":"Addax Internal Type TDengine Data Type Long SMALLINT, TINYINT, INT, BIGINT, TIMESTAMP Double FLOAT, DOUBLE String BINARY, NCHAR Boolean BOOL"},{"location":"reader/tdenginereader/#currently-supported-versions","title":"Currently Supported Versions","text":"<p>TDengine 2.0.16</p>"},{"location":"reader/tdenginereader/#notes","title":"Notes","text":"<ul> <li>TDengine JDBC-JNI driver and dynamic library versions must match one-to-one. Therefore, if your data version is not <code>2.0.16</code>, you need to replace both the dynamic library and JDBC driver in the plugin directory.</li> </ul>"},{"location":"reader/txtfilereader/","title":"Text File Reader","text":"<p>The TxtFileReader plugin reads data from text files with configurable delimiters and encodings.</p>"},{"location":"reader/txtfilereader/#example","title":"Example","text":"<p>Sample CSV file (<code>/tmp/users.csv</code>):</p> <pre><code>id,name,age,email\n1,John Doe,30,john@example.com\n2,Jane Smith,25,jane@example.com\n3,Bob Johnson,35,bob@example.com\n</code></pre> <p>Configuration to read the CSV file:</p> <pre><code>{\n  \"job\": {\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"txtfilereader\",\n          \"parameter\": {\n            \"path\": \"/tmp/users.csv\",\n            \"encoding\": \"UTF-8\",\n            \"column\": [\n              {\n                \"index\": 0,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 1,\n                \"type\": \"string\"\n              },\n              {\n                \"index\": 2,\n                \"type\": \"long\"\n              },\n              {\n                \"index\": 3,\n                \"type\": \"string\"\n              }\n            ],\n            \"fieldDelimiter\": \",\",\n            \"skipHeader\": true\n          }\n        },\n        \"writer\": {\n          \"name\": \"streamwriter\",\n          \"parameter\": {\n            \"encoding\": \"UTF-8\",\n            \"print\": true\n          }\n        }\n      }\n    ],\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"reader/txtfilereader/#parameters","title":"Parameters","text":""},{"location":"reader/txtfilereader/#required-parameters","title":"Required Parameters","text":"Parameter Description Type Default path File path or directory path string None column Column configuration array None"},{"location":"reader/txtfilereader/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Type Default encoding File encoding string UTF-8 fieldDelimiter Field delimiter string , compress Compression format string None skipHeader Skip first line boolean false nullFormat Null value representation string \\N"},{"location":"reader/txtfilereader/#column-configuration","title":"Column Configuration","text":"<p>Each column in the <code>column</code> array can be configured with:</p> Property Description Type Required index Column index (0-based) integer Yes type Data type string Yes value Constant value string No"},{"location":"reader/txtfilereader/#supported-data-types","title":"Supported Data Types","text":"<ul> <li><code>long</code>: Integer numbers</li> <li><code>double</code>: Floating point numbers  </li> <li><code>string</code>: Text data</li> <li><code>date</code>: Date and time</li> <li><code>bool</code>: Boolean values</li> </ul>"},{"location":"reader/txtfilereader/#path-configuration","title":"Path Configuration","text":""},{"location":"reader/txtfilereader/#single-file","title":"Single File","text":"<pre><code>{\n  \"path\": \"/data/users.csv\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#multiple-files-with-wildcards","title":"Multiple Files with Wildcards","text":"<pre><code>{\n  \"path\": \"/data/users_*.csv\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#directory","title":"Directory","text":"<pre><code>{\n  \"path\": \"/data/csv_files/\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#delimiter-examples","title":"Delimiter Examples","text":""},{"location":"reader/txtfilereader/#tab-separated-values","title":"Tab-separated Values","text":"<pre><code>{\n  \"fieldDelimiter\": \"\\t\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#pipe-separated-values","title":"Pipe-separated Values","text":"<pre><code>{\n  \"fieldDelimiter\": \"|\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#fixed-width-using-spaces","title":"Fixed-width (using spaces)","text":"<pre><code>{\n  \"fieldDelimiter\": \" \"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#compression-support","title":"Compression Support","text":""},{"location":"reader/txtfilereader/#gzip-files","title":"GZIP Files","text":"<pre><code>{\n  \"path\": \"/data/users.csv.gz\",\n  \"compress\": \"gzip\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#bzip2-files","title":"BZIP2 Files","text":"<pre><code>{\n  \"path\": \"/data/users.csv.bz2\",  \n  \"compress\": \"bzip2\"\n}\n</code></pre>"},{"location":"reader/txtfilereader/#advanced-examples","title":"Advanced Examples","text":""},{"location":"reader/txtfilereader/#reading-json-lines-format","title":"Reading JSON Lines Format","text":"<pre><code>{\n  \"reader\": {\n    \"name\": \"txtfilereader\",\n    \"parameter\": {\n      \"path\": \"/data/users.jsonl\",\n      \"encoding\": \"UTF-8\",\n      \"column\": [\n        {\n          \"index\": 0,\n          \"type\": \"string\"\n        }\n      ],\n      \"fieldDelimiter\": \"\\n\"\n    }\n  }\n}\n</code></pre>"},{"location":"reader/txtfilereader/#reading-log-files","title":"Reading Log Files","text":"<pre><code>{\n  \"reader\": {\n    \"name\": \"txtfilereader\", \n    \"parameter\": {\n      \"path\": \"/var/log/app.log\",\n      \"encoding\": \"UTF-8\",\n      \"column\": [\n        {\n          \"index\": 0,\n          \"type\": \"string\"\n        }\n      ],\n      \"fieldDelimiter\": \"\\n\",\n      \"nullFormat\": \"\"\n    }\n  }\n}\n</code></pre>"},{"location":"reader/txtfilereader/#constant-values","title":"Constant Values","text":"<pre><code>{\n  \"column\": [\n    {\n      \"index\": 0,\n      \"type\": \"long\"\n    },\n    {\n      \"index\": 1, \n      \"type\": \"string\"\n    },\n    {\n      \"type\": \"string\",\n      \"value\": \"batch_001\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reader/txtfilereader/#error-handling","title":"Error Handling","text":""},{"location":"reader/txtfilereader/#invalid-data-types","title":"Invalid Data Types","text":"<p>When a field cannot be converted to the specified type, it will be treated as dirty data according to your error handling configuration.</p>"},{"location":"reader/txtfilereader/#missing-files","title":"Missing Files","text":"<p>If specified files don't exist, the job will fail. Use wildcards carefully to ensure at least one file matches.</p>"},{"location":"reader/txtfilereader/#encoding-issues","title":"Encoding Issues","text":"<p>If file encoding doesn't match the specified encoding, character corruption may occur. Always verify the actual file encoding.</p>"},{"location":"reader/txtfilereader/#performance-tips","title":"Performance Tips","text":""},{"location":"reader/txtfilereader/#large-files","title":"Large Files","text":"<p>For very large files, consider splitting them or using multiple channels:</p> <pre><code>{\n  \"setting\": {\n    \"speed\": {\n      \"channel\": 4\n    }\n  }\n}\n</code></pre>"},{"location":"reader/txtfilereader/#memory-usage","title":"Memory Usage","text":"<p>The reader loads data in chunks, so memory usage is generally stable regardless of file size.</p>"},{"location":"reader/txtfilereader/#network-files","title":"Network Files","text":"<p>When reading from network-mounted drives, network latency may affect performance. Consider copying files locally first for better performance.</p>"},{"location":"writer/accesswriter/","title":"Access Writer","text":"<p>Access Writer plugin implements the functionality of writing data to Access destination tables.</p>"},{"location":"writer/accesswriter/#example","title":"Example","text":"<p>Assume the Access table to be written has the following DDL statement:</p> <pre><code>create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\n</code></pre> <p>Here we use data generated from memory to import into Access.</p> job/stream2access.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"accesswriter\",\n        \"parameter\": {\n          \"username\": \"wgzhao\",\n          \"password\": \"\",\n          \"column\": [\n            \"name\",\n            \"file_size\",\n            \"file_date\",\n            \"file_open\",\n            \"memo\"\n          ],\n          \"ddl\": \"create table tbl_test(name varchar(20), file_size int, file_date date, file_open boolean, memo blob);\",\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:ucanaccess:////Users/wgzhao/Downloads/AccessThemeDemo.mdb\",\n            \"table\": [\n              \"tbl_test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2access.json</code></p>"},{"location":"writer/accesswriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2access.json\n</code></pre>"},{"location":"writer/accesswriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/accesswriter/#change-log","title":"Change Log","text":"<ol> <li>From version <code>5.0.1</code>, when the Access database file to be written does not exist, it will be automatically created and the database format will be set to <code>Access 2016</code></li> </ol>"},{"location":"writer/cassandrawriter/","title":"Cassandra Writer","text":"<p>Cassandra Writer plugin is used to write data to Cassandra.</p>"},{"location":"writer/cassandrawriter/#configuration-example","title":"Configuration Example","text":"<p>Configure a job to import data from memory to Cassandra:</p> jobs/stream2cassandra.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"name\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"false\",\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": \"addr\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": 1.234,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 12345678,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 2.345,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 3456789,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"value\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10000000\n        }\n      },\n      \"writer\": {\n        \"name\": \"cassandrawriter\",\n        \"parameter\": {\n          \"host\": \"localhost\",\n          \"port\": 9042,\n          \"useSSL\": false,\n          \"keyspace\": \"stresscql\",\n          \"table\": \"dst\",\n          \"batchSize\": 10,\n          \"column\": [\n            \"name\",\n            \"choice\",\n            \"date\",\n            \"address\",\n            \"dbl\",\n            \"lval\",\n            \"fval\",\n            \"ival\",\n            \"uid\",\n            \"value\",\n            \"listval\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/cassandrawriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description host Yes string None Domain or IP of connection points, multiple nodes separated by commas port Yes int 9042 Cassandra port username No string None Username of data source password No string None Password for specified username of data source useSSL No boolean false Whether to use SSL connection connectionsPerHost No int 8 Client connection pool configuration: how many connections to establish with each server node maxPendingPerConnection No int 128 Client connection pool configuration: maximum requests per connection keyspace Yes string None Keyspace where the table to be synchronized is located table Yes string None Selected table to be synchronized column Yes list None Collection of columns to be synchronized in the configured table consistancyLevel No string <code>LOCAL_QUORUM</code> Data consistency level batchSize No int 1 Number of records in one batch submission (UNLOGGED BATCH)"},{"location":"writer/cassandrawriter/#column","title":"column","text":"<p>Content can be column names or <code>writetime()</code>. If a column name is configured as <code>writetime()</code>, the content of this column will be used as timestamp.</p>"},{"location":"writer/cassandrawriter/#consistancylevel","title":"consistancyLevel","text":"<p>Options: <code>ONE, QUORUM, LOCAL_QUORUM, EACH_QUORUM, ALL, ANY, TWO, THREE, LOCAL_ONE</code></p>"},{"location":"writer/cassandrawriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Cassandra Data Type Long int, tinyint, smallint,varint,bigint,time Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob <p>Please note:</p> <p>Currently does not support <code>counter</code> type and <code>custom</code> type.</p>"},{"location":"writer/cassandrawriter/#constraints","title":"Constraints","text":""},{"location":"writer/cassandrawriter/#batchsize","title":"batchSize","text":"<ol> <li>Cannot exceed 65535</li> <li>The content size in batch is limited by server-side <code>batch_size_fail_threshold_in_kb</code>.</li> <li>If batch content exceeds <code>batch_size_warn_threshold_in_kb</code> limit, warn logs will be printed, but it doesn't affect writing and can be ignored.</li> <li>If batch submission fails, all content in this batch will be rewritten record by record.</li> </ol>"},{"location":"writer/clickhousewriter/","title":"ClickHouse Writer","text":"<p>ClickHouse Writer plugin is used to write data to ClickHouse.</p>"},{"location":"writer/clickhousewriter/#example","title":"Example","text":"<p>The following example demonstrates reading content from one table in ClickHouse and writing it to another table with the same table structure, to test the data structures supported by the plugin.</p>"},{"location":"writer/clickhousewriter/#table-structure-and-data","title":"Table Structure and Data","text":"<p>Assume the table structure and data to be read are as follows:</p> <pre><code>CREATE TABLE ck_addax (\n    c_int8 Int8,\n    c_int16 Int16,\n    c_int32 Int32,\n    c_int64 Int64,\n    c_uint8 UInt8,\n    c_uint16 UInt16,\n    c_uint32 UInt32,\n    c_uint64 UInt64,\n    c_float32 Float32,\n    c_float64 Float64,\n    c_decimal Decimal(38,10),\n    c_string String,\n    c_fixstr FixedString(36),\n    c_uuid UUID,\n    c_date Date,\n    c_datetime DateTime('Asia/Chongqing'),\n    c_datetime64 DateTime64(3, 'Asia/Chongqing'),\n    c_enum Enum('hello' = 1, 'world'=2)\n) ENGINE = MergeTree() ORDER BY (c_int8, c_int16) SETTINGS index_granularity = 8192;\n\ninsert into ck_addax values(\n    127,\n    -32768,\n    2147483647,\n    -9223372036854775808,\n    255,\n    65535,\n    4294967295,\n    18446744073709551615,\n    0.9999998,\n    0.999999999999998,\n    1234567891234567891234567891.1234567891,\n    'Hello String',\n    '2c:16:db:a3:3a:4f',\n    '5F042A36-5B0C-4F71-ADFD-4DF4FCA1B863',\n    '2021-01-01',\n    '2021-01-01 11:22:33',\n    '2021-01-01 10:33:23.123',\n    'hello'\n);\n</code></pre> <p>The table to be written uses the same structure as the read table, with the following DDL statement:</p> <pre><code>create table ck_addax_writer as ck_addax;\n</code></pre>"},{"location":"writer/clickhousewriter/#configuration","title":"Configuration","text":"<p>The following is the configuration file</p> job/clickhouse2clickhouse.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"clickhousewriter\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"ck_addax_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/default\"\n          },\n          \"preSql\": [\n            \"alter table @table delete where 1=1\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"clickhousereader\",\n        \"parameter\": {\n          \"username\": \"default\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:clickhouse://127.0.0.1:8123/\",\n            \"table\": [\n              \"ck_addax\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/clickhouse2clickhouse.json</code></p>"},{"location":"writer/clickhousewriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/clickhouse2clickhouse.json\n</code></pre>"},{"location":"writer/clickhousewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/databendwriter/","title":"DatabendWriter","text":"<p>Databend plugin is used to write data to Databend database via JDBC.</p> <p>Databend is a database backend compatible with MySQL protocol, so Databend writing can use MySQLWriter for access.</p>"},{"location":"writer/databendwriter/#example","title":"Example","text":"<p>Assume the table to be written has the following DDL statement:</p> <pre><code>CREATE DATABASE example_db;\nCREATE TABLE `example_db`.`table1`\n(\n    `siteid`   INT DEFAULT CAST(10 AS INT),\n    `citycode` INT,\n    `username` VARCHAR,\n    `pv`       BIGINT\n);\n</code></pre> <p>The following configures a configuration file to read data from memory and write to databend table:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"writer\": {\n        \"name\": \"databendwriter\",\n        \"parameter\": {\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"postSql\": [],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:databend://localhost:8000/addax\",\n            \"table\": [\n              \"table1\"\n            ]\n          },\n          \"username\": \"u1\",\n          \"password\": \"123\",\n          \"column\": [\n            \"*\"\n          ]\n        }\n      },\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2databend.json</code></p> <p>Execute the following command:</p> <pre><code>bin/addax.sh job/stream2Databend.json\n</code></pre>"},{"location":"writer/databendwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds the following configuration items:</p> Configuration Required Type Default Value Description writeMode No string <code>insert</code> Write mode, supports <code>insert</code> and <code>replace</code> modes onConflictColumn No string None Conflict column, when writeMode is <code>replace</code>, must specify conflict column, otherwise write will fail"},{"location":"writer/databendwriter/#writemode","title":"writeMode","text":"<p>Used to support Databend's <code>replace into</code> syntax. When this parameter is set to <code>replace</code>, the <code>onConflictColumn</code> parameter must also be specified to determine whether data is inserted or updated.</p> <p>Example of both parameters:</p> <pre><code>{\n  \"writeMode\": \"replace\",\n  \"onConflictColumn\": [\n    \"id\"\n  ]\n}\n</code></pre>"},{"location":"writer/dbfwriter/","title":"DBF Writer","text":"<p>DBF Writer provides writing DBF-like format to one or more table files in local file system.</p>"},{"location":"writer/dbfwriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"\u4e2d\u6587\u6d4b\u8bd5\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"dbfwriter\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"name\": \"col1\",\n              \"type\": \"char\",\n              \"length\": 100\n            },\n            {\n              \"name\": \"col2\",\n              \"type\": \"numeric\",\n              \"length\": 18,\n              \"scale\": 0\n            },\n            {\n              \"name\": \"col3\",\n              \"type\": \"date\"\n            },\n            {\n              \"name\": \"col4\",\n              \"type\": \"logical\"\n            },\n            {\n              \"name\": \"col5\",\n              \"type\": \"char\",\n              \"length\": 100\n            }\n          ],\n          \"fileName\": \"test.dbf\",\n          \"path\": \"/tmp/out\",\n          \"writeMode\": \"truncate\",\n          \"encoding\": \"GBK\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/dbfwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File directory, note this is a folder, not a file column Yes <code>list&lt;map&gt;</code> None Collection of columns to be synchronized in configured table, see example configuration fileName Yes string None Name of file to write writeMode Yes string None Data cleanup processing mode before writing, see description below encoding No string UTF-8 File encoding, such as <code>GBK</code>, <code>UTF-8</code> nullFormat No string <code>\\N</code> Define which string can represent null dateFormat No string None Format when date type data is serialized to file, e.g. <code>\"yyyy-MM-dd\"</code>"},{"location":"writer/dbfwriter/#writemode","title":"writeMode","text":"<p>Data cleanup processing mode before writing:</p> <ul> <li>truncate: Clean all files with <code>fileName</code> prefix under directory before writing</li> <li>append: No processing before writing, write directly using <code>filename</code> and ensure no filename conflicts</li> <li>nonConflict: If there are files with <code>fileName</code> prefix under directory, report error directly</li> </ul>"},{"location":"writer/dbfwriter/#type-conversion","title":"Type Conversion","text":"<p>Currently this plugin supports the following write types and corresponding relationships:</p> XBase Type XBase Symbol Java Type used in JavaDBF Character C java.lang.String Numeric N java.math.BigDecimal Floating Point F java.math.BigDecimal Logical L java.lang.Boolean"},{"location":"writer/doriswriter/","title":"Doris Writer","text":"<p>Doris Writer plugin implements writing data to Apache Doris.</p>"},{"location":"writer/doriswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Doris database. For detailed configuration and parameters, please refer to the original Doris Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"doriswriter\",\n        \"parameter\": {\n          \"loadUrl\": [\n            \"127.0.0.1:8030\"\n          ],\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"batchSize\": 1024,\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"connection\": {\n            \"table\": \"table1\",\n            \"database\": \"example_db\",\n            \"jdbcUrl\": \"jdbc:mysql://localhost:9030/example_db\"\n          },\n          \"loadProps\": {\n            \"format\": \"json\",\n            \"strip_outer_array\": true\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/doriswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Doris with Stream Load capabilities and configurable connection options.</p>"},{"location":"writer/elasticsearchwriter/","title":"ElasticSearch Writer","text":"<p>ElasticSearch Writer plugin is used to write data to ElasticSearch. It is implemented through elasticsearch's rest api interface, writing data to elasticsearch in batches.</p>"},{"location":"writer/elasticsearchwriter/#configuration-example","title":"Configuration Example","text":"job/stream2es.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1.1.1.1\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"double\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"hello world\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"long text\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"41.12,-71.34\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"2017-05-25 11:22:33\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"elasticsearchwriter\",\n        \"parameter\": {\n          \"endpoint\": \"http://localhost:9200\",\n          \"index\": \"test-1\",\n          \"type\": \"default\",\n          \"cleanup\": true,\n          \"settings\": {\n            \"index\": {\n              \"number_of_shards\": 1,\n              \"number_of_replicas\": 0\n            }\n          },\n          \"discovery\": false,\n          \"batchSize\": 1000,\n          \"splitter\": \",\",\n          \"column\": [\n            {\n              \"name\": \"pk\",\n              \"type\": \"id\"\n            },\n            {\n              \"name\": \"col_ip\",\n              \"type\": \"ip\"\n            },\n            {\n              \"name\": \"col_double\",\n              \"type\": \"double\"\n            },\n            {\n              \"name\": \"col_long\",\n              \"type\": \"long\"\n            },\n            {\n              \"name\": \"col_integer\",\n              \"type\": \"integer\"\n            },\n            {\n              \"name\": \"col_keyword\",\n              \"type\": \"keyword\"\n            },\n            {\n              \"name\": \"col_text\",\n              \"type\": \"text\",\n              \"analyzer\": \"ik_max_word\"\n            },\n            {\n              \"name\": \"col_geo_point\",\n              \"type\": \"geo_point\"\n            },\n            {\n              \"name\": \"col_date\",\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd HH:mm:ss\"\n            },\n            {\n              \"name\": \"col_nested1\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_nested2\",\n              \"type\": \"nested\"\n            },\n            {\n              \"name\": \"col_object1\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_object2\",\n              \"type\": \"object\"\n            },\n            {\n              \"name\": \"col_integer_array\",\n              \"type\": \"integer\",\n              \"array\": true\n            },\n            {\n              \"name\": \"col_geo_shape\",\n              \"type\": \"geo_shape\",\n              \"tree\": \"quadtree\",\n              \"precision\": \"10m\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/elasticsearchwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None ElasticSearch connection address, if cluster, multiple addresses separated by comma (,) accessId No string Empty User in http auth, default is empty accessKey No string Empty Password in http auth index Yes string None Index name type No string <code>default</code> Index type cleanup No boolean false Whether to delete original table batchSize No int 1000 Number of records in each batch trySize No int 30 Number of retries after failure timeout No int 600000 Client timeout in milliseconds (ms) discovery No boolean false Enable node discovery (polling) and periodically update server list in client compression No boolean true Whether to enable http request compression multiThread No boolean true Whether to enable multi-threaded http requests ignoreWriteError No boolean false Whether to retry on write error, if <code>true</code> means always retry, otherwise ignore the record ignoreParseError No boolean true Whether to continue writing when data format parsing error occurs alias No string None Alias to write after data import is completed aliasMode No string append Mode for adding alias after data import completion, append (add mode), exclusive (keep only this one) settings No map None Settings when creating index, same as elasticsearch official splitter No string <code>,</code> If inserted data is array, use specified delimiter column Yes <code>list&lt;map&gt;</code> None Field types, the example in the document includes all supported field types dynamic No boolean false Don't use addax mappings, use es's own automatic mappings"},{"location":"writer/elasticsearchwriter/#constraints","title":"Constraints","text":"<ul> <li>If importing id, data import failures will also retry, re-import will only overwrite, ensuring data consistency</li> <li>If not importing id, it's append_only mode, elasticsearch automatically generates id, speed will improve about 20%, but data cannot be repaired, suitable for log-type data (low precision requirements)</li> </ul>"},{"location":"writer/excelwriter/","title":"Excel Writer","text":"<p>Excel Writer implements the functionality of writing data to Excel files.</p>"},{"location":"writer/excelwriter/#configuration-example","title":"Configuration Example","text":"<p>We assume reading data from memory and writing to Excel file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"DataX\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"excelwriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/out\",\n          \"fileName\": \"test\",\n          \"header\": [\n            \"str\",\n            \"\u957f\u5ea6\",\n            \"\u65e5\u671f\",\n            \"\u662f\u5426\u4e3a\u771f\",\n            \"\u5b57\u8282\u7c7b\u578b\"\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above content as <code>job/stream2excel.json</code></p> <p>Execute the following command:</p> <pre><code>bin/addax.sh job/stream2excel.sh\n</code></pre> <p>You should get output similar to the following:</p> Click to expand <pre><code>  ___      _     _\n / _ \\    | |   | |\n/ /_\\ \\ __| | __| | __ ___  __\n|  _  |/ _` |/ _` |/ _` \\ \\/ /\n| | | | (_| | (_| | (_| |&gt;  &lt;\n\\_| |_/\\__,_|\\__,_|\\__,_/_/\\_\\\n\n:: Addax version ::    (v4.0.3-SNAPSHOT)\n\n2021-09-10 22:16:38.247 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2021-09-10 22:16:38.269 [        main] INFO  Engine               -\n{\n    \"content\":\n        {\n            \"reader\":{\n                \"parameter\":{\n                    \"column\":[\n                        {\n                            \"type\":\"string\",\n                            \"value\":\"DataX\"\n                        },\n                        {\n                            \"type\":\"long\",\n                            \"value\":19890604\n                        },\n                        {\n                            \"type\":\"date\",\n                            \"value\":\"1989-06-04 11:22:33\"\n                        },\n                        {\n                            \"type\":\"bool\",\n                            \"value\":true\n                        },\n                        {\n                            \"type\":\"bytes\",\n                            \"value\":\"test\"\n                        }\n                    ],\n                    \"sliceRecordCount\":1000\n                },\n                \"name\":\"streamreader\"\n            },\n            \"writer\":{\n                \"parameter\":{\n                    \"path\":\"/tmp/out\",\n                    \"fileName\":\"test\",\n                    \"header\":[\n                        \"str\",\n                        \"\u957f\u5ea6\",\n                        \"\u65e5\u671f\",\n                        \"\u662f\u5426\u4e3a\u771f\",\n                        \"\u5b57\u8282\u7c7b\u578b\"\n                    ],\n                    \"writeMode\":\"truncate\"\n                },\n                \"name\":\"excelwriter\"\n            }\n        },\n    \"setting\":{\n        \"speed\":{\n            \"byte\":-1,\n            \"channel\":1\n        }\n    }\n}\n\n2021-09-10 22:16:38.287 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2021-09-10 22:16:38.287 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2021-09-10 22:16:38.289 [        main] INFO  JobContainer         - Set jobId = 0\n2021-09-10 22:16:38.303 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do prepare work .\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channels.\n2021-09-10 22:16:38.304 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2021-09-10 22:16:38.305 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] splits to [1] tasks.\n2021-09-10 22:16:38.325 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2021-09-10 22:16:38.332 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2021-09-10 22:16:38.335 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2021-09-10 22:16:38.336 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2021-09-10 22:16:41.345 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Writer.Job [excelwriter] do post work.\n2021-09-10 22:16:41.346 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2021-09-10 22:16:41.348 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2021-09-10 22:16:41.349 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 1000 records, 26000 bytes | Speed 8.46KB/s, 333 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.528s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2021-09-10 22:16:41.350 [       job-0] INFO  JobContainer         -\n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2021-09-10 22:16:38\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2021-09-10 22:16:41\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            8.46KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :            333rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                1000\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre>"},{"location":"writer/excelwriter/#parameters","title":"Parameters","text":"Configuration Required Type Default Value Description path Yes string None Specify the directory to save files, create if directory doesn't exist fileName Yes string None Excel filename to generate, detailed description below header No list None Excel header"},{"location":"writer/excelwriter/#filename","title":"fileName","text":"<p>For detailed fileName configuration, please refer to the original Excel Writer documentation.</p>"},{"location":"writer/ftpwriter/","title":"FTP Writer","text":"<p>FTP Writer provides the ability to write files to remote FTP/SFTP servers, currently only supports writing text files.</p>"},{"location":"writer/ftpwriter/#configuration-example","title":"Configuration Example","text":"job/stream2ftp.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"ftpwriter\",\n        \"parameter\": {\n          \"protocol\": \"sftp\",\n          \"host\": \"***\",\n          \"port\": 22,\n          \"username\": \"xxx\",\n          \"password\": \"xxx\",\n          \"timeout\": \"60000\",\n          \"connectPattern\": \"PASV\",\n          \"path\": \"/tmp/data/\",\n          \"fileName\": \"test\",\n          \"writeMode\": \"truncate|append|nonConflict\",\n          \"fieldDelimiter\": \",\",\n          \"encoding\": \"UTF-8\",\n          \"nullFormat\": \"null\",\n          \"dateFormat\": \"yyyy-MM-dd\",\n          \"fileFormat\": \"csv\",\n          \"useKey\": false,\n          \"keyPath\": \"\",\n          \"keyPass\": \"\",\n          \"header\": []\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/ftpwriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description protocol Yes string <code>ftp</code> Server protocol, currently supports ftp and sftp transport protocols host Yes string None Server address port No int 22/21 FTP default is 21, SFTP default is 22 timeout No int <code>60000</code> Connection timeout for FTP server, in milliseconds (ms) connectPattern No string <code>PASV</code> Connection mode, only supports <code>PORT</code>, <code>PASV</code> modes. Used for FTP protocol username Yes string None Username password Yes string None Access password useKey No boolean false Whether to use private key login, only valid for SFTP login keyPath No string <code>~/.ssh/id_rsa</code> Private key address keyPass No string None Private key password, no need to configure if no private key password is set path Yes string None Remote FTP file system path information, FtpWriter will write multiple files under Path directory fileName Yes string None Name of file to write, this filename will have random suffix added as actual filename for each thread writeMode Yes string None Data cleanup processing mode before writing, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading"},{"location":"writer/greenplumwriter/","title":"GreenPlum Writer","text":"<p>GreenPlum Writer plugin implements writing data to GreenPlum database.</p>"},{"location":"writer/greenplumwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to GreenPlum database. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/hanawriter/","title":"HANA Writer","text":"<p>HANA Writer plugin implements the functionality of writing data to SAP HANA destination tables.</p>"},{"location":"writer/hanawriter/#example","title":"Example","text":"<p>Assume the HANA table to be written has the following DDL statement:</p> <pre><code>create table system.addax_tbl\n(\ncol1 varchar(200) ,\ncol2 int(4),\ncol3 date,\ncol4 boolean,\ncol5 clob\n);\n</code></pre> <p>Here we use data generated from memory to import into HANA.</p> job/hanawriter.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"hanawriter\",\n        \"parameter\": {\n          \"username\": \"system\",\n          \"password\": \"HXEHana1\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sap://wgzhao-pc:39017/system\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/hana2stream.json</code></p>"},{"location":"writer/hanawriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/hana2stream.json\n</code></pre>"},{"location":"writer/hanawriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/hbase11xsqlwriter/","title":"HBase11x SQL Writer","text":"<p>HBase11x SQL Writer plugin implements writing data to HBase 1.x via Phoenix SQL interface.</p>"},{"location":"writer/hbase11xsqlwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 1.x via Phoenix. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p> <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase11xsqlwriter\",\n        \"parameter\": {\n          \"batchSize\": \"256\",\n          \"column\": [\n            \"UID\",\n            \"TS\",\n            \"EVENTID\",\n            \"CONTENT\"\n          ],\n          \"haveKerberos\": \"true\",\n          \"kerberosPrincipal\": \"hive@EXAMPLE.COM\",\n          \"kerberosKeytabFilePath\": \"/tmp/hive.headless.keytab\",\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"node1,node2,node3:2181\",\n            \"zookeeper.znode.parent\": \"/hbase-secure\"\n          },\n          \"nullMode\": \"skip\",\n          \"table\": \"TEST_TBL\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/","title":"HBase11x Writer","text":"<p>HBase11x Writer plugin implements writing data to HBase 1.x version.</p>"},{"location":"writer/hbase11xwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 1.x database. For detailed configuration and parameters, please refer to the original HBase11x Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase11xwriter\",\n        \"parameter\": {\n          \"hbaseConfig\": {\n            \"hbase.zookeeper.quorum\": \"***\"\n          },\n          \"table\": \"writer\",\n          \"mode\": \"normal\",\n          \"rowkeyColumn\": [\n            {\n              \"index\": 0,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": -1,\n              \"type\": \"string\",\n              \"value\": \"_\"\n            }\n          ],\n          \"column\": [\n            {\n              \"index\": 1,\n              \"name\": \"cf1:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"name\": \"cf1:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"name\": \"cf1:q3\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"name\": \"cf2:q1\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 5,\n              \"name\": \"cf2:q2\",\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 6,\n              \"name\": \"cf2:q3\",\n              \"type\": \"string\"\n            }\n          ],\n          \"versionColumn\": {\n            \"index\": -1,\n            \"value\": \"123456789\"\n          },\n          \"encoding\": \"utf-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hbase11xwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to HBase 1.x with configurable connection, table, and row key options.</p>"},{"location":"writer/hbase20xsqlwriter/","title":"HBase20x SQL Writer","text":"<p>HBase20x SQL Writer plugin implements writing data to HBase 2.x via Phoenix SQL interface.</p>"},{"location":"writer/hbase20xsqlwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to HBase 2.x via Phoenix. This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p> <pre><code>{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": \"/tmp/normal.txt\",\n          \"charset\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"String\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"hbase20xsqlwriter\",\n        \"parameter\": {\n          \"batchSize\": \"100\",\n          \"column\": [\n            \"UID\",\n            \"TS\",\n            \"EVENTID\",\n            \"CONTENT\"\n          ],\n          \"queryServerAddress\": \"http://127.0.0.1:8765\",\n          \"nullMode\": \"skip\",\n          \"table\": \"TEST_TBL\"\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 5,\n        \"bytes\": -1\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hdfswriter/","title":"HDFS Writer","text":"<p>HDFS Writer provides the ability to write files in formats like <code>TextFile</code>, <code>ORCFile</code>, <code>Parquet</code> etc. to specified paths in HDFS file system. File content can be associated with tables in Hive.</p>"},{"location":"writer/hdfswriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 00:00:00\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            },\n            {\n              \"value\": \"['tag1', 'tag2', 'tag3']\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": \"{'loc':'HZ','num':'12'}\",\n              \"type\": \"string\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        },\n        \"writer\": {\n          \"name\": \"hdfswriter\",\n          \"parameter\": {\n            \"defaultFS\": \"hdfs://xxx:port\",\n            \"fileType\": \"orc\",\n            \"path\": \"/user/hive/warehouse/writerorc.db/orcfull\",\n            \"fileName\": \"xxxx\",\n            \"column\": [\n              {\n                \"name\": \"col1\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col2\",\n                \"type\": \"int\"\n              },\n              {\n                \"name\": \"col3\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col4\",\n                \"type\": \"boolean\"\n              },\n              {\n                \"name\": \"col5\",\n                \"type\": \"string\"\n              },\n              {\n                \"name\": \"col6\",\n                \"type\": \"array&lt;string&gt;\"\n              },\n              {\n                \"name\": \"col7\",\n                \"type\": \"map&lt;string,string&gt;\"\n              }\n            ],\n            \"writeMode\": \"overwrite\",\n            \"fieldDelimiter\": \"\\u0001\",\n            \"compress\": \"SNAPPY\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/hdfswriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None File path to read defaultFS Yes string None Detailed description below fileType Yes string None File type, detailed description below fileName Yes string None Filename to write, used as prefix column Yes <code>list&lt;map&gt;</code> None List of fields to write writeMode Yes string None Write mode, detailed description below skipTrash No boolean false Whether to skip trash, related to <code>writeMode</code> configuration fieldDelimiter No string <code>,</code> Field delimiter for text files, not needed for binary files encoding No string <code>utf-8</code> File encoding configuration, currently only supports <code>utf-8</code> nullFormat No string None Define characters representing null, e.g. if user configures <code>\"\\\\N\"</code>, then if source data is <code>\"\\N\"</code>, treat as <code>null</code> field haveKerberos No boolean false Whether to enable Kerberos authentication, if enabled, need to configure the following two items kerberosKeytabFilePath No string None Credential file path for Kerberos authentication, e.g. <code>/your/path/addax.service.keytab</code> kerberosPrincipal No string None Credential principal for Kerberos authentication, e.g. <code>addax/node1@WGZHAO.COM</code> compress No string None File compression format, see below hadoopConfig No map None Can configure some advanced parameters related to Hadoop, such as HA configuration preShell No <code>list</code> None Shell commands to execute before writing data, e.g. <code>hive -e \"truncate table test.hello\"</code>"},{"location":"writer/icebergwriter/","title":"Iceberg Writer","text":"<p>Iceberg Writer plugin implements writing data to Apache Iceberg.</p>"},{"location":"writer/icebergwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Iceberg tables. For detailed configuration and parameters, please refer to the original Iceberg Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"streamreader\",\n          \"parameter\": {\n            \"column\": [\n              {\n                \"value\": \"1\",\n                \"type\": \"long\"\n              },\n              {\n                \"value\": \"1989-06-04 00:00:00\",\n                \"type\": \"timestamp\"\n              },\n              {\n                \"value\": \"test1\",\n                \"type\": \"string\"\n              }\n            ],\n            \"sliceRecordCount\": 1000\n          }\n        },\n        \"writer\": {\n          \"name\": \"icebergwriter\",\n          \"parameter\": {\n            \"tableName\": \"test.test1\",\n            \"writeMode\": \"truncate\",\n            \"catalogType\": \"hadoop\",\n            \"warehouse\": \"s3a://pvc-91d1e2cd-4d25-45c9-8613-6c4f7bf0a4cc/iceberg\",\n            \"hadoopConfig\": {\n              \"fs.s3a.endpoint\": \"http://localhost:9000\",\n              \"fs.s3a.access.key\": \"gy0dX5lALP176g6c9fYf\",\n              \"fs.s3a.secret.key\": \"ReuUrCzzu5wKWAegtswoHIWV389BYl9AB1ZQbiKr\",\n              \"fs.s3a.connection.ssl.enabled\": \"false\",\n              \"fs.s3a.path.style.access\": \"true\",\n              \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/icebergwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Iceberg with configurable catalog, table, and partition options.</p>"},{"location":"writer/influxdb2writer/","title":"InfluxDB2 Writer","text":"<p>InfluxDB2 Writer plugin implements writing data to InfluxDB 2.0 and above versions.</p>"},{"location":"writer/influxdb2writer/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to InfluxDB 2.0+ database. For detailed configuration and parameters, please refer to the original InfluxDB2 Writer documentation.</p>"},{"location":"writer/influxdb2writer/#parameters","title":"Parameters","text":"<p>This plugin supports writing time series data to InfluxDB 2.0+ with token-based authentication and organization/bucket structure.</p>"},{"location":"writer/influxdbwriter/","title":"InfluxDB Writer","text":"<p>InfluxDB Writer plugin implements writing data to InfluxDB time series database.</p>"},{"location":"writer/influxdbwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to InfluxDB database. For detailed configuration and parameters, please refer to the original InfluxDB Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"2001-01-01 00:00:00, 2016-07-07 23:59:59\",\n              \"type\": \"date\"\n            },\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"influxdbwriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"endpoint\": \"http://localhost:8086\",\n            \"database\": \"addax\",\n            \"table\": \"addax_tbl\"\n          },\n          \"connTimeout\": 15,\n          \"readTimeout\": 20,\n          \"writeTimeout\": 20,\n          \"username\": \"influx\",\n          \"password\": \"influx123\",\n          \"column\": [\n            {\n              \"name\": \"time\",\n              \"type\": \"timestamp\"\n            },\n            {\n              \"name\": \"user_id\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"user_name\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"salary\",\n              \"type\": \"double\"\n            }\n          ],\n          \"preSql\": [\n            \"delete from addax_tbl\"\n          ],\n          \"batchSize\": 1024,\n          \"retentionPolicy\": {\n            \"name\": \"one_day_only\",\n            \"duration\": \"1d\",\n            \"replication\": 1\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/influxdbwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing time series data to InfluxDB with configurable database, measurement, and field options.</p>"},{"location":"writer/kafkawriter/","title":"Kafka Writer","text":"<p>Kafka Writer plugin implements the functionality of writing data to Kafka in JSON format.</p>"},{"location":"writer/kafkawriter/#example","title":"Example","text":"<p>The following configuration demonstrates how to read data from memory and write to specified topic in Kafka.</p>"},{"location":"writer/kafkawriter/#create-task-file","title":"Create Task File","text":"<p>First create a task file <code>stream2kafka.json</code> with the following content:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n        \"speed\": {\n            \"channel\": 1\n        }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n            \"name\": \"streamreader\",\n            \"parameter\": {\n              \"column\": [\n                    {\"random\": \"10,1000\", \"type\": \"long\"},\n                    {\"value\": \"1.1.1.1\", \"type\": \"string\"},\n                    {\"value\": 19890604.0, \"type\": \"double\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": 19890604, \"type\": \"long\"},\n                    {\"value\": \"hello world\", \"type\": \"string\"},\n                    {\"value\": \"long text\", \"type\": \"string\"},\n                    {\"value\": \"41.12,-71.34\", \"type\": \"string\"},\n                    {\"value\": \"2017-05-25 11:22:33\", \"type\": \"string\"}\n                    ],\n            \"sliceRecordCount\": 100\n            }\n        },\n        \"writer\": {\n          \"name\": \"kafkawriter\",\n          \"parameter\": {\n            \"brokerList\": \"localhost:9092\",\n            \"topic\": \"test-1\",\n            \"partitions\": 0,\n            \"batchSize\": 1000,\n            \"column\": [\"col1\", \"col2\",\"col3\",\"col4\",\"col5\", \"col6\", \"col7\", \"col8\", \"col9\"]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/kafkawriter/#run","title":"Run","text":"<p>Execute <code>bin/addax.sh stream2kafka.json</code> command to get output similar to the following:</p> <pre><code>2022-02-26 21:59:22.975 [        main] INFO  VMInfo               - VMInfo# operatingSystem class =&gt; sun.management.OperatingSystemImpl\n2022-02-26 21:59:22.985 [        main] INFO  Engine               - \n{\n    \"content\":{\n        \"reader\":{\n            \"parameter\":{\n                \"column\":[\n                    {\n                        \"random\":\"10,1000\",\n                        \"type\":\"long\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"1.1.1.1\"\n                    },\n                    {\n                        \"type\":\"double\",\n                        \"value\":19890604.0\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"long\",\n                        \"value\":19890604\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"hello world\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"long text\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"41.12,-71.34\"\n                    },\n                    {\n                        \"type\":\"string\",\n                        \"value\":\"2017-05-25 11:22:33\"\n                    }\n                ],\n                \"sliceRecordCount\":100\n            },\n            \"name\":\"streamreader\"\n        },\n        \"writer\":{\n            \"parameter\":{\n                \"partitions\":0,\n                \"column\":[\n                    \"col1\",\n                    \"col2\",\n                    \"col3\",\n                    \"col4\",\n                    \"col5\",\n                    \"col6\",\n                    \"col7\",\n                    \"col8\",\n                    \"col9\"\n                ],\n                \"topic\":\"test-1\",\n                \"batchSize\":1000,\n                \"brokerList\":\"localhost:9092\"\n            },\n            \"name\":\"kafkawriter\"\n        }\n    },\n    \"setting\":{\n        \"speed\":{\n            \"channel\":1\n        }\n    }\n}\n\n2022-02-26 21:59:23.002 [        main] INFO  PerfTrace            - PerfTrace traceId=job_-1, isEnable=false, priority=0\n2022-02-26 21:59:23.003 [        main] INFO  JobContainer         - Addax jobContainer starts job.\n2022-02-26 21:59:23.004 [        main] INFO  JobContainer         - Set jobId = 0\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do prepare work .\n2022-02-26 21:59:23.017 [       job-0] INFO  JobContainer         - Job set Channel-Number to 1 channel(s).\n2022-02-26 21:59:23.018 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] splits to [1] tasks.\n2022-02-26 21:59:23.019 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] splits to [1] tasks.\n2022-02-26 21:59:23.039 [       job-0] INFO  JobContainer         - Scheduler starts [1] taskGroups.\n2022-02-26 21:59:23.047 [ taskGroup-0] INFO  TaskGroupContainer   - taskGroupId=[0] start [1] channels for [1] tasks.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set byte_speed_limit to -1, No bps activated.\n2022-02-26 21:59:23.050 [ taskGroup-0] INFO  Channel              - Channel set record_speed_limit to -1, No tps activated.\n2022-02-26 21:59:23.082 [0-0-0-writer] INFO  ProducerConfig       - ProducerConfig values: \n    acks = 1\n    batch.size = 1000\n    bootstrap.servers = [localhost:9092]\n    buffer.memory = 33554432\n    client.id = addax-kafka-writer\n    compression.type = none\n    connections.max.idle.ms = 540000\n    enable.idempotence = false\n    interceptor.classes = []\n    key.serializer = class org.apache.kafka.common.serialization.StringSerializer\n    linger.ms = 0\n    max.block.ms = 60000\n    max.in.flight.requests.per.connection = 5\n    max.request.size = 1048576\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n    partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\n    receive.buffer.bytes = 32768\n    reconnect.backoff.max.ms = 1000\n    reconnect.backoff.ms = 50\n    request.timeout.ms = 30000\n    retries = 0\n    retry.backoff.ms = 100\n    sasl.client.callback.handler.class = null\n    sasl.jaas.config = null\n    sasl.kerberos.kinit.cmd = /usr/bin/kinit\n    sasl.kerberos.min.time.before.relogin = 60000\n    sasl.kerberos.service.name = null\n    sasl.kerberos.ticket.renew.jitter = 0.05\n    sasl.kerberos.ticket.renew.window.factor = 0.8\n    sasl.login.callback.handler.class = null\n    sasl.login.class = null\n    sasl.login.refresh.buffer.seconds = 300\n    sasl.login.refresh.min.period.seconds = 60\n    sasl.login.refresh.window.factor = 0.8\n    sasl.login.refresh.window.jitter = 0.05\n    sasl.mechanism = GSSAPI\n    security.protocol = PLAINTEXT\n    send.buffer.bytes = 131072\n    ssl.cipher.suites = null\n    ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\n    ssl.endpoint.identification.algorithm = https\n    ssl.key.password = null\n    ssl.keymanager.algorithm = SunX509\n    ssl.keystore.location = null\n    ssl.keystore.password = null\n    ssl.keystore.type = JKS\n    ssl.protocol = TLS\n    ssl.provider = null\n    ssl.secure.random.implementation = null\n    ssl.trustmanager.algorithm = PKIX\n    ssl.truststore.location = null\n    ssl.truststore.password = null\n    ssl.truststore.type = JKS\n    transaction.timeout.ms = 60000\n    transactional.id = null\n    value.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\n2022-02-26 21:59:23.412 [0-0-0-writer] INFO  AppInfoParser        - Kafka version : 2.0.0\n2022-02-26 21:59:23.413 [0-0-0-writer] INFO  AppInfoParser        - Kafka commitId : 3402a8361b734732\n2022-02-26 21:59:23.534 [kafka-producer-network-thread | addax-kafka-writer] INFO  Metadata             - Cluster ID: xPAQZFNDTp6y63nZO4LACA\n2022-02-26 21:59:26.061 [       job-0] INFO  AbstractScheduler    - Scheduler accomplished all tasks.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Writer.Job [kafkawriter] do post work.\n2022-02-26 21:59:26.062 [       job-0] INFO  JobContainer         - Addax Reader.Job [streamreader] do post work.\n2022-02-26 21:59:26.063 [       job-0] INFO  JobContainer         - PerfTrace not enable!\n2022-02-26 21:59:26.064 [       job-0] INFO  StandAloneJobContainerCommunicator - Total 100 records, 9200 bytes | Speed 2.99KB/s, 33 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%\n2022-02-26 21:59:26.065 [       job-0] INFO  JobContainer         - \n\u4efb\u52a1\u542f\u52a8\u65f6\u523b                    : 2022-02-26 21:59:23\n\u4efb\u52a1\u7ed3\u675f\u65f6\u523b                    : 2022-02-26 21:59:26\n\u4efb\u52a1\u603b\u8ba1\u8017\u65f6                    :                  3s\n\u4efb\u52a1\u5e73\u5747\u6d41\u91cf                    :            2.99KB/s\n\u8bb0\u5f55\u5199\u5165\u901f\u5ea6                    :             33rec/s\n\u8bfb\u51fa\u8bb0\u5f55\u603b\u6570                    :                 100\n\u8bfb\u5199\u5931\u8d25\u603b\u6570                    :                   0\n</code></pre> <p>We use Kafka's built-in <code>kafka-console-consumer.sh</code> to try reading data, output as follows:</p> <pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-1 --from-beginning\n\n{\"col8\":\"41.12,-71.34\",\"col9\":\"2017-05-25 11:22:33\",\"col6\":\"hello world\",\"col7\":\"long text\",\"col4\":19890604,\"col5\":19890604,\"col2\":\"1.1.1.1\",\"col3\":1.9890604E7,\"col1\":916}\n</code></pre>"},{"location":"writer/kuduwriter/","title":"Kudu Writer","text":"<p>Kudu Writer plugin implements writing data to Apache Kudu.</p>"},{"location":"writer/kuduwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Kudu database. For detailed configuration and parameters, please refer to the original Kudu Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,10\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"1000,50000\",\n              \"type\": \"double\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"kuduwriter\",\n        \"parameter\": {\n          \"masterAddress\": \"127.0.0.1:7051,127.0.0.1:7151,127.0.0.1:7251\",\n          \"timeout\": 60,\n          \"table\": \"users\",\n          \"writeMode\": \"upsert\",\n          \"column\": [ \"user_id\", \"user_name\", \"salary\"],\n          \"batchSize\": 1024,\n          \"bufferSize\": 2048,\n          \"skipFail\": false,\n          \"encoding\": \"UTF-8\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/kuduwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Kudu with configurable master addresses, table operations, and data consistency options.</p>"},{"location":"writer/mongodbwriter/","title":"MongoDB Writer","text":"<p>MongoDB Writer plugin implements writing data to MongoDB.</p>"},{"location":"writer/mongodbwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to MongoDB database. For detailed configuration and parameters, please refer to the original MongoDB Writer documentation.</p>"},{"location":"writer/mongodbwriter/#parameters","title":"Parameters","text":"<p>This plugin provides comprehensive MongoDB writing capabilities with support for various data types and connection options.</p>"},{"location":"writer/mysqlwriter/","title":"MySQL Writer","text":"<p>MySQL Writer plugin implements the functionality of writing data to MySQL destination tables.</p>"},{"location":"writer/mysqlwriter/#example","title":"Example","text":"<p>Assume the MySQL table to be written has the following DDL statement:</p> <pre><code>create table test.addax_tbl\n(\n  col1 varchar(20) ,\n  col2 int(4),\n  col3 datetime,\n  col4 boolean,\n  col5 binary\n) default charset utf8;\n</code></pre> <p>Here we use data generated from memory to import into MySQL.</p> job/stream2mysql.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"mysqlwriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"username\": \"root\",\n          \"password\": \"\",\n          \"column\": [\n            \"*\"\n          ],\n          \"session\": [\n            \"set session sql_mode='ANSI'\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2mysql.json</code></p>"},{"location":"writer/mysqlwriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2mysql.json\n</code></pre>"},{"location":"writer/mysqlwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds some MySQL-specific configuration items.</p> Configuration Required Type Default Value Description writeMode Yes string insert The way data is written to the table, see below batchSize No int 1024 Defines the number of batch data fetched between plugin and database server each time"},{"location":"writer/mysqlwriter/#driver","title":"driver","text":"<p>The current MySQL JDBC driver uses version 8.0 and above, with driver class name <code>com.mysql.cj.jdbc.Driver</code>, not <code>com.mysql.jdbc.Driver</code>. If you need to collect from MySQL server below <code>5.6</code> and need to use <code>Connector/J 5.1</code> driver, you can take the following steps:</p> <ol> <li> <p>Replace the built-in driver in the plugin   <code>rm -f plugin/writer/mysqlwriter/libs/mysql-connector-java-*.jar</code></p> </li> <li> <p>Copy the old driver to the plugin directory   <code>cp mysql-connector-java-5.1.48.jar plugin/writer/mysqlwriter/libs/</code></p> </li> <li> <p>Specify driver class name   In your json file, configure <code>\"driver\": \"com.mysql.jdbc.Driver\"</code></p> </li> </ol>"},{"location":"writer/mysqlwriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> means using <code>insert into</code></li> <li><code>replace</code> means using <code>replace into</code> method</li> <li><code>update</code> means using <code>ON DUPLICATE KEY UPDATE</code> statement</li> </ul>"},{"location":"writer/oraclewriter/","title":"Oracle Writer","text":"<p>Oracle Writer plugin implements the functionality of writing data to Oracle destination tables.</p>"},{"location":"writer/oraclewriter/#configuration-example","title":"Configuration Example","text":"<p>Here we use data generated from memory to import into Oracle.</p> job/stream2oracle.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"oraclewriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"id\",\n            \"name\"\n          ],\n          \"preSql\": [\n            \"delete from test\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\",\n            \"table\": [\n              \"test\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/oraclewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer, and adds some OracleWriter-specific configuration items.</p> Configuration Required Default Value Description writeMode No insert Write mode, supports insert, update, see below"},{"location":"writer/oraclewriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to Oracle tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, which is Oracle's <code>merge into</code> syntax, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre>"},{"location":"writer/paimonwriter/","title":"Paimon Writer","text":"<p>Paimon Writer plugin implements writing data to Apache Paimon.</p>"},{"location":"writer/paimonwriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to Paimon tables. For detailed configuration and parameters, please refer to the original Paimon Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      },\n      \"errorLimit\": {\n        \"record\": 0,\n        \"percentage\": 0\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"rdbmsreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"root\",\n            \"column\": [\n              \"*\"\n            ],\n            \"connection\": [\n              {\n                \"querySql\": [\n                  \"select 1+0 id ,'test1' as name\"\n                ],\n                \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/ruoyi_vue_camunda?allowPublicKeyRetrieval=true\",]\n              }\n            ],\n            \"fetchSize\": 1024\n          }\n        },\n        \"writer\": {\n          \"name\": \"paimonwriter\",\n          \"parameter\": {\n            \"dbName\": \"test\",\n            \"tableName\": \"test2\",\n            \"writeMode\": \"truncate\",\n            \"paimonConfig\": {\n              \"warehouse\": \"file:///g:/paimon\",\n              \"metastore\": \"filesystem\"\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"writer/paimonwriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to Paimon with configurable catalog, table, and schema options.</p>"},{"location":"writer/postgresqlwriter/","title":"Postgresql Writer","text":"<p>Postgresql Writer plugin implements the functionality of writing data to PostgreSQL database tables.</p>"},{"location":"writer/postgresqlwriter/#example","title":"Example","text":"<p>The following configuration demonstrates reading data from a specified PostgreSQL table and inserting it into another table with the same table structure, to test the data types supported by this plugin.</p>"},{"location":"writer/postgresqlwriter/#table-structure-information","title":"Table Structure Information","text":"<p>Assume the table creation statement and input insertion statement are as follows:</p> <pre><code>create table if not exists addax_tbl\n(\n    c_bigint bigint,\n    c_bit bit(3),\n    c_bool boolean,\n    c_byte bytea,\n    c_char char(10),\n    c_varchar varchar(20),\n    c_date date,\n    c_double float8,\n    c_int integer,\n    c_json json,\n    c_number decimal(8, 3),\n    c_real real,\n    c_small smallint,\n    c_text text,\n    c_ts timestamp,\n    c_uuid uuid,\n    c_xml xml,\n    c_money money,\n    c_inet inet,\n    c_cidr cidr,\n    c_macaddr macaddr\n    );\n\ninsert into addax_tbl\nvalues (999988887777,\n        b'101',\n        TRUE,\n        '\\xDEADBEEF',\n        'hello',\n        'hello, world',\n        '2021-01-04',\n        999888.9972,\n        9876542,\n        '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'::json,\n        12345.123,\n        123.123,\n        126,\n        'this is a long text ',\n        '2020-01-04 12:13:14',\n        'A0EEBC99-9C0B-4EF8-BB6D-6BB9BD380A11'::uuid,\n        '&lt;foo&gt;bar&lt;/foo&gt;'::xml,\n        '52093.89'::money,\n        '192.168.1.1'::inet,\n        '192.168.1/24'::cidr,\n        '08002b:010203'::macaddr);\n</code></pre> <p>The statement to create the table to be inserted is as follows:</p> <pre><code>create table addax_tbl1 as select * from  addax_tbl where 1=2;\n</code></pre>"},{"location":"writer/postgresqlwriter/#task-configuration","title":"Task Configuration","text":"<p>The following is the configuration file</p> job/pg2pg.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"postgresqlreader\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"column\": [\n            \"*\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"addax_tbl\"\n            ],\n            \"jdbcUrl\": \"jdbc:postgresql://localhost:5432/pgtest\"\n          }\n        }\n      },\n      \"writer\": {\n        \"name\": \"postgresqlwriter\",\n        \"parameter\": {\n          \"username\": \"pgtest\",\n          \"password\": \"pgtest\",\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"truncate table @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:postgresql://127.0.0.1:5432/pgtest\",\n            \"table\": [\n              \"addax_tbl1\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/pg2pg.json</code></p>"},{"location":"writer/postgresqlwriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/pg2pg.json\n</code></pre>"},{"location":"writer/postgresqlwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/postgresqlwriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to PostgreSQL tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(id)\"\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>\"writeMode\": \"update(col1, col2)\"\n</code></pre> <p>Note: <code>update</code> mode was first added in version <code>3.1.6</code>, previous versions do not support it.</p>"},{"location":"writer/postgresqlwriter/#type-conversion","title":"Type Conversion","text":"<p>Currently PostgresqlWriter supports most PostgreSQL types, but there are also some cases that are not supported. Please check your types carefully.</p> <p>The following lists PostgresqlWriter's type conversion list for PostgreSQL:</p> Addax Internal Type PostgreSQL Data Type Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit, inet,cidr,macaddr,uuid,xml,json Date date, time, timestamp Boolean bool Bytes bytea"},{"location":"writer/postgresqlwriter/#known-limitations","title":"Known Limitations","text":"<p>Except for the data types listed above, other data types are theoretically converted to string type, but accuracy is not guaranteed.</p>"},{"location":"writer/rdbmswriter/","title":"RDBMS Writer","text":"<p>RDBMS Writer plugin supports writing data to traditional RDBMS. This is a generic relational database writer plugin that can support more relational database writing by registering database drivers.</p> <p>At the same time, RDBMS Writer is also the base class for other relational database writer plugins. The following writer plugins all depend on this plugin:</p> <ul> <li>Oracle Writer</li> <li>MySQL Writer</li> <li>PostgreSQL Writer</li> <li>ClickHouse Writer</li> <li>SQLServer Writer</li> <li>Access Writer</li> <li>Databend Writer</li> </ul> <p>Note: If a dedicated database writer plugin is already provided, it is recommended to use the dedicated plugin. If the database you need to write to does not have a dedicated plugin, consider using this generic plugin. Before use, you need to perform the following operations to run normally, otherwise exceptions will occur.</p>"},{"location":"writer/rdbmswriter/#configure-driver","title":"Configure Driver","text":"<p>Suppose you need to write data to IBM DB2. Since no dedicated writer plugin is provided, we can use this plugin to implement it. Before use, you need to perform the following two operations:</p> <ol> <li>Download the corresponding JDBC driver and copy it to the <code>plugin/writer/rdbmswriter/libs</code> directory</li> <li>Modify the task configuration file, find the <code>driver</code> item, and fill in the correct JDBC driver name, such as DB2's driver name <code>com.ibm.db2.jcc.DB2Driver</code></li> </ol> <p>The following lists common databases and their corresponding driver names:</p> <ul> <li>Apache Impala: <code>com.cloudera.impala.jdbc41.Driver</code></li> <li>Enterprise DB: <code>com.edb.Driver</code></li> <li>PrestoDB: <code>com.facebook.presto.jdbc.PrestoDriver</code></li> <li>IBM DB2: <code>com.ibm.db2.jcc.DB2Driver</code></li> <li>MySQL: <code>com.mysql.cj.jdbc.Driver</code></li> <li>Sybase Server: <code>com.sybase.jdbc3.jdbc.SybDriver</code></li> <li>TDengine: <code>com.taosdata.jdbc.TSDBDriver</code></li> <li>\u8fbe\u68a6\u6570\u636e\u5e93: <code>dm.jdbc.driver.DmDriver</code></li> <li>\u661f\u73afInceptor: <code>io.transwarp.jdbc.InceptorDriver</code></li> <li>TrinoDB: <code>io.trino.jdbc.TrinoDriver</code></li> <li>PrestoSQL: <code>io.prestosql.jdbc.PrestoDriver</code></li> <li>Oracle DB: <code>oracle.jdbc.OracleDriver</code></li> <li>PostgreSQL: <code>org.postgresql.Drive</code></li> </ul>"},{"location":"writer/rdbmswriter/#configuration","title":"Configuration","text":"<p>Configure a job to write to RDBMS.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"rdbmswriter\",\n        \"parameter\": {\n          \"username\": \"username\",\n          \"password\": \"password\",\n          \"driver\": \"dm.jdbc.driver.DmDriver\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from XXX;\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:dm://ip:port/database\",\n            \"table\": [\n              \"table\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/rdbmswriter/#parameters","title":"Parameters","text":"<p>This plugin provides configuration for writing to relational databases. For detailed parameter descriptions, please refer to the original RDBMS Writer documentation.</p>"},{"location":"writer/rediswriter/","title":"Redis Writer","text":"<p>Redis Writer plugin is used to write data to Redis database.</p>"},{"location":"writer/rediswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin provides the ability to write data to Redis database. For detailed configuration and parameters, please refer to the original Redis Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"redisreader\",\n        \"parameter\": {\n          \"connection\": [\n            {\n              \"uri\": \"tcp://127.0.0.1:7003\"\n            }\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"rediswriter\",\n        \"parameter\": {\n          \"connection\": {\n            \"uri\": \"tcp://127.0.0.1:6379\",\n            \"auth\": \"123456\"\n          },\n          \"redisCluster\": false,\n          \"flushDB\": false\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/rediswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing various data types to Redis with configurable connection and data format options.</p>"},{"location":"writer/s3writer/","title":"S3 Writer","text":"<p>S3 Writer plugin is used to write data to Amazon AWS S3 storage, as well as S3 protocol compatible storage, such as MinIO.</p> <p>In implementation, this plugin is written based on S3's official SDK 2.0.</p>"},{"location":"writer/s3writer/#configuration-example","title":"Configuration Example","text":"<p>The following configuration is used to read data from memory and write to specified S3 bucket.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"byte\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19890604,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1989-06-04 11:22:33\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 10\n        }\n      },\n      \"writer\": {\n        \"name\": \"s3writer\",\n        \"parameter\": {\n          \"endpoint\": \"https://s3.amazonaws.com\",\n          \"accessId\": \"xxxxxxxxxxxx\",\n          \"accessKey\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n          \"bucket\": \"test\",\n          \"object\": \"upload.csv\",\n          \"region\": \"ap-northeast-1\",\n          \"encoding\": \"\",\n          \"fieldDelimiter\": \",\",\n          \"writeMode\": \"truncate\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/s3writer/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description endpoint Yes string None S3 Server EndPoint address, e.g. <code>s3.xx.amazonaws.com</code> region Yes string None S3 Server Region address, e.g. <code>ap-southeast-1</code> accessId Yes string None Access ID accessKey Yes string None Access Key bucket Yes string None Bucket to write to object Yes string None Object to write to, see notes below fieldDelimiter No char <code>','</code> Field delimiter nullFormat No char <code>\\N</code> What character to use when value is null header No list None Write file header information, e.g. <code>[\"id\",\"title\",\"url\"]</code> maxFileSize No int <code>100000</code> Size of single object, in MB encoding No string <code>utf-8</code> File encoding format"},{"location":"writer/sqlitewriter/","title":"SQLite Writer","text":"<p>SQLite Writer plugin implements the functionality of writing data to SQLite database.</p>"},{"location":"writer/sqlitewriter/#example","title":"Example","text":"<p>Assume the table to be written is as follows:</p> <pre><code>create table addax_tbl\n(\n    col1 varchar(20) ,\n    col2 int(4),\n    col3 datetime,\n    col4 boolean,\n    col5 binary\n);\n</code></pre> <p>Here we use data generated from memory to SQLite.</p> job/stream2sqlite.json <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"value\": \"Addax\",\n              \"type\": \"string\"\n            },\n            {\n              \"value\": 19880808,\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"1988-08-08 08:08:08\",\n              \"type\": \"date\"\n            },\n            {\n              \"value\": true,\n              \"type\": \"bool\"\n            },\n            {\n              \"value\": \"test\",\n              \"type\": \"bytes\"\n            }\n          ],\n          \"sliceRecordCount\": 1000\n        }\n      },\n      \"writer\": {\n        \"name\": \"sqlitewriter\",\n        \"parameter\": {\n          \"writeMode\": \"insert\",\n          \"column\": [\n            \"*\"\n          ],\n          \"preSql\": [\n            \"delete from @table\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sqlite://tmp/writer.sqlite3\",\n            \"table\": [\n              \"addax_tbl\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Save the above configuration file as <code>job/stream2sqlite.json</code></p>"},{"location":"writer/sqlitewriter/#execute-collection-command","title":"Execute Collection Command","text":"<p>Execute the following command for data collection</p> <pre><code>bin/addax.sh job/stream2sqlite.json\n</code></pre>"},{"location":"writer/sqlitewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer. Since SQLite connection does not require username and password, the <code>username</code> and <code>password</code> that other database writer plugins need to configure are not needed here.</p>"},{"location":"writer/sqlitewriter/#writemode","title":"writeMode","text":"<ul> <li><code>insert</code> means using <code>insert into</code></li> <li><code>replace</code> means using <code>replace into</code> method</li> <li><code>update</code> means using <code>ON DUPLICATE KEY UPDATE</code> statement</li> </ul>"},{"location":"writer/sqlitewriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type SQLite Data Type Long integer Double real String varchar Date datetime Boolean bool Bytes blob, binary"},{"location":"writer/sqlserverwriter/","title":"SQLServer Writer","text":"<p>SQLServer Writer plugin implements the functionality of writing data to SQL Server database tables.</p>"},{"location":"writer/sqlserverwriter/#configuration-example","title":"Configuration Example","text":"<p>Here we use data generated from memory to import into SQL Server.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 1,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {},\n      \"writer\": {\n        \"name\": \"sqlserverwriter\",\n        \"parameter\": {\n          \"username\": \"root\",\n          \"password\": \"root\",\n          \"column\": [\n            \"db_id\",\n            \"db_type\",\n            \"db_ip\",\n            \"db_port\",\n            \"db_role\",\n            \"db_name\",\n            \"db_username\",\n            \"db_password\",\n            \"db_modify_time\",\n            \"db_modify_user\",\n            \"db_description\",\n            \"db_tddl_info\"\n          ],\n          \"preSql\": [\n            \"delete from @table where db_id = -1;\"\n          ],\n          \"postSql\": [\n            \"update @table set db_modify_time = now() where db_id = 1;\"\n          ],\n          \"connection\": {\n            \"table\": [\n              \"db_info_for_writer\"\n            ],\n            \"jdbcUrl\": \"jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/sqlserverwriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/sqlserverwriter/#writemode","title":"writeMode","text":"<p>By default, <code>insert into</code> syntax is used to write to SQL Server tables. If you want to use the mode of updating when primary key exists and inserting when it doesn't exist, which is SQL Server's <code>MERGE INTO</code> syntax, you can use <code>update</code> mode. Assuming the table's primary key is <code>id</code>, the <code>writeMode</code> configuration method is as follows:</p> <pre><code>{\n  \"writeMode\": \"update(id)\"\n}\n</code></pre> <p>If it's a composite unique index, the configuration method is as follows:</p> <pre><code>{\n  \"writeMode\": \"update(col1, col2)\"\n}\n</code></pre>"},{"location":"writer/starrockswriter/","title":"StarRocks Writer","text":"<p>StarRocks Writer plugin implements writing data to StarRocks.</p>"},{"location":"writer/starrockswriter/#configuration-example","title":"Configuration Example","text":"<p>This plugin is used to write data to StarRocks database. For detailed configuration and parameters, please refer to the original StarRocks Writer documentation.</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"1,500\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"1,127\",\n              \"type\": \"long\"\n            },\n            {\n              \"value\": \"this is a text\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"5,200\",\n              \"type\": \"long\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"starrockswriter\",\n        \"parameter\": {\n          \"username\": \"test\",\n          \"password\": \"123456\",\n          \"column\": [\n            \"siteid\",\n            \"citycode\",\n            \"username\",\n            \"pv\"\n          ],\n          \"database\": \"example_db\",\n          \"table\": \"table1\",\n          \"jdbcUrl\": \"jdbc:mysql://172.28.17.100:9030/\",\n          \"loadUrl\": [\n            \"172.28.17.100:8030\",\n            \"172.28.17.100:8030\"\n          ],\n          \"loadProps\": {\n            \"column_separator\": \"\\\\x01\",\n            \"row_delimiter\": \"\\\\x02\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/starrockswriter/#parameters","title":"Parameters","text":"<p>This plugin supports writing data to StarRocks with Stream Load capabilities and configurable connection options.</p>"},{"location":"writer/streamwriter/","title":"Stream Writer","text":"<p>Stream Writer is a plugin that writes data to memory, generally used to write acquired data to terminal for debugging data processing of read plugins.</p> <p>A typical Stream Writer configuration is as follows:</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"print\": true,\n    \"nullFormat\": \"NULL\"\n  }\n}\n</code></pre> <p>The above configuration will print the acquired data directly to terminal. Where <code>nullFormat</code> is used to specify how to represent null values in terminal, default is string <code>NULL</code>. If you don't want to print null values, you can set it to <code>\"\"</code>.</p> <p>This plugin also supports writing data to files, configured as follows:</p> <pre><code>{\n  \"name\": \"streamwriter\",\n  \"parameter\": {\n    \"encoding\": \"UTF-8\",\n    \"path\": \"/tmp/out\",\n    \"fileName\": \"out.txt\",\n    \"fieldDelimiter\": \",\",\n    \"recordNumBeforeSleep\": \"100\",\n    \"sleepTime\": \"5\"\n  }\n}\n</code></pre> <p>In the above configuration:</p> <ul> <li><code>fieldDelimiter</code> represents field delimiter, default is tab character (<code>\\t</code>)</li> <li><code>recordNumBeforeSleep</code> represents how many records to acquire before executing sleep, default is 0, meaning this feature is disabled</li> <li><code>sleepTime</code> represents how long to sleep, in seconds, default is 0, meaning this feature is disabled</li> </ul> <p>The meaning of the above configuration is to write data to <code>/tmp/out/out.txt</code> file, sleep for 5 seconds after acquiring every 100 records.</p>"},{"location":"writer/sybasewriter/","title":"Sybase Writer","text":"<p>Sybase Writer plugin implements the functionality of writing data to Sybase database tables.</p>"},{"location":"writer/sybasewriter/#configuration-example","title":"Configuration Example","text":"<p>We can use Docker container to start a Sybase database</p> <pre><code>docker run -tid --rm  -h dksybase --name sybase  -p 5000:5000  ifnazar/sybase_15_7 bash /sybase/start\n</code></pre> <p>Then create a table as follows:</p> <pre><code>create table addax_writer \n(\n    id int,\n    name varchar(255),\n    salary float(2),\n    created_at datetime,\n    updated_at datetime\n);\n</code></pre> <p>Then use the following task configuration file:</p> <pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"streamreader\",\n        \"parameter\": {\n          \"column\": [\n            {\n              \"random\": \"100,1000\",\n              \"type\": \"long\"\n            },\n            {\n              \"random\": \"10,100\",\n              \"type\": \"string\"\n            },\n            {\n              \"random\": \"10,1000\",\n              \"type\": \"double\"\n            },\n            {\n              \"incr\": \"2022-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            },\n            {\n              \"incr\": \"2023-01-01 13:00:00,2,d\",\n              \"type\": \"date\"\n            }\n          ],\n          \"sliceRecordCount\": 100\n        }\n      },\n      \"writer\": {\n        \"name\": \"sybasewriter\",\n        \"parameter\": {\n          \"username\": \"sa\",\n          \"password\": \"password\",\n          \"column\": [\n            \"id\",\n            \"name\",\n            \"salary\",\n            \"created_at\",\n            \"updated_at\"\n          ],\n          \"connection\": {\n            \"jdbcUrl\": \"jdbc:sybase:Tds:127.0.0.1:5000/master\",\n            \"table\": [\n              \"dbo.addax_writer\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/sybasewriter/#parameters","title":"Parameters","text":"<p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/tdenginewriter/","title":"TDengine Writer","text":"<p>TDengine Writer plugin implements writing data to TDengine database system. In the underlying implementation, TDengine Writer connects to remote TDengine database through JDBC JNI driver and executes corresponding SQL statements to batch write data to TDengine database.</p>"},{"location":"writer/tdenginewriter/#prerequisites","title":"Prerequisites","text":"<p>Considering performance issues, this plugin uses TDengine's JDBC-JNI driver, which directly calls the client API (<code>libtaos.so</code> or <code>taos.dll</code>) to send write and query requests to <code>taosd</code> instances. Therefore, dynamic library link files need to be configured before use.</p> <p>First copy <code>plugin/writer/tdenginewriter/libs/libtaos.so.2.0.16.0</code> to <code>/usr/lib64</code> directory, then execute the following commands to create soft links:</p> <pre><code>ln -sf /usr/lib64/libtaos.so.2.0.16.0 /usr/lib64/libtaos.so.1\nln -sf /usr/lib64/libtaos.so.1 /usr/lib64/libtaos.so\n</code></pre>"},{"location":"writer/tdenginewriter/#example","title":"Example","text":"<p>Assume the table to be written is as follows:</p> <pre><code>create table test.addax_test (\n    ts timestamp,\n    name nchar(100),\n    file_size int,\n    file_date timestamp,\n    flag_open bool,\n    memo nchar(100)\n);\n</code></pre> <p>This plugin is based on RDBMS Writer, so you can refer to all configuration items of RDBMS Writer.</p>"},{"location":"writer/txtfilewriter/","title":"TxtFile Writer","text":"<p>TxtFile Writer provides writing CSV-like format to one or more table files in local file system.</p>"},{"location":"writer/txtfilewriter/#configuration-example","title":"Configuration Example","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 2,\n        \"bytes\": -1\n      }\n    },\n    \"content\": {\n      \"reader\": {\n        \"name\": \"txtfilereader\",\n        \"parameter\": {\n          \"path\": [\n            \"/tmp/data\"\n          ],\n          \"encoding\": \"UTF-8\",\n          \"column\": [\n            {\n              \"index\": 0,\n              \"type\": \"long\"\n            },\n            {\n              \"index\": 1,\n              \"type\": \"boolean\"\n            },\n            {\n              \"index\": 2,\n              \"type\": \"double\"\n            },\n            {\n              \"index\": 3,\n              \"type\": \"string\"\n            },\n            {\n              \"index\": 4,\n              \"type\": \"date\",\n              \"format\": \"yyyy.MM.dd\"\n            }\n          ],\n          \"fieldDelimiter\": \",\"\n        }\n      },\n      \"writer\": {\n        \"name\": \"txtfilewriter\",\n        \"parameter\": {\n          \"path\": \"/tmp/result\",\n          \"fileName\": \"luohw\",\n          \"writeMode\": \"truncate\",\n          \"dateFormat\": \"yyyy-MM-dd\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"writer/txtfilewriter/#parameters","title":"Parameters","text":"Configuration Required Data Type Default Value Description path Yes string None Path information of local file system, write multiple files under Path directory fileName Yes string None Name of file to write, this filename will have random suffix added as actual filename for each thread writeMode Yes string None Data cleanup processing mode before writing, see below fieldDelimiter Yes string <code>,</code> Field delimiter for reading compress No string None Text compression type, supports <code>zip</code>, <code>lzo</code>, <code>lzop</code>, <code>tgz</code>, <code>bzip2</code> encoding No string utf-8 Encoding configuration for reading files nullFormat No string <code>\\N</code> Define which strings can represent null dateFormat No string None Format when date type data is serialized to file, e.g. <code>\"yyyy-MM-dd\"</code> fileFormat No string text Format of file output, see below table Yes string None Table name to specify in SQL mode column No list None Optional column names to specify in SQL mode extendedInsert No boolean true Whether to use batch insert syntax in SQL mode, see below batchSize No int 2048 Batch size for batch insert syntax in SQL mode, see below header No list None Table header for text output, example <code>['id', 'name', 'age']</code>"},{"location":"writer/txtfilewriter/#writemode","title":"writeMode","text":"<p>Data cleanup processing mode before writing:</p> <ul> <li>truncate: Clean all files with fileName prefix under directory before writing.</li> <li>append: No processing before writing, write directly using filename and ensure no filename conflicts.</li> <li>nonConflict: If there are files with fileName prefix under directory, report error directly.</li> </ul>"},{"location":"writer/txtfilewriter/#fileformat","title":"fileFormat","text":"<p>Format of file output, including csv, text, and sql (introduced in version <code>4.1.3</code>). CSV is strict csv format, if data to be written includes column delimiter, it will be escaped according to csv escape syntax, with escape symbol being double quotes <code>\"</code>; text format simply separates data to be written with column delimiter, no escaping for data including column delimiter. SQL format means writing data to file in SQL statement (<code>INSERT INTO ... VALUES</code>) format.</p>"},{"location":"writer/txtfilewriter/#table","title":"table","text":"<p>Only required in sql file format, used to specify the table name to write to.</p>"},{"location":"writer/txtfilewriter/#column","title":"column","text":"<p>In sql file format, you can specify column names to write. If specified, the sql statement is like <code>INSERT INTO table (col1, col2, col3) VALUES (val1, val2, val3)</code>, otherwise it's <code>INSERT INTO table VALUES (val1, val2, val3)</code>.</p>"},{"location":"writer/txtfilewriter/#extendedinsert","title":"extendedInsert","text":"<p>Whether to enable batch insert syntax. If enabled, batchSize number of data will be written to file at once, otherwise each data is one line. This parameter borrows from the extended-insert parameter syntax of <code>mysqldump</code> tool.</p>"},{"location":"writer/txtfilewriter/#batchsize","title":"batchSize","text":"<p>Batch size for batch insert syntax. If extendedInsert is true, every batchSize data will be written to file at once, otherwise each data is one line.</p>"},{"location":"writer/txtfilewriter/#type-conversion","title":"Type Conversion","text":"Addax Internal Type Local File Data Type Long Long Double Double string string Boolean Boolean Date Date"}]}